\section{Неразмеченные данные в глубоком обучении}

\subsection{Постановка}
\hspace{2em}Общий подход к решению задачи с неразмеченными данными следующий:

$\mathcal{U} \subset \mathcal{X}$ --- большая неразмеченная выборка,

$\mathcal{D} \subset  \mathcal{X}\times \mathcal{Y}$ --- небольшая размеченная выборка,

требуется построить отображение $f: \mathcal{X} \to \mathcal{Y}$.

\subsection{Суперпозиция}
\hspace{2em}Предположение $f = c  \circ  h$ --- отображение есть суперпозия двух функций:

$h$ --- генерация признакового описания $h: \mathcal{X} \to \mathcal{H}$,

$c$ --- классификатор $c: \mathcal{H} \to \mathcal{Y}$.

Заметим, что $c$ и $h$, например, это некоторое параметрические функции, параметры которых нужно найти.

Простой пример:

$h$ --- все слои полносвязного многослойного перцептрона,

$c$ --- последний слой.

Обычно $h$ является сложной моделью, а $c$ --- линейной. Рассмотрим более подробный пример.

\subsection{Постановка задачи SimCLR}

\hspace{2em}Постановка задачи SimCLR заключается в том, что даётся набор изображений без каких-либо меток, и нужно обучить модель на этих данных так, чтобы она могла быстро адаптироваться к любой задаче распознавания изображений впоследствии.

Цель обучения - найти такое признаковое пространство, в котором изображения одного класса располагаются близко по введённой метрике, а изображения разных классов - далеко. Для этого используют различные \textit{контрастивные функции потерь}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{chapters/unlabeled/simclr.png}
    \caption{Схематичное представление алгоритма SimCLR}
    \label{fig:enter-label-1}
\end{figure}

На рисунке можем схематичное представление последовательности шагов алгоритма со следующими обозначениями:

\begin{quote}
$x$ - исходное изображение,

$t, t'$ - разные аугментации на исходное изображение $x$ для получения представлений $\tilde{x_i}$, $\tilde{x_j}$,

$f$ - функция, генерирующая представления (representation) $h_i$, $h_j$ в виде вектора для входных изображений,

$g$ - функция, которая переводит вектора $h_i$, $h_j$ в величины $z_i$, $z_j$, для которых минимизируется расстояние (maximize agreement) в процессе обучения для аугментаций одного изображения и максимизируется для разных изображений.
\end{quote}

Шаги в обучении следующие:

1. К исходному изображению $x$ применяются аугментации $t, t'$. \textit{Аугментация} - это процесс создания новых данных на основе существующих с помощью преобразований. Такими преобразованиями для изображений являются, например, поворот, изменение яркости, гауссовское размытие и так далее. Она необходима, чтобы в новом признаковом пространстве изменённые изображения так же находились близко к изображениям того же класса.

2. К аугментированным изображениям $\tilde{x_i}$, $\tilde{x_j}$ применяется функция $f$, генерирующая векторное представление этих изображений $h_i$, $h_j$.

3. К векторным представлениям $h_i$, $h_j$ применяется функция $g$ для перевода в другое векторное представление. Данный шаг необязательный для обучения методом SimCLR.

4. Для $z_i$, $z_j$ уменьшается расстояние, если $\tilde{x_i}$, $\tilde{x_j}$ - аугментации одного и того же изображения, и увеличивается - если разных. Происходит это путём минимизации контрастивной функции потерь. 

Таким образом мы находим новое признаковое описание для изображения, с помощью которого можно обучиться на задачу классификации изображений, то есть найти функция $c$ из раздела про Суперпозицию.

\subsection{Вопросы}
\hspace{2em}1. Что такое суперпозиция в контексте построения модели для неразмеченных данных, и как она помогает в решении задачи с использованием методов глубокого обучения?

2. Каковы основные этапы и задачи в методе SimCLR?

3. Как аугментация данных способствует улучшению качества обучаемых представлений?

\section{Метод Expectation Regularization (XR)}

Рассмотрим задачу частичного обучения (\textit{semi-supervised learning}). Пусть дано множество объектов $X$ и множество классов $Y$. Предположим, что у нас имеется небольшой набор размеченных данных:
\[
X^k = \{ x_1, \ldots, x_k \} \quad \text{с метками} \quad \{y_1, \ldots, y_k\},
\]
где $y_i \in Y$ – известная метка класса для объекта $x_i$. Кроме того, нам даны неразмеченные данные:
\[
U = \{x_{k+1}, \ldots, x_{\ell}\},
\]
для которых метки классов неизвестны. Основная цель частичного обучения – используя как размеченные, так и неразмеченные данные, построить алгоритм классификации $a: X \rightarrow Y$ с лучшими обобщающими свойствами. В трансдуктивном случае, зная заранее все неразмеченные объекты, задача сводится к определению для них меток: $\{x_{k+1}, \ldots, x_{\ell}\} \rightarrow \{a_{k+1}, \ldots, a_{\ell}\}$.

Частичное обучение широко применяется: от классификации текстов и изображений до каталогизации и группировки данных в сложных приложениях, где полностью размеченные выборки собрать затруднительно или дорого.

\subsection{Многоклассовая логистическая регрессия.}

В качестве базовой модели рассмотрим многоклассовую логистическую регрессию. Предположим, что имеется конечный набор классов $Y = \{1, \ldots, |Y|\}$. Рассмотрим линейный классификатор:
\[
a(x) = \arg \max_{y \in Y} \langle w_y, x \rangle, 
\]
где $x \in \mathbb{R}^n$ – вектор признаков объекта, а $w_y \in \mathbb{R}^n$ – вектор параметров (весов) для класса $y$.

Вероятность принадлежности объекта $x_i$ классу $y$, согласно модели логистической регрессии, задаётся с помощью функции softmax:
\[
P(y \mid x_i, w) = \frac{\exp\langle w_y, x_i \rangle}{\sum_{c \in Y} \exp\langle w_c, x_i \rangle},
\]
где $w = (w_y: y \in Y)$ – совокупность всех параметров модели.

Оценка параметров модели $w$ производится путём максимизации регуляризованного правдоподобия по размеченным данным:
\[
Q(w) = \sum_{i=1}^k \log P(y_i \mid x_i, w) - \frac{1}{2C}\sum_{y \in Y}\|w_y\|^2 \;\; \to \max_w.
\]
Здесь $C > 0$ – параметр регуляризации, контролирующий степень сглаживания весов. Оптимизацию критерия $Q(w)$ обычно проводят методом стохастического градиента по параметрам $w$.

\subsection{Согласование модели на размеченных и неразмеченных данных.}

Для улучшения качества обучения в частично размеченных задачах целесообразно использовать неразмеченные данные $U = \{x_{k+1}, \ldots, x_{\ell}\}$. Пусть $b_j(x)$ – бинарный признак, связанный с объектом $x$, где $j = 1, \ldots, m$. Например, в задаче классификации текстов признак $b_j(x)$ может означать наличие или отсутствие определённого термина $j$ в документе $x$. 

Наша цель – согласовать вероятностную модель с эмпирическим распределением классов, оценённым по признакам. Рассмотрим вероятность принадлежности класса $y$ объекту, для которого признак $b_j$ активен ($b_j(x)=1$):
\[
\mathrm{P}(y \mid b_j(x)=1).
\]

Эту вероятность можно оценить двумя способами:

1. \textbf{Эмпирическая оценка} по размеченным данным $X^k$:
\[
\hat{p}_j(y) = \frac{\sum_{i=1}^k b_j(x_i)[y_i = y]}{\sum_{i=1}^k b_j(x_i)},
\]
где $[y_i = y]$ – индикаторная функция. Таким образом, $\hat{p}_j(y)$ – это частота встречаемости класса $y$ среди тех размеченных объектов, для которых признак $b_j(x)=1$.

2. \textbf{Оценка по неразмеченным данным} $U$ с использованием текущей вероятностной модели:
\[
p_j(y \mid w) = \frac{\sum_{i=k+1}^{\ell} b_j(x_i) P(y \mid x_i, w)}{\sum_{i=k+1}^{\ell} b_j(x_i)}.
\]
Здесь $P(y \mid x_i, w)$ – вероятность, предсказываемая моделью логистической регрессии, а числитель и знаменатель вычисляют взвешенную частоту класса $y$ среди объектов, в которых признак $b_j$ активен, но метки неизвестны. Таким образом, $p_j(y \mid w)$ – модельная оценка распределения классов по признаку $b_j(x)$, учитывающая неразмеченные данные.

Наша задача – приблизить модельное распределение $p_j(y \mid w)$ к эмпирическому распределению $\hat{p}_j(y)$.

\subsection{Построение регуляризатора (eXpectation Regularization, XR).}

Идея метода XR, предложенная в работе \cite{mann2007simple}, состоит в том, чтобы заставить модельные вероятности согласоваться с эмпирическими оценками. Для этого мы вводим дополнительный регуляризатор, максимизирующий логарифм правдоподобия модельных распределений с учётом признаков.

Для каждого признака $j$ рассмотрим логарифмическое правдоподобие:
\[
L_j(w) = \sum_{y \in Y} \hat{p}_j(y) \log p_j(y \mid w).
\]
Максимизация $L_j(w)$ по $w$ стремится сделать модельные оценки распределения классов по признаку $b_j$ максимально близкими к эмпирическим оценкам. 

Объединяя это с исходным критерием $Q(w)$, получаем:
\[
Q(w) + \gamma \sum_{j=1}^m L_j(w) = \sum_{i=1}^k \log P(y_i \mid x_i, w) 
- \frac{1}{2C}\sum_{y \in Y}\|w_y\|^2 +
\]
\[
+ \gamma \sum_{j=1}^m \sum_{y \in Y} \hat{p}_j(y)\log\left(\sum_{i=k+1}^{\ell} b_j(x_i)P(y \mid x_i, w)\right) \;\; \to \max_w.
\]

Здесь $\gamma > 0$ – коэффициент, отвечающий за степень влияния регуляризации XR. Итоговый критерий учитывает как размеченные данные (первое слагаемое), так и неразмеченные данные через эмпирические распределения по признакам (слагаемое с $L_j(w)$).

Оптимизация проводится методом стохастического градиента, обновляя параметры $w$ итеративно.

\subsection{Особенности и преимущества метода XR.}

Метод XR обладает рядом преимуществ и особенностей:

1. XR – это метод частичного обучения, но он \textbf{не} основан на кластеризации. Вместо этого он стремится согласовать распределения классов, предсказываемые моделью, с эмпирическими распределениями по признакам, оцененными по ограниченному набору размеченных данных.

2. Оптимизация осуществляется методом стохастического градиента, что обеспечивает масштабируемость на большие объемы данных.

3. Выбор признаков $b_j(x)$:

--- Если $b_j(x) \equiv 1$ для всех $x$, тогда $p_j(y \mid b_j(x)=1)$ интерпретируется как априорная вероятность класса $y$. Данная модель называется \textit{label regularization}, она особенно полезна в задачах с несбалансированными классами.

--- Если $b_j(x)$ указывает на присутствие или отсутствие конкретного термина $j$ в тексте $x$, то он подходит для классификации текстовых данных, так как учитывает распределение классов по характерным признакам.

4. Метод XR слабо чувствителен к выбору гиперпараметров $C$ и $\gamma$, устойчив к погрешностям в оценках $\hat{p}_j(y)$ и не требует большого числа размеченных объектов $k$.

5. XR хорошо работает в задачах категоризации текстов, где есть много неразмеченных данных, а получение полного набора разметки обходится слишком дорого или занимает много времени.

\subsection{Задачи.}

\bigskip

\noindent\textbf{Задача 1. Вычисление градиента XR-члена функционала}

Рассмотрим XR-член функционала:
\[
F_{\text{XR}}(w) = \gamma \sum_{j=1}^m \sum_{y \in Y} \hat{p}_j(y) \log\left(\sum_{x \in U_j} P(y \mid x,w)\right),
\]
где $U_j = \{ x_{k+1}, \ldots, x_{\ell} \mid b_j(x)=1 \}$ — подмножество неразмеченных объектов, для которых признак $b_j$ активен, и $\hat{p}_j(y)$ — эмпирическое распределение классов по признаку $b_j$.

Найдите градиент $F_{\text{XR}}(w)$ по параметрам $w_y$.

\subsubsection*{Решение:}

Обозначим
\[
A_{j,y}(w) = \sum_{x \in U_j} P(y \mid x,w).
\]

Тогда
\[
F_{\text{XR}}(w) = \gamma \sum_{j=1}^m \sum_{y \in Y} \hat{p}_j(y) \log A_{j,y}(w).
\]

Частная производная по $w_y$:
\[
\nabla_{w_y} F_{\text{XR}}(w) = \gamma \sum_{j=1}^m \sum_{y' \in Y} \hat{p}_j(y') \frac{1}{A_{j,y'}(w)} \sum_{x \in U_j} \nabla_{w_y}P(y' \mid x,w).
\]

Для логистической регрессии:
\[
\nabla_{w_y}P(y' \mid x,w) = P(y' \mid x,w)(\mathbb{I}[y'=y]-P(y \mid x,w))x.
\]

Подставляя в выражение:
\[
\nabla_{w_y} F_{\text{XR}}(w) = \gamma \sum_{j=1}^m \sum_{y' \in Y} \hat{p}_j(y') \frac{\sum_{x \in U_j} P(y' \mid x,w)(\mathbb{I}[y'=y]-P(y \mid x,w))x}{A_{j,y'}(w)}.
\]

Таким образом, мы получили формулу для градиента по параметрам $w_y$ от XR-члена функционала.

\bigskip

\noindent\textbf{Задача 2. Смешанное влияние априорных предположений и неразмеченных данных}

Пусть мы имеем априорное предположение о распределении классов (например, мы знаем, что класс $y=1$ встречается в 70\% случаев), а также большие неразмеченные данные $U$. Объясните, как совместный учёт априорной информации и структуры неразмеченного множества может улучшить точность классификации по сравнению с использованием только одного из этих источников информации.

\subsubsection*{Решение:}
Априорное предположение о распределении классов помогает задать общий «баланс» между классами и избежать смещения в пользу редких классов. Однако без знания структуры данных оно может быть неточно применено. Неразмеченные данные дают понимание структуры распределения: где классы могут располагаться и как данные группируются. Совместное использование: априорная вероятность подсказывает, какие классы более вероятны в целом, а структура $U$ уточняет, где должна проходить граница, чтобы согласовать эти вероятности с реальным расположением точек. В итоге мы получаем более точный классификатор, чем если бы полагались исключительно на априор или только на структуру $U$.

\bigskip

\noindent\textbf{Задача 3. Интерпретация XR через эмпирические распределения}

Пусть у нас есть многоклассовая логистическая регрессия с параметрами $w$ и небольшой набор размеченных данных $X^k$. Для каждого бинарного признака $b_j(x)$ по размеченной части мы можем оценить эмпирическое распределение классов $\hat{p}_j(y)$. Метод XR добавляет к исходному критерию $Q(w)$ дополнительное слагаемое:
\[
\gamma \sum_{j=1}^m \sum_{y \in Y} \hat{p}_j(y) \log p_j(y \mid w),
\]

Объясните, почему максимизация совокупной функции стремится «подтянуть» модельные вероятности $p_j(y \mid w)$ к эмпирическим оценкам $\hat{p}_j(y)$. В чём заключается смысл регуляризатора XR с точки зрения использования информации о неразмеченных данных?

\subsubsection*{Решение:} 
Регуляризатор стремится сделать так, чтобы предсказания по неразмеченным данным были согласованы с эмпирическими частотами классов по признакам, оцененными на размеченных данных. Это означает, что модель не просто настраивается по размеченным объектам напрямую, но и пытается воспроизводить выявленные «шаблоны» вероятностей классов, используя набор неразмеченных объектов для уточнения вероятностных оценок. 

С точки зрения дивергенции Кульбака-Лейблера (KL-дивергенции), это дополнение минимизирует расхождение между эмпирическими распределениями $\hat{p}_j(y)$ и модельными предсказаниями $p_j(y \mid w)$. KL-дивергенция измеряет, насколько распределение $p_j(y \mid w)$ плохо аппроксимирует эмпирическое распределение $\hat{p}_j(y)$, и штрафует модель за это несоответствие. Таким образом, минимизация KL-дивергенции приводит к «подтягиванию» вероятностей модели к эмпирическим данным.

Смысл регуляризатора XR заключается в том, чтобы использовать информацию о неразмеченных данных для уточнения вероятностных оценок, согласованных с эмпирическими закономерностями, обнаруженными в размеченной выборке. Это снижает риск переобучения, улучшает обобщающую способность модели и позволяет использовать априорные знания для улучшения её предсказаний.

\section*{K-means}
\title{Конспект семинара по теме K-means}


\subsection*{Основные понятия}
\textbf{Обучение без учителя:} анализ данных без меток, задача — выявить скрытые структуры.\\
\textbf{Кластеризация:} разделение объектов на группы (кластеры), где объекты внутри группы схожи.\\
\textbf{K-means:}
\begin{itemize}
    \item Делит данные на $K$ кластеров.
    \item Использует евклидово расстояние для расчета близости.
    \item Основные этапы:
    \begin{enumerate}
        \item Инициализация центроидов.
        \item Назначение объектов к ближайшим центроидам.
        \item Пересчет центроидов.
        \item Повторение до сходимости.
    \end{enumerate}
\end{itemize}

\subsection*{Построение данных}
\begin{itemize}
    \item Используется распределение Дирихле (\texttt{np.random.dirichlet}) с добавлением шума (\texttt{np.random.randn}).
    \item Генерируются два набора данных ($X_1$ и $X_2$), объединенные в $X$.
    \item Визуализация данных:
    \[
    \texttt{plt.plot(X[:, 0], X[:, 1], '.', color=colors[0])}
    \]
\end{itemize}

\subsection*{Пример работы K-means}
\textbf{Шаги эксперимента:}
\begin{enumerate}
    \item Инициализация модели:
    \[
    \texttt{model = KMeans(n\_clusters=n, random\_state=42)}
    \]
    \item Обучение модели:
    \[
    \texttt{model.fit(X)}
    \]
    \item Визуализация кластеров:
    \begin{itemize}
        \item Точки кластера:
        \[
        \texttt{plt.plot(X[model.labels\_ == i, 0], X[model.labels\_ == i, 1], '.', color=colors[i])}
        \]
        \item Центры кластеров:
        \[
        \texttt{plt.plot([model.cluster\_centers\_[i][0]], [model.cluster\_centers\_[i][1]], 'x', c=colors[i], markersize=20)}
        \]
    \end{itemize}
\end{enumerate}

\textbf{Результаты:}
\begin{itemize}
    \item Запуск алгоритма с разным числом кластеров ($n\_clusters = 2, 4, 8$).
    \item При увеличении числа кластеров разделение становится более детальным.
\end{itemize}

\subsection*{Итоги и обсуждение}
\textbf{Преимущества K-means:}
\begin{itemize}
    \item Простота и скорость.
    \item Эффективность при небольшом числе кластеров.
\end{itemize}
\textbf{Недостатки K-means:}
\begin{itemize}
    \item Зависимость от начальной инициализации.
    \item Чувствительность к шуму и выбросам.
\end{itemize}
\textbf{Применение:} маркетинг, медицина, анализ поведения пользователей и др.



\subsection*{Задачи}

\textbf{Задача 1:} \textit{Предположим, у вас есть данные:} 
\[
X = \left\{
\begin{array}{cc}
(1, 2), & (2, 3), \\
(6, 7), & (7, 8)
\end{array}
\right\}.
\]
\textit{Используя K-means, разделите эти данные на 2 кластера.}

\textbf{Решение:}  
Алгоритм начнется с случайных центроидов. После первого шага, кластеры могут быть следующими:
- Кластер 1: $(1, 2), (2, 3)$
- Кластер 2: $(6, 7), (7, 8)$

Итоговые центры кластеров:
- Центр кластера 1: $(1.5, 2.5)$
- Центр кластера 2: $(6.5, 7.5)$

---

\textbf{Задача 2:} \textit{Даны три точки: $(1, 1)$, $(2, 2)$ и $(6, 6)$. Используя K-means с $n=2$, определите, как они будут разделены на два кластера.}

\textbf{Решение:}  
- Инициализация центроидов: случайно выбираем $(1, 1)$ и $(6, 6)$.
- Первый шаг:
  - Точка $(1, 1)$ ближе к центроиду $(1, 1)$.
  - Точка $(2, 2)$ ближе к центроиду $(1, 1)$.
  - Точка $(6, 6)$ ближе к центроиду $(6, 6)$.
- После пересчета центроидов:
  - Центр кластера 1: $(1.5, 1.5)$
  - Центр кластера 2: $(6, 6)$
  
Таким образом, точки $(1, 1)$ и $(2, 2)$ будут в одном кластере, а точка $(6, 6)$ в другом.

---

\textbf{Задача 3:} \textit{Предположим, у вас есть 6 точек: $(1, 2)$, $(2, 3)$, $(3, 4)$, $(8, 9)$, $(9, 10)$, $(10, 11)$. Используя K-means с $n=2$, как будет происходить кластеризация?}

\textbf{Решение:}  
1. Инициализация центроидов: предположим, что начальные центры $(1, 2)$ и $(10, 11)$.
2. После первого шага:
   - Точки $(1, 2)$, $(2, 3)$, $(3, 4)$ ближе к центроиду $(1, 2)$.
   - Точки $(8, 9)$, $(9, 10)$, $(10, 11)$ ближе к центроиду $(10, 11)$.
3. Пересчитаем центроиды:
   - Центр кластера 1: $(2, 3)$
   - Центр кластера 2: $(9, 10)$

Таким образом, кластер 1 содержит точки $(1, 2)$, $(2, 3)$, $(3, 4)$, а кластер 2 — точки $(8, 9)$, $(9, 10)$, $(10, 11)$.

\section{Агломеративная иерархическая кластеризация Ланса-Уильямса}

Агломеративная иерархическая кластеризация — это метод, в котором кластеры последовательно объединяются в более крупные группы. В 1967 году Ланс и Уильямс предложили обобщенную формулу для пересчёта расстояний между кластерами при их объединении.

\subsection{Описание алгоритма}
\begin{enumerate}
    \item Инициализация: каждый объект образует свой собственный кластер $C_1 = \{\{x_1\}, \dots, \{x_\ell\}\}$.
    \item Вычисляются расстояния между всеми парами кластеров: 
    \[
    R_{\{x_i\}\{x_j\}} = \rho(x_i, x_j).
    \]
    \item На каждой итерации $t = 2, \dots, \ell$:
    \begin{itemize}
        \item Найти пару кластеров $(U, V)$ с минимальным расстоянием $R_{UV}$, \textbf{при условии, что в $U \cup V$ нет объектов с разными метками}.
        \item Слить их в один кластер $W = U \cup V$:
        \[
        C_t := C_{t-1} \cup \{W\} \setminus \{U, V\}.
        \]
        \item Для всех остальных кластеров $S \in C_t$ пересчитать расстояния $R_{WS}$ по формуле Ланса–Уильямса:
        \[
        R_{WS} := \alpha_U R_{US} + \alpha_V R_{VS} + \beta R_{UV} + \gamma |R_{US} - R_{VS}|,
        \]
        где $\alpha_U, \alpha_V, \beta, \gamma$ — параметры, определяющие конкретный случай алгоритма.
    \end{itemize}
\end{enumerate}


\subsection{Частные случаи формулы Ланса–Уильямса}
Формула Ланса–Уильямса обобщает несколько известных способов пересчёта расстояний.

\begin{enumerate}
    \item \textbf{Расстояние ближайшего соседа:}
    \[
    R^{б}_{WS} = \min_{w \in W, s \in S} \rho(w, s);
    \]
    параметры:
    \[
    \alpha_U = \alpha_V = \frac{1}{2}, \; \beta = 0, \; \gamma = -\frac{1}{2}.
    \]

    \item \textbf{Расстояние дальнего соседа:}
    \[
    R^{д}_{WS} = \max_{w \in W, s \in S} \rho(w, s);
    \]
    параметры:
    \[
    \alpha_U = \alpha_V = \frac{1}{2}, \; \beta = 0, \; \gamma = \frac{1}{2}.
    \]

    \item \textbf{Групповое среднее расстояние:}
    \[
    R^{г}_{WS} = \frac{1}{|W||S|} \sum_{w \in W} \sum_{s \in S} \rho(w, s);
    \]
    параметры:
    \[
    \alpha_U = \frac{|U|}{|W|}, \; \alpha_V = \frac{|V|}{|W|}, \; \beta = \gamma = 0.
    \]

    \item \textbf{Расстояние между центрами:}
    \[
    R^{ц}_{WS} = \rho^2 \left( \sum_{w \in W} \frac{w}{|W|}, \; \sum_{s \in S} \frac{s}{|S|} \right);
    \]
    параметры:
    \[
    \alpha_U = \frac{|U|}{|W|}, \; \alpha_V = \frac{|V|}{|W|}, \; \beta = -\alpha_U \alpha_V, \; \gamma = 0.
    \]

    \item \textbf{Расстояние Уорда:}
    \[
    R^{y}_{WS} = \frac{|S||W|}{|S| + |W|} \rho^2 \left( \sum_{w \in W} \frac{w}{|W|}, \; \sum_{s \in S} \frac{s}{|S|} \right);
    \]
    параметры:
    \[
    \alpha_U = \frac{|S| + |U|}{|S| + |W|}, \; \alpha_V = \frac{|S| + |V|}{|S| + |W|}, \; \beta = \frac{-|S|}{|S| + |W|}, \; \gamma = 0.
    \]
\end{enumerate}

\subsection{Проблема выбора}
Одной из задач в агломеративной кластеризации является выбор функции расстояния. Разные варианты расстояний по-разному обрабатывают данные и влияют на результат кластеризации. В зависимости от структуры данных, конкретная функция может быть более или менее подходящей.

\subsection{Рекомендации по выбору и числу кластеров}
\begin{itemize}
    \item Рекомендуется пользоваться расстоянием Уорда $R^{y}$.
    \item Обычно строят несколько вариантов кластеризации и выбирают лучший визуально по дендрограмме.
    \item Определение числа кластеров осуществляется по максимуму разности расстояний на соседних итерациях:
    \[
    \max |R_{t+1} - R_t|,
    \]
    где $C_t$ — результирующее множество кластеров.
\end{itemize}


\subsection*{Вопросы для самопроверки}
\begin{enumerate}
    \item В чём заключается основная идея формулы Ланса–Уильямса и как она используется для пересчёта расстояний между кластерами?
    \item Какие частные случаи формулы Ланса–Уильямса вы знаете и чем они отличаются друг от друга?
    \item Как определяется число кластеров в агломеративной кластеризации на основе дендрограммы и разностей расстояний?
\end{enumerate}

\section*{Постановка и актуальность неразмеченных данных}
Сейчас данных много. Модели стали сложными. В глубоком обучении также пытаются использовать неразмеченные данные. Частичное обучение является компромиссом между обучением без учителя (без каких-либо размеченных обучающих данных) и обучением с учителем (с полностью размеченным набором обучения). \par 
Было замечено, что неразмеченные данные, будучи использованными совместно с небольшим количеством размеченных данных, могут обеспечить значительный прирост качества обучения. \par 
Сбор размеченных данных для задачи обучения зачастую требует, чтобы квалифицированный эксперт вручную классифицировал объекты обучения. Затраты, связанные с процессом разметки, могут сделать построение полностью размеченного набора прецедентов невозможным, в то время как сбор неразмеченных данных сравнительно недорог. В подобных ситуациях ценность частичного обучения сложно переоценить.

Общий подход к данной задаче следующий:
\begin{itemize}
    \item $U \subset X$ - большая неразмеченная выборка.
    \item $ D \subset X \times Y$ - небольшая размеченная выборка.
    \item Требуется построить отображение $f : X \rightarrow Y$
\end{itemize}
\section*{Алгоритм DBSCAN}
В дополнение к определениям выше зададим определение эпсилон-окрестности элемента $x$ неразмеченной выборки $U$: $U_{\varepsilon}(x) = \{ u \in U: \rho(x, u) \leq \varepsilon \}$. Разделим все объекты на 3 типа:
\begin{itemize}
    \item корневой: имеющий плотную окрестность, $U_{\varepsilon}(x) \geq m$
    \item граничный: не корневой, но в окрестности корневого
    \item шумовой (выброс): не корневой и не граничный
\end{itemize}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.47\linewidth]{objects.png}
    \caption{Слева направо: корневые, граничные, шумовые объекты и кластеризация}
    \label{fig:enter-label}
\end{figure}
\par
Теперь рассмотрим, собственно, сам алгоритм кластеризации:
\begin{itemize}
    \item Вход: выборка $X^{l} = \{ x_1, ..., x_l\}$; параметры $\varepsilon$ и $m$.
    \item Выход: разбиение выборки на кластеры и шумовые выбросы; $U := X^{l}$ - непомеченные объекты, $a = 0$.
    \item Пока в выборке есть непомеченные точки, то есть $U \neq \emptyset$:
    \begin{itemize}
        \item Берем случайную точку $x \in U$
        \item Если $U_{\varepsilon}(x) < m$, то пометить $x$ как, возможно, шумовой.
        \item Иначе:
        \begin{itemize}
            \item Создаем новый кластер $K := U_{\varepsilon}(x)$; $a = a + 1$
            \item Для всех $x' \in K$, не помеченных или шумовых:
            \begin{itemize}
                \item Если $U_{\varepsilon}(x) \geq m$, то $K := K \cup U_{\varepsilon}(x')$
                \item Иначе помечаем $x'$ как граничный кластера $K$ 
            \end{itemize}
            \item $a_i := a$ для всех $x_i \in K$
            \item $U = U \backslash K$
        \end{itemize}
    \end{itemize}
\end{itemize}
\section*{Преимущества и недостатки алгоритма DBSCAN}
\textbf{Начнем с самых важных преимуществ алгоритма:}
\begin{itemize}
    \item Быстрая кластеризация больших данных: асимптотика в худшем случае $O(l^2)$ и $O(l \log l)$ при эффективной реализации $U_{\varepsilon}(x)$;
    \item Кластеры могут иметь произвольную форму, что часто бывает полезно;
    \item Деление объектов на корневые, граничные и шумовые. Пример такого деления показан на рисунке 2.
\end{itemize}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.47\linewidth]{clusters.png}
    \caption{Пример деления объектов на корневые, граничные и шумовые}
    \label{fig:enter-label}
\end{figure}
\textbf{Однако, как и у любого алгоритма, у DBSCAN есть свои недостатки, а именно:}
\begin{itemize}
    \item DBSCAN не полностью однозначен — краевые точки, которые могут быть достигнуты из более чем одного кластера, могут принадлежать любому из этих кластеров, что зависит от порядка просмотра точек. Однако для большинства наборов данных эти ситуации возникают редко и имеют малое влияние на результат кластеризации, так как основные точки и шум DBSCAN обрабатывает однозначно.
    \item Качество DBSCAN зависит от способа измерения расстояния между объектами. Наиболее часто используемой метрикой расстояний является евклидова метрика. Однако иногда, особенно для кластеризации данных высокой размерности, эта метрика может оказаться не очень хорошей ввиду экспоненциального роста необходимых экспериментальных данных в зависимости от размерности пространства
    \item DBSCAN не может хорошо кластеризовать наборы данных с большой разницей в плотности, так как в таком случае сложно подобрать параметр $m$;
    \item Если данные и масштаб не вполне хорошо поняты, выбор осмысленного параметра $\varepsilon$ может оказаться трудным.
\end{itemize}
\section*{Задачи}
\subsection*{Задача 1}
В каких случаях использование DBSCAN может быть предпочтительнее других алгоритмов кластеризации, таких как K-means?
\newline $ $ \newline
\textit{Решение:} DBSCAN предпочтительнее в случаях, когда данные содержат кластеры произвольной формы и/или когда данные содержат шум. В отличие от K-means, который требует заранее заданного числа кластеров и не учитывает выбросы, DBSCAN может обнаруживать кластеры любой формы (например, эллиптические или извилистые) и автоматически выделять шумовые точки (которые не принадлежат ни к одному кластеру). K-means также чувствителен к выбору начальных центров, а DBSCAN не зависит от инициализации и может успешно работать на данных с переменной плотностью.
\subsection*{Задача 2}
Как изменение параметров $\varepsilon$ и $m$ влияет на результаты кластеризации в DBSCAN?
\newline $ $ \newline
\textit{Решение:} Параметр $\varepsilon$ определяет радиус окрестности для поиска соседей. Если $\varepsilon$ слишком мал, то алгоритм будет воспринимать множество точек как шум и не сможет выделить кластеры, так как для большинства точек не будет достаточного количества соседей. Если $\varepsilon$ слишком велико, то кластеризация может стать слишком грубой, объединив слишком много точек в один кластер, включая выбросы.

Параметр $m$ контролирует минимальное количество соседей, необходимое для того, чтобы точка могла стать центром кластера. Если $m$ слишком велико, то алгоритм может не найти никакие кластеры, так как большинство точек не будут иметь достаточно соседей для формирования кластеров. Если $m$ слишком мало, DBSCAN может ошибочно выделить слишком много маленьких кластеров, что приведет к разбиению данных на множество неинформативных кластеров.

Таким образом, для достижения оптимальных результатов необходимо настроить оба параметра в зависимости от особенностей данных.

\subsection*{Задача 3}
Как изменение масштаба данных влияет на работу алгоритма DBSCAN? Почему важно нормализовать или стандартизировать данные перед применением DBSCAN?
\newline $ $ \newline
\textit{Решение:} Алгоритм DBSCAN использует расстояния между точками для определения их близости, а также для вычисления плотности точек в окрестности (радиус $\varepsilon$). Если данные имеют разные масштабы по различным признакам, то алгоритм будет склонен учитывать признак с более крупным масштабом как более важный при расчете расстояний. Это приведет к искажению результатов кластеризации, так как DBSCAN будет ошибочно воспринимать различия в признаках с большими величинами как более значимые.

Для решения этой проблемы важно нормализовать или стандартизировать данные перед применением DBSCAN:

Нормализация приводит все признаки к одному масштабу, например, в диапазон [0, 1], что позволяет одинаково учитывать все признаки.
Стандартизация изменяет данные так, чтобы каждый признак имел нулевое среднее значение и единичное стандартное отклонение, что также помогает привести данные к одинаковому масштабу.
Если этого не сделать, то один из признаков может доминировать при расчете расстояний, что приведет к неверному определению плотности и кластеров. В результате DBSCAN может либо не обнаружить какие-то кластеры, либо объединить точки, которые не должны быть частью одного кластера.

Таким образом, перед применением DBSCAN важно привести данные к одному масштабу, чтобы алгоритм корректно учитывал все признаки и правильно разделял кластеры.
