\section*{Методы Ньютона-Рафсона, Ньютона-Гаусса}

Мы уже познакомились с задачами линейной регрессии и обсудили несколько методов их решения. Но что делать если задача нелинейна? Оказывается, что идея локальной линейности гладкой функций позволяет свести задачу к более простому. На этой идее основаны методы второго порядка - Ньютона-Рафсона и Ньютона-Гаусса.

\subsection*{Метод Ньютона-Рафсона}

Начнем немного сдалека, а именно рассмотрим задачу поиска нуля $a$ функций $f(x)$. Пусть мы находимся в точке $a^{0}$ и хотим найти такое приращение $h$, чтобы приблизиться к точке $a$: $a^{0} + h \approx a$. Применим разложение в ряд:

\begin{gather*}
    f(a^{0} + h) = f(a^{0}) + f'(a^{0})h + o(h)\\[1em]
    f(a^{0} + h) \approx f(a) = 0 \Rightarrow f(a^{0}) + f'(a^{0})h \approx 0
\end{gather*}

Откуда

\[
h \approx - \frac{f(a^{0})}{f'(a^{0})}
\]

Значит, для поиска нуля выпуклой функций \( f \) можно применить следующий итеративный метод:

\[
a^{k + 1} = a^{k} - \frac{f(a^{k})}{f'(a^k)}
\]

Вернемся к начальной задаче:

\begin{itemize}
    \item обучающая выборка \( X^{l} = (x_{i}, y_{i}) \), где \( x_{i} \) - вектор признаков \( i \)-го объекта
    \item \( y_{i} = y(x_{i}),\quad y: X \to Y \) - неизвестная регрессионная зависимость
    \item \( f(x, \alpha) \) - нелинейная модель регрессии, где \( \alpha \) - вектор параметров
\end{itemize}

Хотим решить задачу оптимизации методом наименьших квадратов:

\[
Q(\alpha, X^{l}) = \sum\limits_{i = 1}^{l}\left(f(x_{i}, \alpha) - y_{i} \right)^{2} \to \min_{\alpha}
\]

Функция потерь \( Q(\alpha, X^{l}) \) выпукла и гладкая в предположений гладкости \( f(\alpha, x) \), поэтому для ее минимизаций достаточно найти нуль производной (градиента). Применяя рассуждения выше и опустив технические детали, получим следующий итерационный процесс:

\[
\alpha^{k + 1} = \alpha^{k} - h_{k}\left(Q''( \alpha^{k} )\right)^{-1} Q'(\alpha^{k})
\]

где \( Q'( \alpha^{k} ) \), \( Q''(\alpha^{k} ) \) - градиент и гессиан \( Q \) в точке \( \alpha^{k} \) соответственно, \( h_{k} \) - величина шага.
\subsection*{Метод Ньютона-Гаусса}

Подсчет обратного гессиана на каждой итерации может дорого обходиться, поэтому посмотрим на полученный выше результат с другой стороны. Для этого запишем несколько формул:

Компоненты градиента \( Q(\alpha, X^{l}) \):

\[
\frac{\partial Q(\alpha, X^{l})}{\partial\alpha_{j}} = 2 \sum\limits_{i = 1}^{l} \left(f(x_{i}, \alpha) - y_{i} \right)\frac{\partial f(x_{i}, \alpha)}{\partial\alpha_{j}}
\]

Компоненты гессиана:

\[
\frac{\partial Q(\alpha, X^{l})}{\partial\alpha_{j}\partial\alpha_{k}} = 2\sum\limits_{i = 1}^{l}\frac{\partial f(x_{i}, \alpha)}{\partial\alpha_{j}}\frac{\partial f(x_{i}, \alpha)}{\partial\alpha_{k}} - 2\sum\limits_{i = 1}^{l}\left(f(x_{i}, \alpha) - y_{i}\right)\frac{\partial^2 f(x_{i}, \alpha)}{\partial\alpha_{j}\partial\alpha_{k}}
\]

Второе слагаемое в формуле выше полагается равным нулю, исходя из линейной аппроксимации функций \( f \). Введем следующие обозначения и перепишем формулу итерации метода Ньютона-Рафсона:

\begin{gather*}
    F_{k} = \left( \frac{\partial f}{\partial\alpha_{j}}(x_{i}, \alpha^{k}) \right)_{i, j}\\[1em]
    f_{k} = \left( f(x_{i}, \alpha^{k}) \right)_{i}
\end{gather*}

Получим:

\[
\alpha^{k + 1} = \alpha^{k} - h_{k}(F_{k}^{T}F_{k})^{-1}F_{k}^{T}(f_{k} - y)
\]

Положив \( \theta = (F_{k}^{T}F_{k})^{-1}F_{k}^{T}(f_{k} - y) \), получим решение задачи линейной регрессии, где новые ответы обучающей выборки -- \( \left(f_{k} - y \right) \), с новой матрицей признаков~--~\( F_{k} \). Таким образом, каждый шаг метода Ньютона-Гаусса сводится к задаче линейной регрессии. 

\section*{Задачи}

\subsection*{Задача 1: Локальная сходимость}

Найдите допустимые значения начального приближения для поиска нуля функции \( f(x) = \frac{x}{\sqrt{1 + x^{2}}} \).

\textbf{Решение:}

Нуль функции \( f \) достигается в точке \( a = 0\). Посчитаем производную \( f \):

\[
f'(x) = \left(\frac{x}{\sqrt{1 + x^{2}}}\right)' = \frac{1}{\sqrt{1 + x^{2}}} - x\cdot\frac{2x}{2(1 + x^{2})^{3/2}} = \frac{1}{(1 + x^{2})^{3/2}}
\]

Распишем формулу итерации метода Ньютона:

\[
a^{k + 1} = a^{k} - \frac{f(a^{k})}{f'(a^{k})} = -(a^{k})^3
\]

Отсюда видно, что сходимость есть при \( |a^{0}| < 1 \). Делаем вывод о важности выбора начального приближения в данном методе.

\subsection*{Задача 2: Квадратичная задача}

Как сработает метод Ньютона-Рафсона для поиска минимума задачи \( f(x)~=~x^{T}Ax + bx + c\),  где $x, b \in \mathbb{R}^{n}$, $A$ - симметричная, положительно определенная матрица.

\textbf{Решение:}

Для поиска минимума нужно найти нуль градиента. Это и будет точкой минимума, так как задача выпукла.
Посчитаем градиент и гессиан:

\begin{gather*}
    \nabla f(x) = Ax + b\\[1em]
    \nabla^{2} f(x) = A
\end{gather*}

Пусть \( x^{0} \) начальная точка. Применяя формулу из метода Ньютона-Рафсона получим:

\[
x^{1} = x^{0} - \left(\nabla^{2} f(x^{0})\right)^{-1}\nabla f(x^{0}) = x^{0} - A^{-1}\left(Ax^{0} + b\right) = -A^{-1}b
\]

но с другой стороны, градиент обращается в нуль в этой точке:

\[
\nabla f(x) = 0 = Ax + b \Rightarrow x = -A^{-1}b
\]


Значит для квадратичной задачи данный метод дает ответ за 1 шаг. Вообще говоря, если функция $\mu$-выпукла и имеет $M$-липшецевый гессиан, то скорость сходимости локально квадратична.

\subsection*{Задача 3: Система уравнений}

Составьте алгоритм решения следующей системы с помощью методов второго порядка:

\[
\begin{cases}
x^{2} + y^{2} = 4\\
y = e^{x}
\end{cases}
\]

\textbf{Решение:}

Нужно найти нули следующей функции от двух переменных
\[
F(x, y) = 
\begin{pmatrix}
x^{2} + y^{2} - 4 \\
y - e^x
\end{pmatrix}
\]

Запишем итерацию метода Ньютона-Рафсона:

\[
x^{k + 1} = x^{k} - J_{F}(x^{k})^{-1}F(x^{k})
\]

Его можно записать как:

\[
J_{F}(x^{k})(x^{k + 1} - x^{k}) = -F(x^{k})
\]

Посчитаем матрицу якоби функции $F$:

\[
J_{F}(x) = 
\begin{pmatrix}
2x   & 2y \\
-e^x & 1
\end{pmatrix}
\]

Тогда на каждом шаге нужно решить следующую систему:
\[
\begin{pmatrix}
    2x^{k} & 2(y^{k})^{2} \\
    -e^{x^{k}} & 1 
\end{pmatrix}
\begin{pmatrix}
    c_{1}^{k + 1}  \\
    c_{2}^{k + 1}
\end{pmatrix} 
=
\begin{pmatrix}
    (x^{k})^2 + (y^{k})^2 - 4\\
    (y^{k}) - e^{x^{k}}
\end{pmatrix} 
\]

где \( c^{k + 1} = -(x^{k + 1} - x^{k})\). Тогда для решения данной задачи нужно взять начальную точку и проделать несколько итераций описанных уравнениями выше.

\section{Обобщенная аддитивная модель}

Обобщённые аддитивные модели (Generalized Additive Models, GAM) предоставляют подход к моделированию данных, который обобщает линейную регрессию за счёт введения нелинейных преобразований отдельных признаков. Основная цель GAM — построить модель, в которой линейная структура сохраняется, но при этом каждый признак может вносить свой вклад через нелинейную функцию преобразования.

\
\textbf{Пример: прогнозирование стоимости недвижимости}

Стоимость квартиры может зависеть от множества факторов: площади, количества комнат, наличия балкона, удалённости от метро и других. Некоторые признаки, такие как площадь, являются числовыми, другие, например наличие балкона, — бинарными. Возникает вопрос, как корректно обработать такой признак, как расстояние до метро. Интуитивно понятно, что стоимость квартиры снижается с увеличением расстояния, но эта зависимость может быть нелинейной. Простое включение признака в линейную модель может не учесть этой особенности, так как предполагает постоянный коэффициент (обычно отрицательный) для расстояния до метро.

Таким образом, чтобы учесть сложную зависимость, расстояние до метро можно преобразовать через некоторую нелинейную функцию $\phi(x)$, которая будет отражать реальный характер этой связи. GAM позволяет обучить эту функцию непосредственно из данных.


Обобщённая аддитивная модель представляется в виде:
$$f(x, \alpha) = \sum_{j=1}^{n}\phi_j(f_j(x), \alpha_j)$$
В частности, при $\phi_j(f_j(x), \alpha_j) = \alpha_jf_j(x)$ это линейная модель.

Идея состоит в том, чтобы поочередно обучать параметры $\phi_j$ при фиксированных остальных. На каждом шаге получается задача обучения одномерной модели регрессии. Также добавлен регуляризатор $R(\alpha_j)$.
$$\sum_{i=1}^{l}(\phi_j(f_j(x_i), \alpha_j) - (y_i - \sum_{k \neq n}\phi_k(f_k(x_i), \alpha_k))^2 + tR(\alpha_i) \to min_{a_i}$$

$R$ - регуляризатор гладкости:  

$$R(\alpha_i) = \int(\phi_j^{''}(\zeta, \alpha_j)^2d\zeta$$

Для оптимизации регуляризаторов используют такие методы как сплайны или ядерное сглаживание 


\subsection{Задача 1}
Объясните, зачем в обобщённых аддитивных моделях (GAM) используется регуляризация для функции $\phi_j(x)$:
\flushleft
\begin{enumerate}
\item Регуляризация предотвращает переобучение модели, штрафуя за чрезмерно сложные функции $\phi_j(x)$. 

\item Регуляризация ускоряет процесс обучения модели, уменьшив количество данных, которые необходимо обработать.
\item Регуляризация заставляет модель использовать только линейные функции для преобразования признаков.
\item Регуляризация повышает точность прогноза, исключая менее важные признаки из модели.
\begin{description}
\end{description}
\end{enumerate}
Ответ: 1

\subsection{Задача 2}

В обобщённой аддитивной модели (GAM) имеется следующая формула:

\[
y = \alpha_0 + \sum_{j=1}^{n} \phi_j(x_j) + \epsilon
\]

где \( \phi_j(x_j) \) — это нелинейные функции преобразования признаков.


\textbf{Вопрос:} Если одна из функций преобразования \( \phi_j(x_j) \) — это просто линейная функция, то какую роль она выполняет в модели? И что произойдёт, если такую функцию исключить из модели?

\textbf{Решение:}
Тогда это эквивалентно линейной регрессии. Если исключить признак из модели, то его влияние будет упущено, что пможет привести к потери важной информации.





\subsection{Задача 3}
Для данных с признаками:
\[
x_1 (\text{площадь}),x_2 (\text{количество комнат}),x_3 (\text{расстояние до метро})
\]
известна модель:
\[
y = 50 + \phi_1(x_1) + \phi_2(x_2) + \phi_3(x_3) + \varepsilon,
\]
где \(x_1 \sim U(20, 100)\), \(x_2 \sim \{1, 2, 3, 4\}\), \(x_3 \sim U(0.5, 10)\).

Постройте GAM для оценки $\phi_i(x)$ и оцените, какой признак вносит наибольший вклад в стоимость.





\section{Backfitting}

На практике встречаются задачи, в которых использование линейных моделей необосновано, но и не удаётся предложить явную нелинейную модель. В таком случае строится модель вида

$$\displaystyle y(x)=f(x,\theta)=\sum_{j=1}^m\theta_j \phi_j(x_j),$$

где $x$ ~--- объект, $x_j$ ~--- признаки объекта, $\phi_j:\mathbb{R}\rightarrow \mathbb{R}$ ~--- нелинейные в общем случае преобразования.

Задача состоит в том, чтобы одновременно подбирать коэффициенты модели $\theta_j$ и неизвестные преобразования $\phi_j$.

Суть метода backfitting (метод настройки с возвращением) заключается в чередовании оптимизации коэффициентов $\theta_j$ при постоянных $\phi_j$ методами линейной регрессии, и оптимизации преобразований $\phi_j$ при постоянных коэффициентах $\theta_j$. 

Для минимизации используется сумма квадратов ошибок на всех объектах:

$Q(\theta, \phi) = \displaystyle \sum_{i=1}^l \left( y(x_i) - y_i \right)^2 = \sum_{i=1}^l \left( \sum_{j=1}^m \theta_j \phi_j(X_{ij}) - y_i \right)^2 $

Здесь $X$ ~--- матрица признаков.

\subsection{Алгоритм backfitting}
\begin{algorithmic}
\State $\phi_j(t) \equiv t$ ~--- изначальное приближение линейными функциями
\While{($Q$ уменьшается)} 
    \State $\theta \gets \text{argmin}_{\theta}Q(\theta, \phi_j)$ ~--- при фиксированных $\phi_1,\dots, \phi_k$
    \For{$j=1\dots m$}
        \State $\displaystyle r_i = y_i - \sum_{k\neq j} \theta_k \phi_k(X_{ik})$
        \State $r_i$ ~--- ошибка модели на $i$ объекте, без учёта $j$ признака
        \State $\displaystyle\phi_j \gets \text{argmin}_{\phi} \sum_{i=1}^l \left( \theta_j \phi(X_{ij}) - r_i \right)^2$ ~--- при $\theta_j=const$
    \EndFor
\EndWhile 
\end{algorithmic}

Оптимизация по коэффициентам $\theta \gets \text{argmin}_{\theta}Q(\theta, \phi_j)$ выполняется методами линейной регрессии, такими как стохастический градиентный спуск.

Оптимизация по функции $\phi_j$ выполняется одномерными методами, такими как ядерное сглаживание.

Варианты дальнейшего развития метода:
\begin{itemize}
    \item [1.] Во внутреннем цикле выбирать индексы $j$ не в фиксированном порядке, а в первую очередь оптимизировать дающие наибольший вклад в $Q$.
    \item [2.] Регуляризация $Q$ по параметрам $\theta$ и по сложности функций $\phi$.
\end{itemize}

\subsection{Задача}

Если матрица признаков $X$ разреженная, с какой проблемой можно столкнуться, применяя алгоритм backfitting, и как её решить?

Решение: если каждая из функций $\phi_j$ вызывается лишь на небольшом наборе аргументов, то её значения на них могут быть почти независимы (если модель $\phi$ достаточно сложна, например, полином высокого порядка), и случится переобучение. Варианты решения: большая константа регуляризации по сложности функций $\phi$; ограничить $\phi$ более простым классом функций (например, только квадратичные функции вместо полиномов); расширить матрицу признаков несколькими простыми нелийными преобразованиями ($\sin x$, $x^2$...) и использовать методы линейной регрессии.

\section{Метод наименьших квадратов с итеративным пересчётом весов (IRLS)}

\textbf{Метод наименьших квадратов с итеративным пересчётом весов} (Iteratively Reweighted Least Squares) применяется для решения задач оптимизации. В частности, применение метода Ньютона-Рафсона к задаче \textit{Логистической регрессии} сводится к \textbf{IRLS}.

Напомним постановку задачи линейной регрессии. Будем искать приближенное решение системы:

\[ \begin{bmatrix} 
    a_{11} & a_{12} & \dots  & a_{1N} \\
    \vdots & \vdots & \ddots & \\
    a_{M1} & \dots  & \dots  & a_{MN}
\end{bmatrix} \begin{bmatrix} 
    x_{1} \\
    \vdots \\
    x_{M} \\ 
\end{bmatrix}
=
\begin{bmatrix} 
    y_{1} \\
    \vdots \\
    y_{M} \\
\end{bmatrix} \]

То же самое в матричных обозначениях:

\[
    \boldsymbol{A} \boldsymbol{x} = \boldsymbol{y}
\]

Поставим задачу минимизировать норму вектор ошибки (невязки):

\[
    \boldsymbol{e} = \boldsymbol{A} \boldsymbol{x} - \boldsymbol{y} 
\]

В качестве нормы возьмем p-норму: $||\boldsymbol{e}||_p = \left( \sum_i |e_i|^p \right)^{1/p}$. Как известно, если методом наименьших квадратов можно аналитически найти решение, минимизирующее Евклидову норму вектора невязки (root-mean-squared error) $\sqrt{\boldsymbol{e}^T \boldsymbol{e}}$. Покажем как можно построить итеративный подход, использующий результаты для взвешенного метода наименьших квадратов, для нахождения оптимального решения для $l_p$ нормы.

\subsection{Взвешенные метод наименьших квадратов}

Для построения IRLS нужно сперва вспомнить основные результаты взвешенной модификации МНК. Добавим в Евклидову норму веса для каждой компоненты вектора $\boldsymbol{x}$ и будем минимизировать норму:

\[
    \boldsymbol{||W e||_2^2} = \sum_i w_i^2 e_i^2 = \boldsymbol{e}^T \boldsymbol{W}^T \boldsymbol{W} \boldsymbol{e} 
\]

\[
    \boldsymbol{W} = diag(w_1, w_2, \dots, w_M)
\]

$\boldsymbol{W}$ - диагональная матрица ненулевых весов. Легко видеть, что такое взвешивание соответствует линейному преобразованию растяжения с коэффициентами $w_1, w_2, \dots w_M$. Для переопределенной системы решение с минимальной взвешенной нормой оказывается равным:

\[
    \boldsymbol{x} = \left[ \boldsymbol{A^T W^T W A} \right]^{-1} \boldsymbol{A^T W^T W y}
\]

\subsection{Алгоритм \textbf{IRLS}}

Поиск минимума $||\boldsymbol{e}||_p = \left( \sum_i |e_i|^p \right)^{1/p}$ сводится к итеративному алгоритму, где на каждом шаге применяется взвешенный МНК. Набор весов $w_1, w_2, \dots, w_M$ пересчитывается на каждой итерации $n$.

\[
||e(n +1)||_p = \left( \sum_i w_i^2(n) |e_i(n)|^2 \right)^{1/2}
\]

\[
w_i(n) = e_i(n)^{\frac{p - 2}{2}}
\]

Начальные веса $w_1, w_2, \dots, w_M$ берутся единичными, т.е. для первой итерации используется в качестве приближения используется стандартный МНК.

\subsection{Задачи}

\subsubsection{Задача 1: WLS}

Получить оптимальное решение для переопределенной системы с взвешенной нормой. \\

\textbf{Решение:}

Вспомним результат обычного МНК:

\[
    \boldsymbol{A} \boldsymbol{x} = \boldsymbol{y}
\]

\[
    \boldsymbol{x} = \left[\boldsymbol{X^T X}\right]^{-1} \boldsymbol{X y}
\]

Домножая слева на $\boldsymbol{W}$ получим: $\boldsymbol{W} \boldsymbol{A} \boldsymbol{x} = \boldsymbol{W} \boldsymbol{y}$. Получаем исходную задачу МНК с матрицей $\boldsymbol{W} \boldsymbol{A}$ и правой частью $\boldsymbol{W} \boldsymbol{y}$. Отсюда сразу получается ответ:

\[
    \boldsymbol{x} = \left[ \boldsymbol{A^T W^T W A} \right]^{-1} \boldsymbol{A^T W^T W y}
\]

\subsubsection{Задача 2: Пример IRLS для $l_1$}

Реализовать IRLS и применить его для нахождения линейной модели по набору точек $x, y$: $(1, 2), (2, 3), (4, 6)$. Применить реализованный метод и убедиться, чтобы итеративный метод сходится к $3 / 2$ для $p = 1$ (норма $l_1$) и к $32 / 21$ для $p = 2$.

\subsubsection{Задача 3: Зависимость весов для $l_p$}

Обосновать выбор $w_i(n) = e_i(n)^{\frac{p - 2}{2}}$, считая что итеративный метод сходится.

\subsubsection{Задача 4: Логистическая регрессия как IRLS}

Показать, как применение метода Ньютона-Рафсона к логистической регрессии приводит к IRLS.

\textbf{Решение:}

\textit{см. раздел про логистическую регрессию}

\section{Гауссовы процессы для задачи классификации. Применение Лапласовой аппроксимации для Гауссовсого процесса}

В вероятностном подходе к задаче классификации, наша цель — смоделировать апостериорные
вероятности целевой переменной для нового входного вектора, учитывая набор обучающих
данных. Эти вероятности должны лежать в интервале (0,1). Проблема в том, что модель гауссовского процесса делает предсказание для всей действительной оси. Однако мы можем легко
адаптировать гауссовские процессы к задачам классификации, преобразуя вывод
гауссовских процессов с помощью подходящей нелинейной функции активации.

Рассмотрим сначала задачу классификации с целевой переменной $y \in \{0,1\}$. Если мы определим гауссовский процесс c функцией $a(\mathbf{x})$, а затем преобразуем функцию с помощью сигмоиды $z=\sigma(a)$, заданной формулой $\frac{1}{1+exp(-a)}$, то мы получим негауссовский
стохастический процесс c функциями $z(x)$, где $z \in \{0,1\}$.

Обозначим входные данные обучающей выборки как $x_1,...,x_N$ с соответствующими
наблюдаемыми целевыми переменными $\mathbf{y} =(y_1,...,y_N)^T$. Мы также рассматриваем одну контрольную точку
$x_{N+1}$ с целевым значением $y_{N+1}$. Наша цель — определить предсказательное распределение
$p(y_{N+1}|\mathbf{y})$, где зависимость от входных переменных оставлена неявной. Для этого мы строим гауссовский процесс по вектору $\mathbf{a}_{N+1}$, который имеет компоненты $a(\mathbf{x}_1),...,a(\mathbf{x}_{N+1})$. Это, в свою очередь, определяет негауссовский процесс по $y_{N+1}$,
и, условно оценивая по обучающим данным $y_N$, мы получаем требуемое предсказательное распределение. Гауссовский процесс для $a_{N+1}$ принимает вид
\[ p(\mathbf{a}_{N+1})=N(\mathbf{a}_{N+1}|\mathbf{0},\mathbf{C}_{N+1})\]

По численным причинам удобно ввести шумоподобный член, определяемый
параметром $\nu$. Шумоподобный член нам нужен для того, чтобы ковариационная матрица была положительно определена. Таким образом,
Ковариационная матрица $\mathbf{C}_{N+1}$ состоит из элементов определяемых следующим уравнением $C(\mathbf{x}_n,\mathbf{x}_m)=k(\mathbf{x}_n,\mathbf{x}_m)+\nu\delta_{nm}$
где $k(\mathbf{x}_n,\mathbf{x}_m)$ — любая положительно полуопределенная функция ядра вида: $\phi(x_n)^T\phi(x_m)$, а значение $\nu$ обычно фиксируется заранее. Мы предположим, что
функция ядра $k(\mathbf{x},\mathbf{x}^{\prime})$ определяется вектором $\mathbf{\theta}$ параметров. В дальнейшем мы узнаем, как можно узнать вектор $\mathbf{\theta}$ из обучающих данных.

Для задачи классификации с двумя классами достаточно предсказать $p(y_{N+1} =1|y_{N})$, поскольку
значение $p(y_{N+1} =0|y_{N})$ тогда задается как $1-p(y_{N+1} =1|y_{N})$. Тогда требуемое
предсказательное распределение задается как
\begin{equation}
p(y_{N+1} =1|\mathbf{y}_{N})= \int p(t_{N+1} =1|a_{N+1})p(a_{N+1}|\mathbf{y}_N)da_{N+1},
\end{equation}
где $p(t_{N+1} =1|a_{N+1})=\sigma(a_{N+1})$. Этот интеграл аналитически неразрешим. Однако в вычислении этого предсказательного распределения нам поможет аппроксимация Лапласа, которую мы и рассмотрим.

\subsection{Лапласова аппроксимация}
Воспользуемся теоремой Байеса, получим следующую цепочку равенств
\begin{equation}
\label{integral}
\begin{split}
&p(a_{N+1}|\mathbf{y}_{N}) = \int p(a_{N+1},\mathbf{a}_{N}|\mathbf{y}_N)d\mathbf{a}_{N}
= \frac{1}{p(\mathbf{y}_N)} \int p(a_{N+1},\mathbf{a}_{N})p(\mathbf{y}_{N}|a_{N+1},\mathbf{a}_{N})da_{N}= \\
&=\frac{1}{p(\mathbf{y}_{N})} \int p(a_{N+1}|\mathbf{a}_{N})p(\mathbf{a}_{N})p(\mathbf{y}_{N}|\mathbf{a}_{N})d\mathbf{a}_{N}= \int p(a_{N+1}|\mathbf{a}_{N})p(\mathbf{a}_{N}|\mathbf{y}_{N})d\mathbf{a}_{N}
\end{split}
\end{equation}
где мы использовали формулу $p(\mathbf{y}_{N}|a_{N+1},\mathbf{a}_{N})=p(\mathbf{y}_{N}|\mathbf{a}_{N})$. 
Условное распределение $p(a_{N+1}|\mathbf{a}_{N})$ получается использованием формулы  $m(x_{N+1}) = \mathbf{k}^T\mathbf{C}^{-1}_N\mathbf{y}$ и формулы для дисперсии $\sigma^2(x_{N+1}) = c-\mathbf{k}^T\mathbf{C}^{-1}_N\mathbf{k}$. Таким образом мы получаем
\[
\label{condition}
p(a_{N+1}|\mathbf{a}_{N})=N(a_{N+1}|\mathbf{k}^T\mathbf{C}^
{-1}_N\mathbf{a}_{N},c-\mathbf{k}^T\mathbf{C}^{-1}_N\mathbf{k})\]

Поэтому мы можем оценить интеграл в \ref{integral}, найдя приближение Лапласа
для распределения $p(\mathbf{a}_{N}|\mathbf{y}_{N})$, а затем используя стандартную формулу для
свертки двух гауссовых распределений.

$p(\mathbf{a}_{N})$ задается гауссовым процессом c нулевым средним и ковариационной матрицей $\mathbf{C}_{N}$, $p(\mathbf{y}_{N}|\mathbf{a}_{N})$ задается как
\[
p(\mathbf{y}_{N}|\mathbf{a}_{N})=\displaystyle \prod_{n=1}^{N}\sigma(a_{N})^{y_n}(1-\sigma(a_{n}))^{1-y_n} =
\displaystyle \prod_{n=1}^{N} e^{a_ny_n}\sigma(-a_n)\]

Затем мы получаем аппроксимацию Лапласа разложением Тейлора логарифма $p(\mathbf{a}_{N}|\mathbf{y}_N)$, который, с точностью до аддитивной константы, задается следующей формулой
\begin{equation}
\label{PSI}
 \Psi(\mathbf{a}_N) = \ln p(\mathbf{a}_{N})+\ln p(\mathbf{y}_N|\mathbf{a}_N)
 = -\frac{1}{2}\mathbf{a}^{T}_N\mathbf{C}^{-1}_N \mathbf{a}_N-\frac{N}{2} \ln(2\pi)-\frac{1}{2}\ln|\mathbf{C}_N|+\mathbf{y}^T_N\mathbf{a}_N-\displaystyle \sum^{N}_{n=1}\ln(1+e^{a_n})+const
\end{equation}

Сначала нам нужно найти моду апостериорного распределения, и для этого требуется вычислить градиент $\Psi(a_N)$, который задаётся выражением:
\[
\nabla \Psi(\mathbf{a}_N) = \mathbf{y}_N - \sigma_{N} - \mathbf{C}_{N}^{-1}\mathbf{a}^N
\]
где $\mathbf{\sigma}_N$ — вектор с элементами $\sigma(a_n)$. \\

\textbf{Задача 1:}
Почему мы не можем найти моду, просто приравнивая этот градиент к нулю? 

\textbf{Решение:}
Потому что $\mathbf{\sigma}_N$ нелинейно зависит от $a_N$ 

Мы прибегаем к итеративной схеме, основанной на методе Ньютона-Рафсона, что приводит к итеративному взвешенному методу наименьших квадратов (IRLS). Это требует вычисления вторых производных $\Psi(\mathbf{a}_N)$, которые нам также необходимы для лапласовского приближения, и которые задаются выражением:
\[
\nabla \nabla \Psi(\mathbf{a}_N) = -\mathbf{W}_{N} - \mathbf{C}_{N}^{-1} 
\]
Мы видим, что гессиан $A=-\nabla \nabla \Psi(a_N)$ является положительно определённой матрицей, и поэтому апостериорное распределение $p(a_N|y_N)$ является лог-выпуклым и, следовательно, имеет единственную моду, которая является глобальным максимумом.

\textbf{Задача 2:}
Докажите, что гессиан $A=-\nabla \nabla \Psi(a^N)$ является положительно определённой матрицей


\textbf{Решение:}
$\mathbf{W}_{N}$ — диагональная матрица с элементами $\sigma(a_n)(1-\sigma(a_n))$, так как $\frac{d\sigma}{da}=\sigma(1-\sigma)$. Заметим, что эти диагональные элементы лежат в диапазоне $(0, 1/4)$, и, следовательно, $\mathbf{W}_{N}$ является положительно определённой матрицей. Поскольку $\mathbf{C}_N$ (и, следовательно, её обратная матрица) является положительно определёнными по построению. Таким образом мы видим, что гессиан $\mathbf{A}=-\nabla \nabla \Psi(\mathbf{a}_N)$ является положительно определённой матрицей.


Однако апостериорное распределение не является гауссовским, поскольку гессиан является функцией $\mathbf{a}_N$.
Используя формулу Ньютона-Рафсона, итерационное уравнение для $a_N$ имеет вид:
\[
\mathbf{a}_N^{new} = \mathbf{C}_N(\mathbf{I} + \mathbf{W}_N\mathbf{C}_N)^{-1}(\mathbf{y}_N-\mathbf{\sigma}_N+\mathbf{W}_N\mathbf{a}_N) 
\]
Эти уравнения итерируются до тех пор, пока не сойдутся к модe, которую мы обозначаем как $\mathbf{a}_N^{\star}$. На этой моде градиент $\nabla \Psi(\mathbf{a}_N)$ занулится, и, следовательно, $a_N^{\star}$ будет удовлетворять уравнению:
\begin{equation}
\label{star}
\mathbf{a}_N^{\star} = \mathbf{C}_N(\mathbf{y}_N - \mathbf{\sigma}_N) 
\end{equation}
Как только мы нашли моду $a_N^{\star}$ апостериорного распределения, мы можем вычислить гессиан, заданный выражением:
\[
\mathbf{H} = \nabla \nabla \Psi(\mathbf{a}_N) = \mathbf{W}_N+\mathbf{C}_N^{-1} 
\]
где элементы $\mathbf{W}_N$ вычисляются с помощью $\mathbf{a}_N^{\star}$. Это определяет нашу гауссову аппроксимацию к апостериорному распределению $p(\mathbf{a}_N|\mathbf{y}_N)$, задаваемую выражением:
\[
q(a_N) = N(\mathbf{a}_N|\mathbf{a}_N^{\star}, \mathbf{H}^{-1}) 
\]
Теперь мы можем объединить это с \ref{condition} и, следовательно, вычислить интеграл \ref{integral}. Поскольку это соответствует линейной гауссовской модели, мы можем использовать следующую формулы:
\[
\mathbb{E}[a_{N+1}|\mathbf{y}_N] =\mathbf{k}^T(\mathbf{y}_N - \mathbf{\sigma}_N) 
\]
\[
var[a_{N+1}|\mathbf{y}_N] = c-\mathbf{k}^T(\mathbf{W}_N^{-1} + \mathbf{C}_N)^{-1}\mathbf{k} 
\]
Теперь, когда у нас есть гауссово распределение для $p(a_{N+1}|\mathbf{y}_{N})$, мы можем аппроксимировать интеграл \ref{integral}, используя формулу.
\[
\int \sigma(a)N(a|\mu,\sigma^{2})da\simeq \sigma(\kappa(\sigma^{2})\mu)
\]

Нам также нужно определить параметры $\mathbf{\theta}$ функции ковариации. Один из возможных подходов — максимизировать функцию правдоподобия, заданную как $p(\mathbf{y}_N|\mathbf{\theta})$, для чего нам нужны выражения для логарифма функции правдоподобия и её градиента. При желании можно также добавить подходящие регуляризирующие члены, что приведёт к решению с помощью метода максимального правдоподобия со штрафом. Функция правдоподобия определяется выражением:
\[
p(\mathbf{y}_N|\mathbf{\theta}) = \int p(\mathbf{y}_N|\mathbf{a}_N)p(\mathbf{a}_N|\mathbf{\theta})d\mathbf{a}_N 
\]
Этот интеграл аналитически невычислим, поэтому снова используем лапласовское приближение. Используя формулу, 
\[
\int f(\mathbf{z})d\mathbf{z} \simeq f(\mathbf{z}_0)\frac{(2 \pi)^{N/2}}{|\mathbf{A}|^{1/2}}
\]
где $\mathbf{A}=-\nabla \nabla \ln f(\mathbf{z})|_{z=z_0}$ - $N \times N$-матрица мы получаем следующее приближение для логарифма функции правдоподобия:
\begin{equation}
\label{log}
\ln p(\mathbf{y}_N|\mathbf{\theta}) = \Psi(\mathbf{a}_N^{\star}) - \frac{1}{2}\ln|\mathbf{W}_N+ \mathbf{C}_N^{-1}| + \frac{N}{2}\ln(2\pi) 
\end{equation}
где $\Psi(\mathbf{a}_N^{\star}) = \ln p(\mathbf{a}_N^{\star}|\mathbf{\theta}) + \ln p(\mathbf{y}_N|\mathbf{a}_N^{\star})$. Нам также нужно вычислить градиент $\ln p(\mathbf{y}_N|\mathbf{\theta})$ c учётом вектора параметров $\mathbf{\theta}$. Заметим, что изменение $\theta$ вызовут изменения $a_N^{\star}$, что приведёт к появлению дополнительных слагаемых в градиенте. Таким образом, когда мы дифференцируем \ref{log} по $\theta$, мы получаем два набора слагаемых, первый возникает из-за зависимости ковариационной матрицы $C_N$ от $\theta$, а второй — из-за зависимости $a_N^{\star}$ от $\theta$.

Слагаемые, возникающие из-за явной зависимости от $\theta$, могут быть найдены с помощью \ref{PSI} вместе с формулами: 
\begin{equation}
\label{odin}
    \frac{\partial}{\partial x}(\mathbf{A}^{-1})=-\mathbf{A}^{-1}\frac{\partial \mathbf{A}}{\partial x}\mathbf{A}^{-1}
\end{equation}
и 
\begin{equation}
\label{dwa}
    \frac{\partial}{\partial x}\ln(|\mathbf{A}|)=Tr(\mathbf{A}^{-1}\frac{\partial \mathbf{A}}{\partial x})
\end{equation} 
и задаются выражением:
\[
\label{urav1}
\frac{\ln p(\mathbf{y}_N|\theta)}{\partial \theta_j} = \frac{1}{2}a_N^{\star T}\mathbf{C}_N^{-1}\frac{\partial \mathbf{C}_N}{\partial \theta_j}\mathbf{C}_N^{-1}a_N^{\star} - \frac{1}{2}Tr[(\mathbf{I} + \mathbf{C}_N\mathbf{W}_N)^{-1}\mathbf{W}_N\frac{\partial \mathbf{C}_N}{\partial \theta_j}] 
\]
Для вычисления слагаемых, возникающих из-за зависимости $\mathbf{a}_N^{\star}$ от $\mathbf{\theta}$, заметим, что лапласовское приближение построено таким образом, что $\Psi(a_N)$ имеет нулевой градиент, если $\mathbf{a}_N=\mathbf{a}_N^{\star}$, Следовательно $\Psi(a_N^{\star})$ не даёт вклада в градиент. Отсюда получаем:
\[
\label{urav2}
-\frac{1}{2} \displaystyle \sum_{i=1}^{N} \frac{\partial \ln|\mathbf{W}_N + \mathbf{C}_N^{-1}|}{\partial a_n^{\star}})\frac{\partial a_n^{\star}}{\partial \theta_j} = -\frac{1}{2} \displaystyle \sum_{i=1}^N [\mathbf{I} + \mathbf{C}_N\mathbf{W}_N)^{-1}\mathbf{C}_N]_{nn} \sigma_{n}^{\star}(1-\sigma_{n}^{\star})(1-2\sigma_{n}^{\star})\frac{\partial a_n^{\star}}{\partial \theta_j} 
\]

где $\sigma_n^{\star}= \sigma(a_n^{\star})$, и снова мы использовали формулу \ref{dwa} вместе с определением $\mathbf{W}_N$. Производную $a_N^{\star}$ по $\theta_j$ можно найти, продифференцировав соотношение \ref{star} по $\theta_j$:

\[
\frac{\partial a_n^{\star}}{\partial \theta_j} = \frac{\partial C_N}{\partial \theta_j} (\mathbf{y}_N-\sigma_N)-\mathbf{C}_N \mathbf{W}_N \frac{\partial a_n^{\star}}{\partial \theta_j}  
\]
После преобразования получим:
\[
\label{urav3}
\frac{\partial a_n^{\star}}{\partial \theta_j} = (\mathbf{I} + \mathbf{W}_NC_N)^{-1} \frac{\partial \mathbf{C}_N}{\partial \theta_j} (\mathbf{y}_N-\sigma_N)  
\]
Объединив последние 4 уравнения, мы можем вычислить градиент функции логарифмического правдоподобия, который может быть использован со стандартными алгоритмами нелинейной оптимизации для определения значения $\mathbf{\theta}$.

\textbf{Задача 3:}
Докажите формулы \ref{odin}, \ref{dwa} \

\textbf{Решение:}
Применим правило Лейбница
\[
\frac{\partial}{\partial x} (\mathbf{AB})=\frac{\partial \mathbf{A}}{\partial x}
 \mathbf{B}+\mathbf{A}\frac{\partial \mathbf{B}}{\partial x}.
\]
к соотношению $\mathbf{AA}^{-1}=\mathbf{I}$, после чего умножим справа на $\mathbf{A}^{-1}$, мы получили таким образом формулу \ref{odin}

воспользуемся формулами 
\[
Tr(\mathbf{A})=\displaystyle \sum_{i=1}^{N} \lambda_i
\]
\[
|\mathbf{A}|=\displaystyle \prod_{i=1}^{N} \lambda_i
\]
\[
\mathbf{A}=\displaystyle \sum_{i=1}^{N} \lambda_i \mathbf{u}_i\mathbf{u}_i^{T}
\]
\[
\mathbf{A}^{-1}=\displaystyle \sum_{i=1}^{N} \frac{1}{\lambda_i} \mathbf{u}_i\mathbf{u}_i^{T}
\]
где $\lambda_i$-cj- собственные числа, а $\mathbf{u}_i$-собственные векторы
Подставляя эти формулы в соответствующие части формулы \ref{dwa}, а также пользуясь в результате преобразований тождеством $\mathbf{u}_i^{T}\mathbf{u}_j=I_{ij}$, для действительной симметрической матрицы, получаем равенство. Так мы доказали \ref{dwa}
