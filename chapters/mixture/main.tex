\section{Mixture of Experts (Смесь экспертов)}

Модель \textit{Смеси экспертов} (Квазилинейный ансамбль, Mixture of Experts, MoE) является архитектурой машинного обучения, которая сочетает в себе несколько моделей (экспертов) для решения сложных задач. Основная идея заключается в том, чтобы разделить пространство входных данных на части, в каждом из которых определенный эксперт специализируется. Общая модель обучается так, чтобы комбинировать выходы экспертов с учетом их специализации.

Математически, выход модели MoE для признакового описания объекта $\mathbf{x}$ может быть представлен как:

$$
y = \sum_{k=1}^{K} g_k(\mathbf{x}) f_k(\mathbf{x}),
$$

где:
\begin{itemize}
    \item $K$ — количество экспертов,
    \item $f_k(\mathbf{x})$ — локальная модель, функция $k$-го эксперта, производящая прогноз,
    \item $g_k(\mathbf{x})$ — шлюзовая функция или функция компетентности, определяющая вес вклада $k$-го эксперта, причём $\sum_{k=1}^{K} g_k(\mathbf{x}) = 1$ и $g_k(\mathbf{x}) \geq 0$ для всех $k$.
\end{itemize}

Шлюзовая (Gate) функция обычно моделируется с помощью функции softmax

$$
g_k(\mathbf{x}) = \frac{\exp(h_k(\mathbf{x}))}{\sum_{j=1}^{K} \exp(h_j(\mathbf{x}))},
$$

где $h_k(\mathbf{x})$ — функция компетентности для $k$-го эксперта. Выбираются из каких-либо содержательных соображений.

Преимущество MoE заключается в способности моделировать сложные зависимости путем разделения задачи между специализированными экспертами, что может улучшить обобщающую способность и эффективность обучения.

\subsection{Задачи и решения}

\subsubsection{Задача 1}

Пусть имеется модель MoE с двумя экспертами, функции которых заданы как $f_1(x) = 2x$ и $f_2(x) = x^2$. Функции $h_k (x):$ $h_1 (x) = -x$, $h_2 (x) = x$.

Требуется найти выражение для общего выхода модели $y$ в зависимости от $x$.

\textbf{Решение:}

Шлюзовая функция моделируется как:

$$
g_1(x) = \frac{\exp(-x)}{\exp(-x) + \exp(x)}, \quad g_2(x) = \frac{\exp(x)}{\exp(-x) + \exp(x)}.
$$

Суммарный выход модели:

$$
y = g_1(x) f_1(x) + g_2(x) f_2(x) = g_1(x) \cdot 2x + g_2(x) \cdot x^2.
$$

Подставим выражения для $g_1(x)$ и $g_2(x)$:

$$
y = \frac{\exp(-x)}{\exp(-x) + \exp(x)} \cdot 2x + \frac{\exp(x)}{\exp(-x) + \exp(x)} \cdot x^2.
$$

Поэтому итоговое выражение для $y$:

$$
y = \frac{\exp(-x) \cdot 2x + \exp(x) \cdot x^2}{\exp(-x) + \exp(x)}.
$$

\subsubsection{Задача 2}

Рассмотрим модель MoE, где шлюзовая функция выбирает только одного эксперта в зависимости от знака $x$:
$$
g_1(x) = 
\begin{cases} 
1, & \text{если } x \geq 0, \\ 
0, & \text{если } x < 0,
\end{cases}
$$
$$
g_2(x) = 
\begin{cases} 
0, & \text{если } x \geq 0, \\ 
1, & \text{если } x < 0.
\end{cases}
$$
Функции экспертов заданы как $f_1(x) = x^2$ и $f_2(x) = -x$. Найдите общий выход модели $y$ для любого $x$.

\textbf{Решение:}

Поскольку в каждый момент времени активен только один эксперт, общий выход модели определяется функцией активного эксперта.

Для $x \geq 0$: 
$$
g_1(x) = 1, \quad g_2(x) = 0, \quad y = g_1(x) f_1(x) = x^2.
$$

Для $x < 0$: 
$$
g_1(x) = 0, \quad g_2(x) = 1, \quad y = g_2(x) f_2(x) = -x.
$$

Таким образом, общий выход модели равен:
$$
y = 
\begin{cases} 
x^2, & \text{если } x \geq 0, \\ 
-x, & \text{если } x < 0.
\end{cases}
$$
Это означает, что модель ведет себя по-разному на положительных и отрицательных значениях $x$, отражая специализацию экспертов на разных областях входных данных.

\subsubsection{Задача 3}

Рассмотрим модель Mixture of Experts, состоящую из двух экспертных моделей $f_1$ и $f_2$, а также шлюзовой функции $g$. Пусть на вход подаётся одно признаковое значение $x$. Выражения для выходов моделей заданы следующим образом:

$$
f_1(x) = w_1 x + b_1, \quad f_2(x) = w_2 x + b_2, \quad g(x) = \sigma(v x + c)
$$

где $\sigma(z) = \frac{1}{1 + e^{-z}}$ — сигмоидальная функция активации, а $w_1, w_2, b_1, b_2, v, c$ — параметры модели.

Выход всей модели MoE определяется как:

$$
y(x) = g(x) f_1(x) + (1 - g(x)) f_2(x)
$$

\textbf{Вопрос:} Предположим, что при $x = 0$ мы наблюдаем, что $y(0) = 1$, $f_1(0) = 1$, $f_2(0) = 3$. Найдите значение $g(0)$.

\textbf{Решение:}

Из условия задачи при $x = 0$ имеем:

$$
y(0) = g(0) \cdot f_1(0) + (1 - g(0)) \cdot f_2(0) = g(0) \cdot 1 + (1 - g(0)) \cdot 3
$$

Подставляем известное значение $y(0) = 1$:

$$
g(0) \cdot 1 + (1 - g(0)) \cdot 3 = 1
$$
$$
g(0) + 3 - 3g(0) = 1
$$
$$
-2g(0) + 3 = 1
$$
$$
-2g(0) = -2
$$
$$
g(0) = 1
$$

Таким образом, $g(0) = 1$.

\section{Блендинг}
Ансамблирование (ensemble methods) --- способ улучшить точность и устойчивость предсказаний за счёт комбинирования нескольких моделей машинного обучения. Отдельно взятая модель может иметь свои слабые стороны и распределённые ошибки, в то время как ансамбль (например, суммарное взвешенное предсказание нескольких базовых моделей) позволяет компенсировать ошибки отдельных участников и повысить итоговое качество.

Среди распространённых методов ансамблирования выделяют:
\begin{itemize}
    \item \textbf{Баггинг (Bagging)}: параллельное обучение нескольких независимых моделей с последующим усреднением или голосованием;
    \item \textbf{Бустинг (Boosting)}: последовательное обучение ``слабых'' моделей, где каждая следующая фокусируется на ошибках предыдущей;
    \item \textbf{Стэкинг (Stacking)}: построение ``мета-модели'' на предиктах базовых алгоритмов, обученных в $k$-fold-схеме;
    \item \textbf{Блендинг (Blending)}: способ объединения моделей, близкий к стэкингу, но использующий простое деление обучающих данных на train/hold-out, где hold-out выборка служит для определения весов или правил объединения.
\end{itemize}

В данной статье мы сосредоточимся на \textbf{блендинге} и рассмотрим его теоретические аспекты.

\section{Метод блендинга: формулировка}
\subsection{Общая схема}
В \textbf{blending} обучающую выборку делят на две части:
\begin{enumerate}
    \item \textbf{Основная (train)} --- для обучения базовых моделей;
    \item \textbf{Hold-out} --- небольшое подмножество, \textit{не} участвующее в обучении базовых моделей.
\end{enumerate}

Далее:
\begin{enumerate}
    \item Тренируем базовые модели на \textbf{train} части.
    \item Для каждого объекта из \textbf{hold-out} получаем предсказания от каждой базовой модели.
    \item По предсказаниям (например, по MSE или логистической потере) подбираем оптимальные веса $w_i$ или параметры для объединения.  
\end{enumerate}

Финальное предсказание --- это взвешенная комбинация выходов базовых моделей:
\[
\hat{y} = \sum_{i=1}^n w_i \cdot \hat{y}_i.
\]

\textbf{Отличие от стэкинга} в том, что здесь чаще всего берётся \textbf{одна} hold-out выборка, а не $k$ фолдов, и итоговая ``мета-модель'' упрощена до простой линейной (или другой) схемы, которая не обучается на кросс-валидации и не использует всю обучающую выборку для ``второго уровня''.

\section{Теоретические аспекты блендинга}
\subsection{Байесовская интерпретация и усреднение гипотез}
С позиции байесовского подхода, каждая модель $M_i$ может рассматриваться как гипотеза с апостериорной вероятностью $p(M_i \mid D)$, где $D$ --- обучающие данные. Если наши модели дают предсказания вероятности $P(y=1 \mid x, M_i)$, то оптимальным в смысле минимизации байесовского риска считается байесовское усреднение по всем гипотезам:
\[
p(y=1 \mid x, D) = \sum_{i=1}^n p(M_i \mid D) \, P(y=1 \mid x, M_i).
\]
Блендинг может рассматриваться как \textit{эмпирическое} приближение этого байесовского усреднения, где веса $w_i$ являются оценками $p(M_i \mid D_{holdout})$ на hold-out выборке.

\subsection{Оптимизация весов: линейная регрессия или минимизация ошибки}
На \textbf{hold-out} выборке $\{(x_j, y_j)\}$, $j=1,\ldots,m$, мы имеем предсказания базовых моделей $ \{\hat{y}_{1j}, \hat{y}_{2j}, \dots, \hat{y}_{nj}\}$ для $n$ моделей. Цель --- найти вектор весов $\mathbf{w} = (w_1, \ldots, w_n)^\top$, который минимизирует заданный функционал ошибки $L$ (MSE, MAE, логистическая потеря и пр.):
\[
\mathbf{w}^* = \arg\min_{\mathbf{w}} \sum_{j=1}^m L\Bigl(y_j,\ \sum_{i=1}^n w_i \hat{y}_{ij}\Bigr).
\]
В самом простом случае с MSE эта задача сводится к линейной регрессии (без регуляризации или с L2-регуляризацией). Однако можно решать её прямыми методами оптимизации (например, градиентными методами) --- при желании, даже нелинейно (но тогда это ближе к ``мини-стэкингу'').

\subsection{Корреляция ошибок и выигрыш от блендинга}
Теоретический выигрыш от ансамблирования (в частности, блендинга) связан с \textbf{корреляцией ошибок} базовых моделей. Если две модели сильно коррелированы по ошибкам, их усреднение мало что даст. Если же они допускают разные ошибки (низкая корреляция остаточных ошибок), то их комбинация может существенно улучшить итог.

\subsection{Bias-Variance разложение для ансамбля}
В рамках bias-variance разложения (применяя для простоты MSE-подход), можно показать, что \textbf{дисперсия} ансамбля может уменьшаться при условии, что ошибки различных базовых моделей не совпадают. При этом совместный \textit{bias} ансамбля может быть ниже или ниже/сопоставим с bias отдельно взятой модели, в зависимости от того, как подбираются веса. 

\subsection{Проблема ``утечки данных'' (data leakage)}
\textbf{Data leakage} возникает, когда hold-out выборка не изолирована от данных, которые использовались для обучения базовых моделей. Если те же данные фигурируют и при обучении, и при подборе весов, ансамбль может ``запомнить'' специфические паттерны, характерные для train. Это приводит к \textit{оптимистичной} оценке на hold-out и ухудшению генерализации на настоящем тесте. Правильная схема blending предполагает, что hold-out часть полностью не пересекается с train, и базовые модели никогда не видят hold-out при обучении.

\subsection{Ограничения блендинга}
\begin{itemize}
    \item \textbf{Неэффективное использование данных}: часть выборки ``простаивает'' в hold-out, не используется при обучении базовых моделей.
    \item \textbf{Малая hold-out выборка}: низкая точность оценки оптимальных весов, особенно если базовых моделей много.
    \item \textbf{Нет глубокой интеграции}: обычно используется простая схема (линейная комбинация) и одна разделённая выборка, что может быть менее гибким, чем полноценный стэкинг с $k$-fold и сложной мета-моделью.
\end{itemize}

\section{Теоретические задачи и их решения}

Ниже представлены три \textbf{теоретические} задачи, углубляющиеся в аспекты блендинга.

\subsection{Теоретическая Задача 1: Байесовская трактовка усреднения моделей}
Допустим, у нас есть набор моделей $\{M_1, ..., M_n\}$, каждая из которых оценивает вероятность события $P(y=1 \mid x)$ в задаче бинарной классификации. Покажите, что простое взвешенное усреднение предсказаний моделей может быть приближением к байесовскому усреднению гипотез. Каким образом оцениваются веса $\{w_i\}$ с байесовской точки зрения?

\begin{itemize}
    \item В байесовском подходе модель $M_i$ рассматривается как гипотеза с некоторой апостериорной вероятностью $p(M_i \mid D)$.
    \item Финальное предсказание апостериорной вероятности события --- это взвешенная сумма предсказаний базовых гипотез, где в качестве весов выступают $p(M_i\mid D)$.
    \item В реальной практике мы не имеем явного байесовского вывода для $p(M_i\mid D)$, но можем приблизить эти вероятности путём оптимизации весов на hold-out (например, минимизируя логистическую потерю). Фактически $w_i$ служат оценкой ``доверия'' к гипотезе $M_i$.
\end{itemize}

\subsection{Теоретическая Задача 2: Обоснование полезности блендинга через уменьшение дисперсии}
Рассмотрите две модели $M_1$ и $M_2$, прогнозирующие вещественную переменную (регрессия). Пусть ошибки каждой модели представляют собой случайные величины $\epsilon_1$ и $\epsilon_2$ с нулевым средним. Покажите, что дисперсия ансамбля (при простом равновесном усреднении) убывает, если ошибки $\epsilon_1$ и $\epsilon_2$ не идеально коррелированы.

\begin{itemize}
    \item Финальное предсказание блендинга: $\hat{y}_{ens} = \frac{1}{2}\hat{y}_1 + \frac{1}{2}\hat{y}_2$.
    \item Ошибка ансамбля: $\epsilon_{ens} = \frac{1}{2}\epsilon_1 + \frac{1}{2}\epsilon_2$.
    \item Дисперсия ошибки ансамбля: 
    \[
    \mathrm{Var}(\epsilon_{ens}) = \mathrm{Var}\left(\frac{\epsilon_1}{2} + \frac{\epsilon_2}{2}\right) = \frac{1}{4}\mathrm{Var}(\epsilon_1) + \frac{1}{4}\mathrm{Var}(\epsilon_2) + \frac{1}{2}\mathrm{Cov}(\epsilon_1,\epsilon_2).
    \]
    \item Если $\rho$ --- коэффициент корреляции между $\epsilon_1$ и $\epsilon_2$, то
    \[
    \mathrm{Var}(\epsilon_{ens}) = \frac{1}{4}\bigl(\mathrm{Var}(\epsilon_1) + \mathrm{Var}(\epsilon_2) + 2\rho\sqrt{\mathrm{Var}(\epsilon_1)\mathrm{Var}(\epsilon_2)}\bigr).
    \]
    \item При $\rho<1$ данная величина меньше $\frac{1}{2}(\mathrm{Var}(\epsilon_1) + \mathrm{Var}(\epsilon_2))$, то есть дисперсия снижается. Максимальный выигрыш достигается, когда корреляция отрицательна или нулевая.
\end{itemize}

\subsection{Теоретическая Задача 3: Использование hold-out при малом количестве данных}
Пусть у вас есть небольшой датасет объёмом $N$, и вы решили применить блендинг: отложить $\alpha N$ объектов как hold-out. Каковы потенциальные проблемы при слишком большой или слишком маленькой доле $\alpha$? Как это может сказаться на результатах ансамбля?

\begin{itemize}
    \item \textbf{Слишком маленькая hold-out} (малая $\alpha$): веса $w_i$ будут определены неустойчиво (большая дисперсия оценки), возможна пере/недо-настройка весов.  
    \item \textbf{Слишком большая hold-out} (большая $\alpha$): уменьшается объём train для базовых моделей, они становятся менее обученными (большая ошибка моделей), итоговое качество ансамбля может упасть.
    \item Есть компромисс: $\alpha$ не должна быть слишком мала, чтобы надёжно оценить веса, но и не слишком велика, чтобы не ``обеднять'' обучение базовых моделей.
    \item При крайне маленьких объёмах данных классический блендинг может быть неэффективен; альтернативой является полноценный стэкинг с $k$-fold, где весь объём задействован более рационально.
\end{itemize}
