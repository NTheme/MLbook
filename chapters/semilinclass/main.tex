\section{Квазилинейные композиции}

Линейные корректирующие операции естественным образом обобщаются на тот случай, когда веса базовых алгоритмов не постоянны и зависят от положения объекта $x$ в пространстве $X$.

Поставим каждому базовому алгоритму $b_{t}(x)$ в соответствие функцию компетентности $g_{t}(x)$, принимающую значения из отрезка [0, 1]. Определим композицию так, чтобы ответ базового алгоритма $b_{t}(x)$ на объекте $x$ учитывался с весом $g_{t}(x)$.

\textbf{Определение.} Квазилинейная корректирующая операция есть функция $F: \mathbb{R}^{2T} \rightarrow \mathbb{R}$:

\begin{equation}
    F(b_{1}, \dots, b_{T}, g_{1}, \dots, g_{T}) = \sum_{t=1}^{T}g_{t} b_{t}.
    \label{main_eq}
\end{equation}

Соответственно, алгоритмическая композиция имеет вид:

\begin{equation*}
    a(x) = C(\sum_{t=1}^{T}g_{t}(x)b_{t}(x)),
\end{equation*}

где $C$ - фиксированное решающее правило. Чем больше значение $g_{t}(x)$, тем с большим весом учитывается ответ алгоритма $b_{t}(x)$ на объекте $x$. Множество:

\begin{equation*}
    \Omega_{t} = \{x\in X: g_{t}(x) > g_{s}(x), s = 1, \dots, T, s \neqt\}
\end{equation*}

называется областью компетентности базового алгоритма $b_{t}(x)$.

Основная идея смесей заключается в применении принципа «разделяй и властвуй». Используя достаточно простые базовые алгоритмы $b_{t}(x)$ и функции компетентности $g_{t}(x)$, можно восстанавливать довольно сложные зависимости.

Понятие области компетентности было введено Растригиным в \cite{растригин1981коллективные}. В англоязычной литературе композиции вида (\ref{main_eq}) принято называть смесями экспертов
(mixture of experts, ME), базовые алгоритмы — экспертами, функции компетентности — шлюзами (gates) \cite{jacobs1991adaptive}. Шлюзы определяют, к каким экспертам должен быть
направлен объект x. По-русски «смесь экспертов» звучит не очень удачно, поэтому
будем говорить о «смесях алгоритмов». Произведения $g_{t}(x)b_{t}(x)$ называются компонентами смеси.

Естественным требованием к шлюзам является условие нормировки:

\begin{equation}
    \sum_{t=1}^{T}g_{t}(x) = 1, \forall x \in X.
    \label{norm}
\end{equation}

Если оно выполнено, то для любого объекта x коррекция сводится к усреднению ответов базовых алгоритмов с некоторыми весами, причём результат никогда
не выходит за пределы отрезка $[\min_{t}b_{t}(x), \max_{t}b_{t}(x)]$.

Чтобы гарантировать выполнение условия нормировки (\ref{norm}), используют функцию «мягкого максимума» SoftMax: $\mathbb{R}^{T} \rightarrow \mathbb{R}^{T}$:

\begin{equation*}
    \widetilde{g}_{t} (x) = SoftMax_{t}(g_{1}(x), \dots, g_{T}(x)) = \dfrac{e^{\beta g_{t}(x)}}{e^{\beta g_{1}(x)} + \cdot + e^{\beta g_{T}(x)}} .
\end{equation*}


Эта функция переводит вектор $(g_{1}, \dots, g_{T})$ в нормированный вектор $(\widetilde{g}_{1}, \dots, \widetilde{g}_{T})$.
При $\beta \rightarrow \infty$, все компоненты $\widetilde{g}_{t}$ стремятся к нулю, кроме одной, соответствующей максимальному значению $\max_{t} g_{t}$, которая стремится к единице. Чем больше значение параметра $\beta$, тем «чётче» границы областей компетентности.

В задачах классификации с пороговым решающим правилом $C(b) = [b > 0]$ или $C(b) = sign(b)$ делать нормировку, вообще говоря, не обязательно, поскольку для классификации важен только знак выражения (\ref{main_eq}).

\subsection{Задача обнаружения нетипичных объектов}

 Если значение $g_{t}(x)$ меньше некоторого порога для всех $t = 1,\dots,T$, то можно полагать, что объект x попадает в «область неуверенности», где все алгоритмы не компетентны. Как правило, это связано с тем, что в данной области обучающих прецедентов просто не было. Таким образом, смеси алгоритмов позволяют решать задачу обнаружения нетипичных объектов или новизны (novelty detection). Иногда эту задачу называют классификацией с одним классом (one-class classification).

 \subsection{Вероятностная интерпретация}
 
Функции $g_{t}(x)$ можно интерпретировать как
нечёткие множества, а при условии нормировки — и как вероятностные распределения. Если для байесовского классификатора расписать апостериорные вероятности
классов, то функции компетентности приобретут смысл априорных вероятностей:

\begin{equation*}
    a(x) = arg_{y\in Y} max \lambda_{y} P\{y|x\} = arg_{y\in Y} max \lambda_{y} \sum_{t=1}^{T} \underbrace{P(t)}_{g_{t}(x)} \underbrace{P\{y|t, x\}}_{b_{t_{y}}(x)}.
\end{equation*}

Здесь предполагается, что каждый базовый алгоритм $b_{t}(x)$ возвращает вектор апостериорных вероятностей $(P\{y|t, x\})_{y\in Y}$. Затем квазилинейная корректирующая операция переводит T таких векторов в один вектор апостериорных вероятностей $(P\{y| x\})_{y\in Y}$, и к нему применяется решающее правило $arg\:max_{y}$.


Вероятностная интерпретация приводит к ЕМ-алгоритму, который обрастает массой технических усложнений. Мы рассмотрим более простые оптимизационные методы, основанные на применении выпуклых функций потерь.

\subsection{Вид функций компетентности}


Вид функций компетентности определяется спецификой предметной области. Например, может иметься априорная информация о том, что при больших и малых значениях некоторого признака f(x) поведение целевой зависимости принципиально различно. Тогда имеет смысл использовать весовую функцию вида:

\begin{equation*}
    g(x;\alpha, \beta) = \sigma(\alpha f(x) + \beta),
\end{equation*}

где $\alpha, \beta \in \mathbb{R}$ - свободные параметры, $\sigma(z) = (1 + e^{-z})^{-1}$ - сигмоидная функция позволяющая описать плавный переход между двумя областями компетентности.


Если априорной информации нет, то в качестве «универсальных» областей компетентности берут области простой формы. Например, в пространстве $X = \mathbb{R}^{n}$ можно взять полуплоскости:

\begin{equation*}
    g(x;\alpha, \beta) = \sigma(x^{T}\alpha + \beta),
\end{equation*}


или сферические гауссианы:

\begin{equation*}
    g(x;\alpha, \beta) = e^{-\beta ||x - \alpha ||^{2}},
\end{equation*}


 где $\alpha \in \mathbb{R}^{n}$, $\beta \in \mathbb{R}$ — свободные параметры, которые настраиваются по выборке аналогично тому, как настраиваются параметры базовых алгоритмов.
\subsection{Выпуклые функции потерь}

Преимущество смесей в том, что они позволяют разбить всё пространство X на области, в каждой из которых задача решается относительно просто, и затем «склеить» эти решения в единую композицию. Основной технический приём, позволяющий решить эти относительно простые подзадачи по-отдельности, независимо друг от друга, связан с использованием выпуклых функций потерь. Рассмотрим функционал, характеризующий качество алгоритмических операторов при заданном векторе весов объектов $W^{l} = (w_{l}, \dots , w_{l})$.

\textbf{Гипотеза.} Функция потерь $\widetilde{L}(b, y)$ является выпуклой по b, то есть для любых
$b_{1}, b_{2} \in \mathbb{R}$, $y \in Y$ и неотрицательных $g_{1},\;g_{2}$, таких, что $g_{1} + g_{2} = 1$, выполняется:

\begin{equation*}
    \widetilde{L}(g_{1}b_{1} + g_{2}b_{2}, y) \leq g_{1} \widetilde{L}(b_{1}, y) + g_{2} \widetilde{L}(b_{2}, y).
\end{equation*}

Условие выпуклости имеет прозрачную интерпретацию: потери растут не медленнее, чем величина отклонения от правильного ответа y. Во многих задачах требование выпуклости является естественным. В частности, ему удовлетворяет квадратичная функция $\widetilde{L}(b, y) = (b−y)^{2}$, применяемая в регрессионных задачах. К сожалению, пороговая функция $\widetilde{L}(b, y) = [by < 0]$, применяемая в задачах классификации при $Y = \{-1, +1\}$, не является выпуклой. Однако некоторые её непрерывные аппроксимации выпуклы:

\begin{equation*}
    \widetilde{L}(b, y) = [by < 0] \leq 
    \begin{cases}
        e^{-by} &- \text{экспоненциальная (AdaBoost);} \\
        log_{2}(1 + e^{-by})&- \text{логарифмическая (LR);}\\
        (1-by)_{+}&- \text{кусочно-линейная (SVM).}
    \end{cases}
\end{equation*}

Замена пороговой функции потерь на непрерывную является распространённой практикой. В частности, логарифмическая аппроксимация используется в логистической регрессии , экспоненциальная — в алгоритме AdaBoost , кусочно-линейная — в методе опорных векторов SVM . Непрерывные функции потерь имеют важное достоинство — они штрафуют обучающие объекты за приближение к границе классов, что способствует увеличению зазора между классами и повышению надёжности классификации. Таким образом, требование выпуклости функции потерь не слишком обременительно и даже естественно как для задач регрессии, так и для классификации.

Итак, пусть выполнено условие нормировки (\ref{norm}) и функция потерь $\widetilde{L}$ выпукла. Вместо функционала Q(a) будем минимизировать его верхнюю оценку, которая непосредственно вытекает из условия выпуклости:

\begin{equation}
    Q(a) = \sum_{i=1}^{l} \widetilde{L}(\sum_{t=1}^{T} g_{t}(x_{i})b_{t}(x_{i}), y_{i}) \leq \sum_{i=1}^{l}\underbrace{\sum_{t=1}^{T} g_{t}(x_{i}) \widetilde{L}(b_{t}(x_{i}), y_{i})}_{Q(b_{t},W^{l})}.
    \label{qa}
\end{equation}


Функционал Q(a) распадается на сумму T функционалов, каждый из которых зависит только от $b_{t}$ и $g_{t}$. Если зафиксировать функции компетентности $g_{t}$, то все базовые алгоритмы $b_{t}$ можно настроить по-отдельности, независимо друг от друга. Это стандартная задача обучения базовых алгоритмов. Затем можно зафиксировать базовые алгоритмы и настроить их функции компетентности. Для настройки всей композиции организуется итерационный процесс, в котором эти два шага выполняются по очереди. Остаётся только понять, каким образом компоненты будут добавляться в смесь, как строить для них начальные приближения, и когда прекращать порождение новых компонент.


 \subsection*{Задачи}
 \begin{enumerate}
     \item Исследовать на выпуклость функции потерь:
     \begin{enumerate}
         \item $\widetilde{L}(M) = (1 - M)^{2}$ - квадратичная функция потерь,
         \item $\widetilde{L}(M) = 2(1 + e^{M})^{-1}$ - сигмоидная функция потерь.
     \end{enumerate}
     %\item Почему функция $SoftMax$ нельзя заменить простой нормировкой вида:
     %\begin{equation*}
     %    \widetilde{g}_{t}(x) = \dfrac{g_{t}(x)}{\sum_{i = 1}^{T}g_{t}(x)}.
     %\end{equation*}
     \item Дана задача линейной регрессии для пары экспертов. Пусть выбраны функции компетентности сигмоидного типа с $n = 1$, $\beta_{1, 2} = 0$, $\alpha_{1} = 0,14$ и $\alpha_{2} = 3,14$. Определить предсказание "двух экспертов" в точке 0,5 , если эксперты имеют вид $\mathcal{N}_{1}$(0, 2) и $\mathcal{N}_{2}$(1, 1).

     \item Дано: два решающих пня с порогами 5 и 10, веса объектов одинаковы и равны 0,1 . Найти функции компетентности для данных решающих пней из функций вида:
     \begin{equation*}
         g(x;\alpha, \beta) = e^{-\beta (x - \alpha)^{2}}.
     \end{equation*}

     Данные для задачи:
     \begin{table*}[h]
     \centering
\begin{tabular}{|c|c|c|}
\hline
 i&         x_{i}     &    y_{i}         \\ \hline
0 & 0,744  & \cellcolor[HTML]{3166FF}-1 \\ \hline
1 & 1,619  & \cellcolor[HTML]{3166FF}-1 \\ \hline
2 & 4,694  & \cellcolor[HTML]{3166FF}-1 \\ \hline
3 & 4,882  & \cellcolor[HTML]{FE0000}+1  \\ \hline
4 & 6,08   & \cellcolor[HTML]{3166FF}-1 \\ \hline
5 & 8,726  & \cellcolor[HTML]{FE0000}+1  \\ \hline
6 & 10,753 & \cellcolor[HTML]{FE0000}+1  \\ \hline
7 & 11,754 & \cellcolor[HTML]{3166FF}-1 \\ \hline
8 & 18,349 & \cellcolor[HTML]{FE0000}+1  \\ \hline
9 & 18,669 & \cellcolor[HTML]{FE0000}+1  \\ \hline
\end{tabular}
\end{table*}
 \end{enumerate}

 \subsection*{Ответы к задачам}
 \begin{enumerate}
    \item (a) - выпуклая, (b) - невыпуклая.
    \item (В работе).
    \item (В работе)
 \end{enumerate}
