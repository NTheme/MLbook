\subsection*{Причины выбора нестандартных функций потерь. Робастная регрессия.}

Нестандартные функции потерь находят применение в задачах, где стандартные методы, такие как квадратичная функция потерь, оказываются недостаточно эффективными. Например, при наличии выбросов в данных метод наименьших модулей (\(L(\varepsilon) = |\varepsilon|\)) демонстрирует устойчивость, обеспечивая медианную оценку вместо чувствительного к выбросам среднего. Квантильная регрессия обобщает этот подход, позволяя различно штрафовать положительные и отрицательные ошибки, что полезно при асимметричных требованиях к модели. Кроме того, нестандартные функции потерь применяются для учёта специфических метрик, работы с шумными данными, несбалансированных классов и в задачах, где ошибки различной природы требуют разного подхода к оптимизации. Эти подходы позволяют улучшить устойчивость и адаптируемость модели в сложных сценариях. Итого, каждая из этих функций потерь оптимизирует модель под конкретные требования и помогает улучшить результаты в зависимости от особенностей данных. Так что важно помнить, что выбор нестандартной функции потерь зависит от конкретной задачи, и должен быть хорошо обоснован.


\section*{Примеры нестандартных функций потерь}

\begin{enumerate}
    \item \text{Взвешенная кросс-энтропия (Weighted Cross-Entropy Loss)} — используется в задачах классификации с дисбалансом классов, где присваиваются разные веса для классов, чтобы избежать доминирования более часто встречающихся классов.
    
    \item \text{Фокальная потеря (Focal Loss)} — помогает акцентировать внимание на сложных примерах, особенно при сильно дисбалансированных данных. Уменьшает вес для легко классифицируемых примеров.
    
    \item \text{Huber Loss} — более устойчива к выбросам, чем среднеквадратичная ошибка (MSE), комбинирует преимущества L1 и L2 потерь.
    
    \item \text{Log-Cosh Loss} — ещё одна альтернатива MSE, минимизирует логарифм гиперболического косинуса разности между предсказанным и истинным значением, за счёт чего менее чувствительна к выбросам.
    
    \item \text{KL-дивергенция (Kullback-Leibler Divergence)} — измеряет расхождение между двумя вероятностными распределениями, часто используется в задачах с вероятностными моделями.
    
    \item \text{Smooth L1 Loss} — используется для задач регрессии, сочетает в себе преимущества L1 и L2 потерь, особенно полезна для задачи, где нужно уменьшить влияние выбросов, но без резких изменений.
    
    \item \text{Contrastive Loss} — используется для обучения на парных данных, минимизирует расстояние между похожими объектами, увеличивает расстояние между различными.
    
    \item \text{Cosine Embedding Loss} — используется для задач, связанных с измерением сходства между объектами, использует косинусное расстояние для минимизации или максимизации сходства.
    
    \item \text{Quantile Loss} — применяется для регрессии с целью предсказания определенных квантилей, например, медианы или других перцентилей, вместо среднего значения.
    
    \item \text{Earth Mover’s Distance (Wasserstein Loss)} — используется в генеративных моделях (например, в Wasserstein GAN), измеряет расстояние между распределениями, основываясь на идее перемещения массы.
    
    \item \text{Dice Loss} — часто используется в задачах сегментации изображений, особенно для классов с малым количеством примеров, улучшает результат, направляя внимание на малые области.
    
    \item \text{Perceptual Loss} — используется в генеративных задачах, когда важно сравнивать изображения на уровне высокоуровневых признаков, а не только пиксельных значений.
    
    \item \text{RankNet Loss} — применяется для задач ранжирования, основана на вероятностных парных сравнениях элементов.
\end{enumerate}


\subsection*{Робастная регрессия}
Робастная регрессия представляет собой подход к построению моделей, устойчивых к выбросам и шуму в данных. В отличие от стандартных методов, таких как линейная регрессия с квадратичной функцией потерь, робастные методы минимизируют влияние аномалий, обеспечивая более надёжные результаты в задачах с ненормальным распределением ошибок.

\subsubsection*{Метод наименьших модулей}
Метод наименьших модулей использует функцию потерь $L(\varepsilon) = |\varepsilon|$, которая штрафует отклонения линейно. Это делает метод устойчивым к выбросам, так как большие ошибки не оказывают квадратичного влияния на итоговый результат. Оптимальное значение параметра $a$ в случае постоянной модели соответствует медиане распределения отклонений.

\subsubsection*{Huber Loss}
Функция потерь Хьюбера объединяет квадратичную и линейную функции потерь:

Параметр $\delta$ контролирует границу между квадратичным и линейным режимами. Это позволяет сохранять чувствительность к малым ошибкам, но снижать влияние крупных выбросов.

\subsubsection*{Correntropy Loss}
Эта функция потерь использует концепцию коррентропии для уменьшения влияния выбросов. Она определяется как:

где $\sigma$ — параметр, управляющий шириной окна коррентропии. Этот метод применим в задачах с плотным шумом и высокими выбросами.

Более подробно эти методы рассматриваются в следующих параграфах данной главы.

\subsection*{Применение робастной регрессии}
Робастные методы широко применяются в задачах, где выбросы могут существенно искажать результаты. Примеры включают:
\begin{itemize}
\item экономические данные с редкими, но значительными аномалиями;
\item обработку изображений, где пиксельные выбросы могут нарушить модели;
\item медицину, где некорректные измерения могут искажать анализ.
\end{itemize}


\subsection*{Задача 1.}
В процессе разработки модели для задачи классификации редких заболеваний на медицинских изображениях, вы столкнулись с сильным дисбалансом классов: большинство изображений не содержат признаков заболевания, а лишь небольшая часть включает патологические изменения. Какую нестандартную функцию потерь вы бы выбрали для обучения модели и почему? Какие потенциальные проблемы могут возникнуть при использовании стандартной функции потерь (например, кросс-энтропии) в такой задаче, и как выбранная вами функция потерь поможет решить эти проблемы?

\subsubsection*{Решение}
В данной задаче с сильно дисбалансированными классами эффективным выбором может быть фокальная потеря (Focal Loss). Эта функция потерь нацелена на уменьшение веса для примеров, которые легко классифицируются, и увеличение веса для сложных, редко встречающихся примеров. Это помогает модели сосредоточиться на трудных случаях (патологических изменениях), что особенно важно при анализе редких заболеваний. Стандартная кросс-энтропия в данной задаче может привести к тому, что модель будет игнорировать редкие положительные примеры (болезнь), так как она будет слишком часто предсказывать отрицательный класс, который доминирует в данных. Фокальная потеря решает эту проблему, увеличивая влияние на редкие примеры, что способствует лучшему обучению на таких данных.

\subsection*{Задача 2.}

В задаче бинарной классификации имеются два класса: класс $A$, содержащий 95\% объектов, и класс $B$, содержащий 5\% объектов. Если использовать стандартную бинарную кросс-энтропию в качестве функции потерь, модель склонна недооценивать вклад класса $B$. Почему стандартная функция потерь плохо справляется с задачей? Предложите подходящую модификацию функции потерь для решения проблемы и объясните её преимущества.

\subsubsection*{Решение}

Стандартная бинарная кросс-энтропия определяется как:
\[
\mathcal{L}_{\text{BCE}} = - \frac{1}{N} \sum_{i=1}^N \left[ y_i \log(p_i) + (1-y_i)\log(1-p_i) \right],
\]
где $y_i \in \{0, 1\}$ — истинная метка класса, $p_i \in [0, 1]$ — вероятность, предсказанная моделью.

В случае сильного дисбаланса классов, модель может минимизировать функцию потерь, просто предсказывая класс $A$ (большинство), игнорируя меньшинство $B$. Таким образом, стандартная функция потерь не учитывает важность редкого класса.

Для решения этой проблемы используют взвешенную кросс-энтропию:
\[
\mathcal{L}_{\text{weighted}} = - \frac{1}{N} \sum_{i=1}^N \left[ w_A y_i \log(p_i) + w_B (1-y_i)\log(1-p_i) \right],
\]
где веса $w_A$ и $w_B$ определяются как обратные пропорции к числу объектов соответствующих классов:
\[
w_A = \frac{1}{N_A}, \quad w_B = \frac{1}{N_B}.
\]

Также отметим основные преимущества взвешенной функции потерь:
\begin{itemize}
    \item Увеличивается вклад редкого класса в итоговую функцию потерь.
    \item Модель становится более чувствительной к объектам меньшинства.
\end{itemize}

\subsection*{Задача 3.}

Рассмотрим задачу регрессии, где истинные значения $y_i$ содержат выбросы. Использование стандартной MSE (среднеквадратичной ошибки) приводит к тому, что модель чрезмерно штрафуется за большие ошибки, вызванные выбросами:
\[
\mathcal{L}_{\text{MSE}} = \frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2.
\]

Почему MSE плохо справляется с выбросами? Предложите альтернативную функцию потерь и объясните, как она помогает.

\subsubsection*{Решение}

MSE акцентирует внимание на крупных ошибках из-за квадратичной зависимости. Это делает модель чувствительной к выбросам.

Для повышения робастности используют функцию потерь Хубера (Huber):
\[
\mathcal{L}_{\text{Huber}}(\delta) = \begin{cases}
\frac{1}{2}(y_i - \hat{y}_i)^2, & \text{если } |y_i - \hat{y}_i| \leq \delta, \\
\delta |y_i - \hat{y}_i| - \frac{1}{2}\delta^2, & \text{если } |y_i - \hat{y}_i| > \delta.
\end{cases}
\]

Где $\delta$ — порог, определяющий границу между квадратичным и линейным режимами.

Также отметим основные преимущества функции потерь Хубера:
\begin{itemize}
    \item При малых ошибках поведение аналогично MSE, что обеспечивает точность на корректных данных.
    \item При больших ошибках штраф линейный, что снижает влияние выбросов.
\end{itemize}










\section*{Нестандартные функции потерь. Метод наименьших модулей. Квант\'{и}льная регрессия.}

Квадратичная функция потерь обычно дает хорошие результаты, является удобной в использовании, а потому и применяется чаще всего. Но в некоторых ситуациях она все же неприменима, и приходится использовать нестандартные функции потерь.

\subsection*{Метод наименьших модулей.}

Здесь и далее будем использовать следующие обозначения: $\ell$ - количество объектов в тренировочной выборке, $x_i$ ($i \in \left\{1, \dotsc, \ell \right\}$) - вектор признаков, $y_i$ ($i \in \left\{1, \dotsc, \ell \right\}$) - таргет, $a$ - модель, $\varepsilon_i := a(x_i) - y_i$ ($i \in \left\{1, \dotsc, \ell \right\}$).

Для стандартной квадратичной функции потерь $\mathscr{L}(\varepsilon) = \varepsilon^2$ задача будет выглядеть так:
$$\frac{1}{\ell}\sum\limits_{i=1}^\ell\left(a(x_i) - y_i\right)^2 \longrightarrow \min\limits_{a}.$$

Заменим функцию потерь на $\mathscr{L}(\varepsilon) = |\varepsilon|$.
Задача станет выглядеть так:
$$\frac{1}{\ell}\sum\limits_{i=1}^\ell\left|a(x_i) - y_i\right| \longrightarrow \min\limits_{a}.$$
Данный метод называется \textit{методом наименьших модулей}.

В какой ситуации такой подход может оказаться полезным? Рассмотрим ситуацию,  когда наша модель - это константа, то есть она вообще не зависит от признаков. Для квадратичной функции потерь получим:
$$\frac{1}{\ell}\sum\limits_{i=1}^\ell\left(a - y_i\right)^2 \longrightarrow \min\limits_{a}.$$
Здесь можно посчитать ответ аналитически, оптимальной константой $a$ будет $a = \frac{1}{\ell}\sum\limits_{i=1}^\ell y_i$, то есть среднее арифметическое таргетов. Это плохая оценка, если, например, в нашей выборке присутствуют выбросы, или распределение ошибок имеет тяжёлые <<хвосты>>.

А в случае использования метода наименьших модулей задача будет выглядеть так:
$$\frac{1}{\ell}\sum\limits_{i=1}^\ell\left|a - y_i\right| \longrightarrow \min\limits_{a}.$$
Здесь опять же можно посчитать ответ аналитически, оптимальным $a$ будет $a = \text{median}\left\{ y_1, \dotsc, y_\ell \right\} = y_{(\ell / 2)}$ (серединный член вариационного ряда). Эта оценка хороша тем, что устойчива к выбросам, хорошо работает для распределений ошибок с тяжелыми <<хвостами>>.

Таким образом, использование метода наименьших модулей в некоторых ситуациях помогает бороться с выбросами.

\subsection*{Квант\'{и}льная регрессия.}

Метод наименьших модулей можно обобщить. Давайте по-разному штрафовать отрицательные и положительные ошибки. Т.е. будем рассматривать функцию потерь вида
$$\mathscr{L}(\varepsilon) = \begin{cases}
    C_+|\varepsilon|, \varepsilon \geq 0,\\
    C_-|\varepsilon|, \varepsilon < 0.
\end{cases}$$

\begin{figure}[h]
    \centering
    \includegraphics{chapters/nonstandart_error/images/ФПКР.png}
\end{figure}

Опять же рассмотрим случай, когда модель не зависит от признаков, то есть является константой:
$$\frac{1}{\ell}\sum\limits_{i=1}^\ell\mathscr{L}\left(a - y_i\right) \longrightarrow \min\limits_{a}.$$
Решение данной задачи опять же можно получить аналитически, оптимальным $a$ будет $a = y_{(q)}$ ($q$-тый член вариационного ряда), где $q = \frac{\ell C_-}{C_- + C_+}$.
Именно поэтому используемый метод называется \textit{методом квант\'{и}льной регрессии}.

Рассмотрим также еще один случай, когда квантильная регрессия оказывается хорошим вариантом с точки зрения решения задачи оптимизации, а именно случай линейной модели: $a(x) = \left\langle x, w \right\rangle$, где $w$ - вектор весов.

Сделаем замену переменных $\varepsilon_i^+ = (a(x_i) - y_i)_+$, $\varepsilon_i^- = (a(x_i) - y_i)_-$. Тогда наша задача будет иметь вид
$$\begin{cases}
    \frac{1}{\ell}\sum_{i=1}^\ell C_+\varepsilon_i^+ + C_-\varepsilon_i^- \longrightarrow \min\limits_{w},\\
    \left\langle w, x_i \right\rangle - y_i = \varepsilon_i^+ - \varepsilon_i^-, i \in \left\{ 1 , \dotsc, \ell \right\}.
\end{cases}$$

Это задача линейного программирования, для решения которой существует масса способов.

\newpage

\subsection*{Задачи.}

\subsubsection*{Задача 1.}

Предположим, у Вас есть датасет с данными о жителях некоторой страны Южной или Центральной Африки, а также данные об их среднем ежедневном доходе, и Вы хотите научиться предсказывать этот доход по значениям рассматриваемых признаков. Полученная модель в последующем будет использоваться для составления плана по оказанию гуманитарной помощи населению. Что использовать предпочтительнее: квадратичную функцию потерь или функцию потерь квант\'{и}льной регрессии? Почему? Если использовать предпочтительнее функцию потерь квант\'{и}льной регрессии, то каким должно быть соотношение параметров $C_+$ и $C_-$ и почему?

\begin{solution}
    В условии задачи не зря указано, из какого региона у нас страна. Б\'{о}льшая часть населения в ней, скорее всего, крайне бедна, при этом есть очень незначительное количество сверхбогатых людей, т.е., в нашей терминологии, выбросов. Поэтому функция потерь квант\'{и}льной регрессии более предпочтительна, чем квадратичная функция потерь.

    Теперь обратим внимание на то, как будет использована модель, чтобы понять, какая ошибка для нас наиболее страшна: положительная или отрицательная. Заметим, что если мы предсказали доход человека больше реального, т.е. ошибка положительна, то, скорее всего, на него будет выделено меньше помощи. Это кажется более страшным, чем ситуация, в которой мы предсказали доход человека меньше реального, и дали ему чуть больше помощи. Поэтому штрафовать положительные ошибки стоит сильнее, чем отрицательные, то есть стоит выбрать $C_+$ и $C_-$ так, чтобы $C_+/C_- > 1$.
\end{solution}

\subsubsection*{Задача 2.}

Предположим, что у в природе некоторый таргет действительно линейно зависит от вектора признаков, но вектор таргетов, который у нас есть, зашумлен ошбиками, причем ошибки эти приходят из симметричного распределения Коши. Почему использование квадратичной функции потерь в данном случае будет крайне нежелательным? А если ошибки приходит из распределения $\mathcal{N}(a, \sigma^2)$, где $a \neq 0$?

\begin{solution}
    Из теории вероятностей и математической статистики известно, что у распределения Коши тяжелые <<хвосты>>, из-за чего у него даже нет матожидания. Именно поэтому квадратичная функция потерь, очень сильно штрафующая за большие ошибки, здесь не подходит. А вот метод наименьших модулей подхолит лучше.

    Также можно заметить, что квадратичная функция потерь одинаково штрафует положительные и отрицательные ошибки, то есть неявно подразумевается симметричность распределения. В общем случае это не так. Квант\'{и}льная регрессия позволяет более гибко работать с несимметричными распределениями.
\end{solution}

\subsubsection*{Задача 3.}

Предположим, что Вы тестируете новое лекарство на мышах. Вы хотите понять, какая доза лекарства оптимальна: уже вылечивает мышь, но все еще не дает негативных побочных эффектов. Известно, что каждый новый побочный эффект проявляется при превышении некоторого примерно одинакового для всех мышей порога передозировки и действует все сильнее и сильнее при дальнейшем превышении этого порога. Также известно, что оптимум не единственен, а достигается на некотором отрезке, длина которого вам тоже заранее известна. Наконец, ниже некоторого порога лекарство действует все хуже и хуже. У Вас есть некоторые данные о мышах, а также таргеты - максимальные оптимальные дозы лекарств. Предложите модификацию функции потерь квант\'{и}льной регрессии, оптимальную для данной задачи.

\begin{solution}
    Основная идея квант\'{и}льной регрессии по сравнению с методом наименьших модулей состоит в том, что мы по-разному штрафуем положительные и отрицательные ошибки. Давайте разовьем эту идею, и будем по-разному штрафовать ошибки в зависимости от того, в какую часть числовой прямой мы попали.

    Так, для нашей задачи, пусть новые побочные эффекты проявляются при превышении дозировки на $a_1, \dotsc, a_n$ у.е., где $0= a_1 < \ldots < a_n$. А длина оптимального отрезка дозировки равна $b$, Тогда можно взять такую функцию потерь:
    $$\mathscr{L}(\varepsilon) = \begin{cases}
        -v_b\varepsilon, \varepsilon < -b,\\
        0, \varepsilon \in [-b; a_1],\\
        v_{a_1}\varepsilon, \varepsilon \in (a_1; a_2],\\
        (v_{a_1} + v_{a_2})\varepsilon, \varepsilon \in (a_2; a_3],\\
        \vdots\\
        (v_{a_1} + \dotsb + v_{a_{n - 1}})\varepsilon, \varepsilon \in (a_{n-1}; a_n],\\
        (v_{a_1} + \dotsb + v_{a_n})\varepsilon, \varepsilon > a_n,
    \end{cases}$$
    где $v_b, v_{a_1}, \dotsc, v_{a_n}$ - скорости роста соответствующих проблем.
\end{solution}

\newpage
\section*{SVM-Regression}

Регрессионные модели на основе метода опорных векторов (SVM-regression) зачастую используют особую функцию потерь, отличающуюся от стандартной квадратичной ошибки (MSE). В частности, классический SVM для регрессии использует \(\varepsilon\)-insensitive функцию потерь (\(\varepsilon\)-insensitive loss), которая игнорирует ошибки, величина которых меньше заранее заданного порога \(\varepsilon\). Однако существуют и нестандартные функции потерь, которые могут быть полезны в специфических задачах, например, когда необходимо учитывать асимметрию ошибок, различные веса отдельных наблюдений или же более сложные, ориентированные на распределение отклонений, формулы.

Классическая \(\varepsilon\)-insensitive функция потерь:
Для модели вида \(f(x) = \langle w, x \rangle + b\) функция потерь определяется следующим образом:
\begin{align*}
L(\varepsilon) = \max(0, | y - f(x) | - \varepsilon).
\end{align*}

Если \(| y - f(x) | \leq \varepsilon\), то штраф равен нулю, что делает модель нечувствительной к небольшим отклонениям и позволяет контролировать сложность аппроксимации.

Нестандартные функции потерь могут иметь следующие особенности:

\begin{enumerate}
    \item Асимметричные функции потерь: например, более сильное штрафование положительных отклонений, чем отрицательных, или наоборот. Это может быть полезно, если цена переоценки или недооценки целевой переменной различна.
    \item Частично-кусочные функции: использование различных режимов штрафования в зависимости от величины отклонения, например, линейная область штрафа для малых ошибок и квадратичная — для больших.
    \item Функции с весами для разных точек: если некоторые точки обучающей выборки важнее, можно ввести веса и штрафовать ошибки по более значимым точкам сильнее.
    \item Нелинейные преобразования ошибки: например, логарифмическая или экспоненциальная функция потерь, которая меняет чувствительность модели к большим отклонениям.
\end{enumerate}

Пример асимметричной \(\varepsilon\)-insensitive функции потерь:
\begin{align*}
    L(\varepsilon, \alpha) = 
        \begin{cases}
            0, & | y - f(x) | \leq \varepsilon \\
            \alpha(y - f(x)) - \varepsilon, & y - f(x) > \varepsilon \\
            \frac{(y - f(x))}{\alpha} - \varepsilon, & f(x) - y > \varepsilon
        \end{cases}
\end{align*}
где \(\alpha > 1\) задаёт степень асимметрии штрафования. Такая функция потерь позволит модели сильнее реагировать на недостаточную оценку и слабее —-- на переоценку.

\newpage
\subsection*{Задачи}

\subsubsection*{Задача 1.}
Пусть у нас есть набор точек для регрессии \(\{(x_i, y_i)\}_{i=1}^m\), и используется \(\varepsilon\)-insensitive функция потерь. Как будет влиять на итоговую модель выбор \(\varepsilon\)? Что произойдёт с моделью при увеличении и при уменьшении \(\varepsilon\)?

\subsubsection*{Решение}
При увеличении \(\varepsilon\) расширяется область, внутри которой ошибка не штрафуется. Что означает, что модель меньше старается подогнать точно каждую точку. Итоговая функция при этом может стать более гладкой и с меньшей чувствительностью к выбросам, но при этом с большой вероятностью систематической ошибки внутри расширенной области.

При уменьшении \(\varepsilon\) модель стремится к более точному описанию данных. При этом модель станет более чувствительной к шуму и выбросам.

\subsubsection{Задача 2.}
Предположим, что мы хотим использовать асимметричную \(\varepsilon\)-нечувствительную функцию потерь. Запишите целевую функцию и ограничения для такой SVM-регрессии (линейный случай), если функция потерь равна:

\begin{align*}
    L(\varepsilon) = 
    \begin{cases}
            0, & | y_i - f(x_i) | \leq \varepsilon \\
            2(y_i - f(x_i) - \varepsilon), & y_i - f(x_i) > \varepsilon \\
            f(x_i) - y_i- \varepsilon, & f(x_i) - y_i > \varepsilon
        \end{cases}
\end{align*}

\subsubsection{Решение}
Введём неотрицательные переменные \(\xi_i^+\) и \(\xi_i^-\) для верхних и нижних ошибок. Тогда ограничения на ошибки с учётом \(\varepsilon\):

\begin{align*}
    y_i - (w^Tx_i + b) \leq \varepsilon + \xi_i^+\\
    (w^Tx_i + b) - y_i \leq \varepsilon + \xi_i^-.
\end{align*}

Целевая функция с учётом асимметрии штрафов:
\begin{align*}
    \min_{w, b, \xi_i^+, \xi_i^-} \frac{1}{2} \| w \|^2 + C \sum\limits_{i = 1}^n (2\xi_i^+ + \xi_i^-)
\end{align*}

Таким образом, мы увеличили штраф в два раза для положительных отклонений (когда модель переоценивает значение, \(f(x_i) > y_i +\varepsilon\)) по сравнению с отрицательными отклонениями.

\subsubsection{Задача 3.}
Пусть у нас есть три варианта функций потерь для SVM-регрессии над одним и тем же набором данных \(\{(x_i, y_i)\}_{i=1}^m\):
\begin{enumerate}
    \item Стандартная \(\varepsilon\)-insensitive функция потерь.
    \item Модифицированная \(\varepsilon\)-insensitive функция потерь, где штраф начинается не с \(0\), а с небольшой линейной части: 
    \begin{align*}
        L(\varepsilon) = \max(0, | y - f(x) | - \varepsilon) + \delta(y - f(x)),
    \end{align*}
    где \(\delta > 0\) малый коэффициент.

    \item Huber-функция потерь с параметром \(\delta\).
\end{enumerate}

Предположим, что мы обучили три SVM-регрессии, каждый с одной из этих функций потерь, при одинаковых параметрах \(C\). Укажите, в каких случаях может быть предпочтительна Huber-функция по сравнению с 
\(\varepsilon\)-insensitive, и для чего может пригодиться модификация с добавлением линейной части \(\delta(y - f(x))\).

\subsubsection{Решение}
\begin{enumerate}
    \item Huber-функция потерь предпочтительна в ситуациях, когда в данных присутствуют выбросы или редкие, но крупные отклонения. Она сочетает в себе квадратичную форму для небольших ошибок (что способствует более мягкой подгонке и «дружелюбно» относится к шуму) и линейную для больших отклонений (не позволяя слишком сильно штрафовать крупные ошибки и тем самым уменьшая чувствительность к выбросам).
    \item Модифицированная \(\varepsilon\)-insensitive функция потерь с дополнительной линейной частью может быть полезна, когда нам важно учесть даже малые ошибки, но мы хотим сохранить идею \(\varepsilon\)-зоны. Добавление \(\delta(y - f(x))\) означает, что даже если ошибка меньше \(\varepsilon\), мы всё же учитываем некий штраф (хотя и небольшой). Это может помочь, когда не хочется полностью игнорировать малые отклонения, но при этом сохранять определённую «зону толерантности» к маленьким ошибкам, чтобы не переобучаться на шумах.
\end{enumerate}

\section*{Энтропийные функции потерь. Перекрёстная энтропия.}

В задачах классификации (бинарной или многоклассовой) очень распространено использование энтропийных функций потерь, в первую очередь перекрёстной энтропии. Эти функции потерь тесно связаны с понятиями теории вероятностей и информационной теории, в частности с понятием дивергенции Кульбака–Лейблера и энтропией Шеннона.

Основная идея: мы хотим не просто предсказать «класс», но и оценить распределение вероятностей классов. Пусть у нас есть:

\begin{itemize}
    \item Набор объектов: $(x_1, y_1), \dots, (x_\ell, y_\ell)$, где $x_i$ – вектор признаков, а $y_i$ – соответствующий истинный класс объекта (для простоты – номер класса или унитарный вектор-«one-hot»).
    \item Модель $a$, выдающая оценку распределения вероятностей по классам для входа $x_i$. Обозначим предсказанную моделью вероятность класса $k$ как $p_{ik} = a_k(x_i)$, где $\sum_k p_{ik} = 1$.
\end{itemize}

\subsection*{Бинарная классификация}

В случае двух классов, обозначим целевой класс как $y_i \in \{0,1\}$ и предсказанную моделью вероятность класса 1 для объекта $i$ как $p_i = a_1(x_i)$. Тогда функция потерь на одном объекте может быть записана как (перекрёстная энтропия для бинарного случая):

$$
\mathcal{L}(y_i, p_i) = -\left[y_i\log(p_i) + (1 - y_i)\log(1 - p_i)\right].
$$

Эта функция потерь равна отрицательному логарифму правдоподобия при условии, что $y_i$ берётся из Бернуллиевского распределения с параметром $p_i$. Минимизация этой функции потерь равносильна максимизации правдоподобия.

\subsection*{Многоклассовая классификация}

Пусть класс $y_i$ закодирован в one-hot формате как вектор $Y_i = (y_{i1}, \dots, y_{iK})$, где $y_{ik} = 1$, если объект относится к классу $k$, и 0 в противном случае. Предсказание модели есть вероятностный вектор $P_i = (p_{i1}, \dots, p_{iK})$. Тогда перекрёстная энтропия:

$$
\mathcal{L}(Y_i, P_i) = -\sum_{k=1}^{K} y_{ik}\log(p_{ik}).
$$

Оптимизируя по параметрам модели, мы стремимся сделать предсказанное распределение вероятностей $P_i$ как можно более «острым» и совпадающим с истинным распределением мишени $Y_i$ (которое в обучающих данных детерминировано и задаётся one-hot вектором).

\subsection*{Связь с KL-дивергенцией}

Перекрёстная энтропия между истинным распределением $Y_i$ и предсказанным $P_i$ связана с дивергенцией Кульбака–Лейблера:

$$
H(Y_i, P_i) = H(Y_i) + D_{\text{KL}}(Y_i \| P_i),
$$

где $H(Y_i)$ – энтропия истинного распределения (константа в рамках оптимизации), а $D_{\text{KL}}(Y_i \| P_i)$ – дивергенция Кульбака–Лейблера, которая всегда неотрицательна. Минимизируя перекрёстную энтропию, мы минимизируем $D_{\text{KL}}$, что приводит к более точному приближению истинного распределения предсказанным.

\subsection*{Модификации}

\begin{itemize}
    \item \textbf{Взвешенная перекрёстная энтропия}: для решения проблем несбалансированных классов можно вводить веса $w_k$ для каждого класса:
    $$
    \mathcal{L}(Y_i, P_i) = -\sum_{k=1}^{K} w_k y_{ik}\log(p_{ik}).
    $$
    
    \item \textbf{Label smoothing}: позволяет сгладить «жесткие» one-hot лейблы, заменяя значение 1 на $1-\alpha$ и распределяя оставшуюся массу $\alpha$ равномерно по остальным классам. Это снижает перенакрой и делает модель более устойчивой.
\end{itemize}

\newpage

\section*{Задачи}

\subsubsection*{Задача 1 (Бинарная классификация и правдоподобие)}

Пусть у нас есть задача бинарной классификации: $y_i \in \{0,1\}$, и модель $a(x_i)$, дающая оценку вероятности класса 1 для $x_i$. Предположим, что истинное распределение метки $y_i$ для данного $x_i$ – это бернуллиевское распределение с параметром $p_i = a(x_i)$. Покажите, что минимизация средней перекрёстной энтропии

$$
\frac{1}{\ell}\sum_{i=1}^{\ell} \left[-y_i\log(p_i) - (1 - y_i)\log(1 - p_i)\right]
$$

эквивалентна максимизации правдоподобия выборки. Объясните, почему это даёт статистически обоснованную функцию потерь.

\begin{solution}
Правдоподобие выборки при условии независимости объектов есть:

$$
L = \prod_{i=1}^{\ell} p_i^{y_i}(1 - p_i)^{1 - y_i}.
$$

Взяв логарифм, получим:

$$
\log L = \sum_{i=1}^{\ell} [y_i\log(p_i) + (1 - y_i)\log(1 - p_i)].
$$

Максимизация $\log L$ по параметрам модели эквивалентна минимизации

$$
-\log L = -\sum_{i=1}^{\ell} [y_i\log(p_i) + (1 - y_i)\log(1 - p_i)],
$$

что есть сумма (или среднее) перекрёстной энтропии. Таким образом, минимизация перекрёстной энтропии совпадает с максимизацией правдоподобия. Это придаёт функции потерь статистическое обоснование: мы получаем состоятельную оценку параметров модели при верном специфицировании вероятностной модели.
\end{solution}

\subsubsection*{Задача 2 (Небаланс классов)}

Предположим, что у вас есть задача многоклассовой классификации с сильно несбалансированными классами. Один из классов встречается существенно реже других. Объясните, как можно модифицировать функцию перекрёстной энтропии, чтобы придать более высокий штраф за неправильную классификацию редкого класса. Предложите конкретную формулу и аргументируйте её использование.

\begin{solution}
Стандартная перекрёстная энтропия для многоклассовой задачи:

$$
\mathcal{L}(Y_i, P_i) = -\sum_{k=1}^K y_{ik}\log(p_{ik}).
$$

Если класс $r$ встречается редко, мы можем ввести для него повышающий вес $w_r > 1$. Тогда функция потерь модифицируется так:

$$
\mathcal{L}(Y_i, P_i) = -\sum_{k=1}^K w_k y_{ik}\log(p_{ik}),
$$

где $w_k = 1$ для всех «частых» классов, а $w_r > 1$ для редкого класса. Это увеличивает штраф за ошибку на редком классе и стимулирует модель уделять ему больше внимания. В итоге модель будет «стараться» точнее предсказывать редкий класс, жертвуя немного точностью на других, более частых классах, что зачастую улучшает общую полезность модели при решении практических задач с несбалансированными данными.
\end{solution}

\subsubsection*{Задача 3 (Label Smoothing)}

Предположим, что у вас есть задача многоклассовой классификации с $K$ классами. Истинные метки заданы в виде one-hot векторов. Вы подозреваете, что модель может слишком «уверенно» переобучаться, пытаясь подогнать вероятности к очень жёсткому распределению (где истинный класс имеет вероятность 1, а остальные 0). Предложите модификацию перекрёстной энтропии (label smoothing), объясните, в чём она заключается и как именно изменится функция потерь.

\begin{solution}
Идея label smoothing состоит в том, чтобы заменить истинный one-hot вектор $Y_i$ на более «сглаженный» вектор $Y_i'$. Пусть $\alpha$ – небольшой положительный параметр сглаживания (например, 0.1). Тогда:

\begin{itemize}
    \item Для истинного класса $c_i$ объекта $i$ мы присваиваем $y_{ic_i}' = 1 - \alpha$.
    \item Для всех остальных классов $k \neq c_i$ мы присваиваем $y_{ik}' = \frac{\alpha}{K-1}$.
\end{itemize}

Таким образом, истинный вектор $(0,\dots,0,1,0,\dots,0)$ превращается в вектор, где истинный класс имеет вероятность чуть меньше 1, а остальные классы получают небольшую ненулевую вероятность.

Новая функция потерь будет выглядеть так:

$$
\mathcal{L}(Y_i', P_i) = -\sum_{k=1}^K y_{ik}' \log(p_{ik}).
$$

Поскольку $Y_i'$ теперь не является «жёстким» one-hot вектором, модель не будет излишне стремиться предсказывать для истинного класса вероятность ровно 1, что снижает риск переобучения и делает распределение прогнозов более гладким и устойчивым.
\end{solution}


\section*{Проксимальный метод.}

\subsection*{Основная идея.}

Явный вид шага градиентного спуска:

\begin{equation}
    \frac{x_{i+1}-x_i}\alpha=-\nabla f(x_k)
\end{equation}

\begin{equation}
    x_{i+1}=x_i-\alpha\nabla f(x_k)
\end{equation}

Неявный вид шага градиентного спуска:

\begin{equation}
    \frac{x_{i+1}-x_i}\alpha=-\nabla f(x_k+1)
\end{equation}

\begin{equation}
    x_{i+1}-x_i+\alpha\nabla f(x_k)=0
\end{equation}

\begin{equation}
    \nabla\left(\frac12\|x_{i+1}-x_i\|_2^2+\alpha f(x_k)\right)=0
\end{equation}

\begin{equation}
    x_{i+1}=\mathop{argmin}\limits_{x\in\mathbb{R}^n}\left(\frac12\|x-x_i\|_2^2+\alpha f(x)\right)
\end{equation}

Определим проксимальное отображение следующим образом:

\begin{equation}
    prox_r(x)=\mathop{argmin}\limits_{y\in\mathbb{R}^n}\left(\frac12\|y-x\|_2^2+r(y)\right).
\end{equation}

Идея. Пусть минимизируемый функционал принимает вид суммы гладкой (сложной) функции $f$ и негладкой Функции (простой) $r$. В таком случае из-за негладкости $r$ обычный алгоритм градиентного спуска будет работать не очень шорошо.

\begin{equation}
    0\in\nabla f(x^*)+\partial r(x^*)
\end{equation}

\begin{equation}
    x^*\in\alpha\nabla f(x^*)+(I+\alpha\partial r)(x^*)
\end{equation}

\begin{equation}
    x^*-\alpha\nabla f(x^*)\in(I+\alpha\partial r)(x^*)
\end{equation}

\begin{equation}
    x^*=(I+\alpha\partial r)^{-1}(x^*-\alpha\nabla f(x^*))
\end{equation}

\begin{equation}
    x^*=prox_{\alpha r}(x^*-\alpha\nabla f(x^*))
\end{equation}

Последнее выражение называется шагом проксимального градиентного спуска.

\subsection*{Свойства.}

\subsubsection*{Существование и единственность.}

Теорема.

Пусть r:$\mathbb{R}^n\mapsto \mathbb{R}\cap\{+\infty\}$ -- выпуклая функция, которая хотя бы в одной точке принимает конечное значение.
Тогда $prox_r$ определено однозначно.

Доказательство:

$r(y)+\frac12\|y-x\|_2^2$ сильно выпуклая функция, которая хотя бы в одной точке принимает конечное значение. Значит, $r(y)+\frac12|y-x|^2$ имеет единственный минимум, и $prox_r(x)=\mathop{argmin}\limits_{y\in\mathbb{R}^n}\left(\frac12\|y-x\|_2^2+r(y)\right)$ определено однозначно.

\subsubsection*{Основные свойства.}

Теорема.

Следующие свойства равносильны:

\begin{itemize}
\item\[prox_r(x)=y,\]
\item\[x-y\in\partial(y),\]
\item\[\forall z\in\mathbb{R}^n:\langle x-y,z-y\rangle\leq r(z)-r(y).\]
\end{itemize}

Доказательство.

Согласно определению субдиференциала:

\begin{equation}
    g\in\partial(y)\iff\forall z\in\mathbb{R}^n:\langle g,z-y\rangle\leq r(z)-r(y).
\end{equation}

Поэтому последние лва свойства эквивалентны.

Согласно определению проксимального оператора:

\begin{equation}
    prox_r(x)=y\iff y=\mathop{argmin}\limits_{z\in\mathbb{R}^n}\left(\frac12\|z-x\|_2^2+r(z)\right).
\end{equation}

То, что фунция достигает минимума в неторой функции, равносильно тому 0 -- субградиент функции в этой точке:

\begin{equation}
    y=\mathop{argmin}\limits_{z\in\mathbb{R}^n}\left(\frac12\|z-x\|_2^2+r(z)\right)\iff
    0\in\partial\left(\frac12\|y-x\|_2^2+r(y)\right).
\end{equation}

\begin{equation}
    0\in\partial\left(\frac12\|y-x\|_2^2+r(y)\right)=
    (y-x)+\partial r(y)\iff x-y\in\partial r(y).
\end{equation}

Таким образом первые два свойства эквивалентны.

\subsubsection*{Нерасширямость.}

Теорема.

$prox_r(x)$ -- firmly nonexpansive, то есть:

\begin{equation}
    \|prox_r(x)-prox_r(y)\|_2^2\leq
    \langle prox_r(x)-prox_r(y),x-y\rangle.
\end{equation}

В частности (по неравенству КБШ):

\begin{equation}
    \|prox_r(x)-prox_r(y)\|_2\leq\|x-y\|_2.
\end{equation}

Доказательство.

Из предыдущей теоремы для $y_i=prox_r(x_i)$ следует:

\begin{equation}
    \forall z_i\in\mathbb{R}^n:
    \langle x_i-y_i,z-y_i\rangle\leq r(z_i)-r(y_i).
\end{equation}

Подставляя $z_i=y_{1-i})$ и суммируя для $i$ равным 0 и 1, получаем:

\begin{equation}
    \langle x_0-y_0,y_1-y_0\rangle+
    \langle x_1-y_1,y_0-y_1\rangle\leq0,
\end{equation}

что эквивалентно:

\begin{equation}
    \|y_0-y_1\|^2\leq\langle x_0-x_1,y_0-y_1\rangle.
\end{equation}

\subsubsection*{Сохранение точки минимума.}

Покажем, что шаг градиентного спуска переводит точку минимума $x^*$ функции $f(x)+r(x)$ в себя.

$x^*$ -- минимум функции $f(x)+r(x)$:

\begin{equation}
    0=\nabla f(x^*)+\partial r(x^*)
\end{equation}

\begin{equation}
    (x^*-\alpha\nabla f(x^*))-x^*=\partial\alpha r(x^*)
\end{equation}

Доказанное ранее свойство:

\begin{equation}
    y=prox_r(x)\iff x-y\in\partial r(y).
\end{equation}

Таким образом для $x=x^*-\alpha\nabla f(x^*)$ и проксимального оператора от $\alpha r$ выполнено, что $x^*=y=prox_{\alpha r}(x)=prox_{\alpha r}(x^*-\alpha\nabla f(x^*))$.

\subsection*{Сходимость проксимального градиентного спуска.}

Во всех теоремах минимизируется функция $\phi(x)=f(x)+r(x)$. Требования к функциям $f(x)$ и $r(x)$:

$f$ выпукло и дифференцируемо. $\nabla f$ -- Липшицева функция с константой $L$.

$r$ выпукло, значение фунции $prox_{\alpha r}$ можно вычислить.

Шаг $\alpha$ поястоянен и не превосходит $\frac1L$.

\subsubsection*{Сходимость в общем случае.}

Теорема.

Шаг проксимального градинтного спуска:

\begin{equation}
    x_{k+1}=prox_{\alpha r}(x_k-\alpha\nabla f(x_k)).
\end{equation}

Тогда проксимальный градинтный спуск с шагом $\alpha=\frac1L$ сходится со скоростью:

\begin{equation}
    \phi(x_k)-\phi(x^*)\leq\frac{L\|x_0-x^*\|_2^2}{2k}.
\end{equation}

Доказательство.

Определим $ G_\alpha(x)$:

\begin{equation}
    G_\alpha(x)=\frac1\alpha\left(x-prox_{\alpha r}(x-\alpha\nabla f)\right).
\end{equation}

Из выпуклости $f$ и L-липшицевости $\nabla f$:

\begin{equation}
\begin{aligned}
    f(x_{k+1})\leq
    f(f_k)+\langle\nabla f(x_k),x_{k+1}-x_k\rangle+\frac L2\|x_{k+1}-x_k\|_2^2\leq\\\leq
    f(x)-\langle\nabla f(x_k),x-x_k\rangle+\langle\nabla f(x_k),x_{k+1}-x_k\rangle+\frac L2\|\alpha G_\alpha(x_k\|_2^2=\\=
    f(x)+\langle\nabla f(x_k),x_{k+1}-x\rangle+\frac{\alpha^2L}2\|G_\alpha(x_k)\|_2^2.
\end{aligned}
\end{equation}

Из свойств проксимального оператора:

\begin{equation}
\begin{aligned}
    G_\alpha(x_k)-\nabla f(x_k)=
    \frac1\alpha\left(x_k-\alpha\nabla f(x_k)-prox_{\alpha r}(x_k-\alpha\nabla f(x_k))\right)\in\\\in\partial r(prox_{\alpha r}(x_k-\alpha\nabla f(x_k))=\partial r(x_{k+1}).
\end{aligned}
\end{equation}

Согласно определению субградиента выпуклой функции $r$:

\begin{equation}
    r(x)\geq r(x_{k+1})+\langle G_\alpha(x_k)-\nabla f(x_k),x-x_{k+1}\rangle.
\end{equation}

Разность неравенств на $f$ и $r$ даёт неравенство на $\phi(x)=f(x)+r(x)$:

\begin{equation}
\begin{aligned}
    \phi(x_{k+1})-\phi(x)\leq\langle\nabla f(x_{k+1}),x_{k+1}-x\rangle+\frac{\alpha^2L}2\|G_\alpha(x_k)\|_2^2-\\-\langle G_\alpha(x_k)-\nabla f(x_{k+1}),x-x_{k+1}\rangle=
    \langle G_\alpha(x_k),x_{k+1}-x\rangle+\frac{\alpha^2L}2\|G_\alpha(x_k)\|_2^2=\\=
    \langle G_\alpha(x_k),x_k-x\rangle-\langle G_\alpha(x_k),\alpha G_\alpha(x_k)\rangle+\frac{\alpha^2L}2\|G_\alpha(x_k)\|_2^2=\\=
    \langle G_\alpha(x_k),x_k-x\rangle+\frac\alpha2(\alpha L-2)\|G_\alpha(x_k)\|_2^2\leq
    \langle G_\alpha(x_k),x_k-x\rangle-\frac\alpha2\|G_\alpha(x_k)\|_2^2.
\end{aligned}
\end{equation}

Подставив $x=x_k$ получаем убывание значения функции $\phi(x_k)$:

\begin{equation}
    \phi(x_{k+1})-\phi(x_k)\leq-\frac\alpha2\|G_\alpha(x_k)\|_2^2.
\end{equation}

Подставим $x=x^*$:

\begin{equation}
\begin{aligned}
    \phi(x_{k+1})-\phi(x^*)\leq
    \langle G_\alpha(x_k),x_k-x^*\rangle-\frac\alpha2\|G_\alpha(x_k)\|_2^2=\\=
    \frac1{2\alpha}\left(2\langle\alpha G_\alpha(x_k),x_k-x^*\rangle-\|\alpha G_\alpha(x_k)\|_2^2-\|x_k-x^*\|_2^2+\|x_k-x^*\|_2^2\right)=\\=
    \frac1{2\alpha}\left(\|x_k-x^*\|_2^2-\|x_k-\alpha G_\alpha(x_k)-x^*\|_2^2\right)=\\=
    \frac1{2\alpha}\left(\|x_k-x^*\|_2^2-\|x_{k+1}-x^*\|_2^2\right).
\end{aligned}
\end{equation}

Просуммируем неравенство для разных $k$:

\begin{equation}
\begin{aligned}
    \sum\limits_{i=1}^k(\phi(x_i)-\phi(x^*))\leq
    \sum\limits_{i=1}^k\frac1{2\alpha}\left(\|x_{i-1}-x^*\|_2^2-\|x_i-x^*\|_2^2\right)=\\=
    \frac1{2\alpha}\left(\|x_0-x^*\|_2^2-\|x_k-x^*\|_2^2\right)\leq
    \frac1{2\alpha}\|x_0-x^*\|_2^2.
\end{aligned}
\end{equation}

Поскольку $\phi(x_k)$ -- убывающая последовательность:

\begin{equation}
    \phi(x_k)-\phi(x^*)\leq
    \frac1k\sum\limits_{i=1}^k(\phi(x_i)-\phi(x^*))\leq
    \frac{\|x_0-x^*\|_2^2}{2\alpha k}.
\end{equation}

Для $\alpha$ в точности равного $\frac1L$:

\begin{equation}
    \phi(x_k)-\phi(x^*)\leq
    \frac1k\sum\limits_{i=1}^k(\phi(x_i)-\phi(x^*))\leq
    \frac{L\|x_0-x^*\|_2^2}{2k}.
\end{equation}

\subsubsection*{Сходимость в сильно выпуклом случае.}

Теорема.

Шаг проксимального градинтного спуска:

\begin{equation}
    x_{k+1}=prox_{\alpha r}(x_k-\alpha\nabla f(x_k)).
\end{equation}

На $f(x)$ дополнительно налагается требование $\mu$-сильной выпуклости.

Тогда проксимальный градинтный спуск с шагом $\alpha=\frac1L$ сходится со скоростью:

\begin{equation}
    \|x_k-x^*\|_2^2\leq(1-\alpha\mu)^k\|x_0-x^*\|_2^2.
\end{equation}

Доказательство.

Подставим шаг проксимального градинтного спуска:

\begin{equation}
\begin{aligned}
    \|x_{k+1}-x^*\|_2^2=
    \|prox_{\alpha r}(x_k-\alpha\nabla f(x_k))-x^*\|_2^2=\\=
    \|prox_{\alpha r}(x_k-\alpha\nabla f(x_k))-
    prox_{\alpha r}(x^*-\alpha\nabla f(x^*))\|_2^2\leq\\\leq
    \|x_k-\alpha\nabla f(x_k)-x^*+\alpha\nabla f(x^*)\|_2^2=\\=
    \|x_k-x^*\|_2^2-
    2\alpha\langle\nabla f(x_k)-\nabla f(x^*),x_k-x^*\rangle+
    \alpha^2\|\nabla f(x_k)-\nabla f(x^*)\|_2^2.
\end{aligned}
\end{equation}

Из L-липшицевости $\nabla f$:

\begin{equation}
    \alpha^2\|\nabla f(x_k)-\nabla f(x^*)\|_2^2\leq
    2L\left(f(x_k)-f(x^*)-\langle\nabla f(x^*),x_k-x^*\rangle\right).
\end{equation}

Из сильной выпуклости $f$:

\begin{equation}
    \langle\nabla f(x_k)-\nabla f(x^*),x_k-x^*\rangle\geq
    \left(f(x_k)-f(x^*)+\frac\mu2\|x_k-x^*\|_2^2\right)+\langle\nabla f(x^*),x_k-x^*\rangle.
\end{equation}

Подставим:

\begin{equation}
\begin{aligned}
    \|x_{k+1}-x^*\|_2^2\leq
    \|x_k-x^*\|_2^2-
    2\alpha\left(f(x_k)-f(x^*)+\frac\mu2\|x_k-x^*\|_2^2\right)-\\-
    2\alpha\langle\nabla f(x^*),x_k-x^*\rangle+
    2\alpha^2 L\left(f(x_k)-f(x^*)-\langle\nabla f(x^*),x_k-x^*\rangle\right)=\\=
    (1-\alpha\mu)\|x_k-x^*\|_2^2+
    2\alpha(\alpha L-1)\left(f(x_k)-f(x^*)-\langle\nabla f(x^*),x_k-x^*\rangle\right).
\end{aligned}
\end{equation}

Поскольку $\alpha\leq\frac1L$ и из выпуклости функции $f$ следует $f(x_k)-f(x^*)-\langle\nabla f(x^*),x_k-x^*\geq0$, выполнено:

\begin{equation}
    \|x_{k+1}-x^*\|_2^2\leq(1-\alpha\mu)\|x_k-x^*\|_2^2.
\end{equation}

Таким образом по индукции:

\begin{equation}
    \|x_k-x^*\|_2^2\leq(1-\alpha\mu)^k\|x_0-x^*\|_2^2.
\end{equation}

\subsubsection*{Сходимость ускоренного проксимального градиентного метода.}

Теорема.

Шаг ускоренного проксимального градинтного метода:

\begin{equation}
    y_k=prox_{\alpha r}(x_{k-1}-\alpha\nabla f({k-1})).
\end{equation}

\begin{equation}
    x_k=y_k+\frac{k-1}{k+2}(x_k-x_{k-1}).
\end{equation}

Ускоренный проксимальный градинтный спуск с шагом $\alpha=\frac1L$ сходится со скоростью:

\begin{equation}
    \phi(x_k)-\phi(x^*)\leq\frac{2L\|x_0-x^*\|_2^2}{k^2}.
\end{equation}

\subsection*{Задачи.}

\subsubsection*{Задача 1.}

Показать, что оператор проекции на множество -- частный случай проксимального оператора.

Идея: использовать индикатор множества:

\begin{equation}
    I_M(x)=\begin{cases}
        0,&x\in M\\
        +\infty,&x\not\in M
    \end{cases}.
\end{equation}

\subsubsection*{Задача 2.}

Найти $prox_{\alpha\|\cdot\|_1}(x)$ и $prox_{\alpha\|\cdot\|_2^2}(x)$.

Идея: минимизурумая функция представляется в виде покоординатной суммы, и каждую координату можно минимзировать отдельно.

Ответ:

\begin{equation}
    prox_{\alpha\|\cdot\|_2^2}(x)=\frac{x}{1+\alpha},
\end{equation}

\begin{equation}
    prox_{\alpha\|\cdot\|_1}(x_i)=\begin{cases}
        x-\alpha&x\geq\alpha\\
        0&|x|\leq\alpha\\
        x+\alpha&x\leq-\alpha\\
    \end{cases}.
\end{equation}

\subsubsection*{Задача 3.}

Написать алгоритм оптимизации ElasticNet методом проксимального градиентного спуска.

Идея: использовать ответ предыдущей задачи и стандартный алгоритм проксимального градиентного спуска.


\section*{Функции потерь для задач несбалансированной классификации}

\subsection*{Введение}

В задачах классификации, где данные имеют несбалансированное распределение классов, выбор функции потерь играет критическую роль в обучении моделей. Несбалансированные классы могут привести к тому, что стандартные функции потерь, такие как кросс-энтропия, будут недостаточно чувствительны к меньшинственным классам, что может негативно сказаться на качестве предсказаний. В этом параграфе будут рассмотрены подходы к созданию функций потерь, которые учитывают диспропорцию между классами и помогают модели лучше справляться с трудными для классификации примерами.

Введем следующие обозначения:
\\\indent $X$ -- пространство признаков;
\\\indent $x \in X$ -- объект;
\\\indent $w$ -- параметры модели;
\\\indent $C$ -- количество классов, которые могут принимать значения от 1 до $C$;
\\\indent $p=p(x,w) \in [0, 1]^C$ -- вектор предсказанных моделью вероятностей;
\\\indent $y=y(x) \in \{1,\ldots,C\}$ -- истинный класс объекта $x$;
\\\indent $\mathcal{L}(p,y) = \mathcal{L}(p(x, w), y(x))$ -- функция потерь.

\subsection*{Focal Loss}

Focal Loss является динамически масштабируемой модификацией Cross Entropy, которая снижает вклад хорошо классифицированных примеров в задачах с сильно несбалансированными классами, сосредоточивая обучение на трудных примерах.

\subsubsection*{Cross Entropy}

Введем Focal Loss (FL) с рассмотрения Cross Entropy (CE) для бинарной классификации

\[ 
    \text{CE}(p, y) = \begin{cases}
        -\log(p) & \text{if } y=1 \\
        -\log(1-p) & \text{otherwise,}
    \end{cases}
\] 
где $p$ -- предсказанная вероятность класса 1, а $y$ —- метка истины (0 или 1).

Для удобства обозначим
\[
    p_t = \begin{cases}
        p & \text{if } y=1 \\
        1-p & \text{otherwise,}
    \end{cases}
\]
тогда перепишем
\[
    \text{CE}(p, y) = \text{CE}(p_t) = -\log(p_t).
\]
 
Balanced CE с весами $\alpha\in[0,1]$ для класса 1 и $1-\alpha$ для класса 0 будет записываться как
\[
    \text{CE}(p_t) = -\alpha_t\log(p_t).
\]

Эксперименты показывают, что большой дисбаланс классов подавляет функцию потерь CE. Легко классифицируемые объекты составляют большую часть потери и доминируют в градиенте.

\subsubsection*{Математическая формулировка}

Предлагается изменить форму функции потерь, чтобы уменьшить влияние легких примеров и, тем самым, сосредоточить обучение на трудных. Для этого добавим модулирующий фактор $(1-p_t)^\gamma$ к CE с настраиваемым параметром фокусировки $\gamma$. Таким образом, определим фокусную потерю как:
\[
    \text{FL}(p_t)=-\alpha_t(1-p_t)^\gamma\log(p_t).
\]

Перепишем, раскрыв обозначения:
\[ 
    \text{FL}(p, y) = -\alpha y(1 - p)^{\gamma} \log(p) - (1 - \alpha)(1 - y) p^{\gamma} \log(1 - p) 
\] 

Аналогично можно определить Focal Loss для категориальной классификации
\[
    \text{FL}(p,y)=-\alpha_y (1-p_y)^\gamma\log(p_y),
\]
где
\\\indent $\alpha_i \ge 0$ -- вес класса $i$, компенсирующий несбалансированность классов;
\\\indent $\gamma \ge 0$ -- фокусирующий параметр, который контролирует степень влияния трудных примеров.

\subsection*{Class Balanced Loss}

\subsubsection*{Выборка данных как случайное покрытие}

Пусть $S$ -- множество всех возможных данных в пространстве признаков заданного класса. Будем предполагать, что 
$S$ имеет объем, равный $N\ge1$, а каждый объект представляет собой подмножество $S$ с объемом 1. Выборку можно рассматривать как случайное покрытие этими объектами. Ожидаемый объем выбранных точек растет с их увеличением числа и ограничен $N$. 

\begin{definition}
    Эффективное число выборки $E_n$ -- это ожидаемый объем выбранных $n$ точек.
\end{definition}

Вычисление этого объема сложно и зависит от формы выборки и размерности пространства. Для упрощения не будем учитывать частичное перекрытие: новая точка может быть либо внутри ранее выбранных данных (с вероятностью $p$), либо снаружи (с вероятностью $1-p$).

\begin{proposition}
    $E_n=(1-\beta^n)/(1-\beta)$, где $\beta=(N-1)/N$
\end{proposition}

\subsubsection*{Математическая формулировка}

Class Balanced Loss предлагает решить проблему несбалансированных классов, добавив в функцию потерь вес, обратно пропорциональный эффективному числу объектов соответствующего класса. 

Пусть для объекта $x$ с классом $y$ предсказаны вероятности $p\in[0,1]^C$. Обозначим функцию потерь $\mathcal{L}(p,y)$. Предложенное эффективное число для класса $i\in\{1,\ldots,C\}$ есть $E_{n_i}=(1-\beta_i^{n_i})/(1-\beta_i)$, где $n_i$ -- число объектов класса $i$ из выборки, $\beta_i=(N_i-1)/N_i$.

Без дополнительной информации о данных для каждого класса трудно эмпирически найти набор хороших гиперпараметров $N_i$ для всех классов. Поэтому на практике мы предполагаем, что $N_i$ зависит только от набора данных, и положим $N_i = N$, $\beta_i = \beta = (N - 1)/N$ для всех $i$.

Тогда Class Balanced Loss определяется следующим образом:
\[
    \mathcal{L}_\text{CB}(p,y) = \dfrac{1}{E_{n_y}}\mathcal{L}(p,y) = \dfrac{1-\beta}{1-\beta^{n_y}}\mathcal{L}(p,y),
\]
где $\beta \in [0,1)$ — гиперпараметр, который контролирует степень балансировки.

\begin{remark}
    Class Balanced Loss можно использовать в сочетании с различными функциями потерь, например, с Focal Loss
    \[
        \text{FL}_\text{CB}(p,y)=-\alpha_y\dfrac{1-\beta}{1-\beta^{n_y}}(1-p_y)^\gamma\log(p_y),
    \]
\end{remark}

\subsection*{Задачи}

\begin{problem}
    Покажите, что $\text{FL}(p,y)$ является выпуклой относительно первого (прогностического) аргумента $p$ для всех $\gamma \ge 0$ как для задачи бинарной классификации, так и категориальной.
\end{problem}

\begin{solution}
    Начнем с бинарной классификации. Рассмотрим отдельно особые случаи $\gamma=0$ и $\gamma=1$. При $\gamma=0$ FL совпадает с CE, которая является выпуклой. При $\gamma=1$:
    \[
        \dfrac{\partial^2\left(-(1-p)\log(p)\right)}{\partial p^2}=\dfrac{1+p}{p^2}>0 \quad \forall p \in (0,1).
    \]
    
    Теперь рассматриваем случаи, когда $\gamma\notin\{0,1\}$:
    \[
        \dfrac{\partial^2\left(-(1-p)^{\gamma-1}\log(p)\right)}{\partial p^2}=\dfrac{\gamma(1-p)^{\gamma-1}}{p}-\gamma(\gamma-1)(1-p)^{\gamma-2}\log(p)-\dfrac{-\gamma(1-p)^{\gamma-1}p-(1-p)^\gamma}{p^2}.
    \]
    Перепишем в виде квадратного трехчлена относительно $\gamma$:
    \[
        \gamma^2\left(-\log(p)(1-p)^{\gamma-2}\right)+\gamma\left(\dfrac{2(1-p)^{\gamma-1}}{p}+(1-p)^{\gamma-2}\log(p)\right)+\dfrac{(1-p)^\gamma}{p^2}.
    \]
    $\forall p \in (0,1)$ все коэффициенты этого квадратного трехчлена положительны, поэтому этот трехчлен тоже положителен $\forall \gamma>0$.

    Для категориальной классификации покажем положительную определенность гессиана. Его диагональные элементы положительны, поскольку совпадают с производными выше, недиагональные равны 0, так как FL зависит от предсказанной вероятности только истинного класса:
    \[
        \dfrac{\partial^2\text{FL}}{\partial p_i \partial p_j}=0 \quad \forall i \neq j.
    \]
    Матрицы с такой структурой всегда положительны определены, Q.E.D.
\end{solution}

\begin{problem}
    В пункте <<Выборка данных как случайное покрытие>>  доказать предложение
    \[
        E_n=\dfrac{1-\beta^n}{1-\beta}.
    \]
\end{problem}

\begin{solution}
    Доказательство по индукции. Очевидно, что при $E_1=1$, так как при выборе одного объекта нет перекрытий, поэтому база индукции выполняется:
    \[
        E_1=\dfrac{1-\beta^1}{1-\beta}=1.
    \]

    Пусть ожидаемый объем выборки на шаге $n-1$ равным $E_{n-1}$, тогда вероятность новой точки быть покрытой $p=E_{n-1}/N$. Таким образом, ожидаемый объем выборки на $n$-лм шаге
    \[
        E_n=pE_{n-1}+(1-p)(E_{n-1}+1)=1+\dfrac{N-1}{N}E_{n-1}.
    \]
    Пользуясь предположением индукции для $n-1$ шага, запишем
    \[
        E_n=1+\beta\dfrac{1-\beta^{n-1}}{1-\beta}=\dfrac{1-\beta+\beta+\beta^{n}}{1-\beta}=\dfrac{1-\beta^{n}}{1-\beta},
    \]
    переход доказан, Q.E.D.
\end{solution}

\begin{problem}
    В пункте <<Выборка данных как случайное покрытие>>  покажите, что если $N$ велико, то можно считать, что каждый объект уникален, а при $N=1$ -- класс представим единственным объектом.
\end{problem}

\begin{solution}
    Заметим, что $\beta=0$ при $N=1$ и $\beta \to 1$ при $N \to \infty$. Тогда
    \[
        E_n=\begin{cases}
            \dfrac{1-0^n}{1-0}=1 & \text{if } N=1 \\
            \lim\limits_{\beta \to 1}\dfrac{1-\beta^n}{1-\beta}=\lim\limits_{\beta \to 1}\dfrac{-n\beta^{n-1}}{-1}=n & \text{if } N \to \infty
        \end{cases}.
    \]
    При большом $N$ эффективное число выборки равно ее размеру. Это значит, что нет перекрытия данных, следовательно каждый объект уникален. С другой стороны, если $N=1$, $E_n=1$, что означает существования единственного прототипа, так что все данные в этом классе могут быть представлены им посредством аугментации данных, преобразований и т.п., Q.E.D.
\end{solution}


\section*{Triplet Loss}

Triplet Loss является одной из нестандартных функций потерь, используемых для обучения моделей, которые могут различать объекты на основе их векторных представлений. Векторное представление объектов называется эмбеддингом. Основная идея Triplet Loss заключается в формировании триплетов, состоящих из трех элементов: анкерного примера (anchor), положительного примера (positive) и отрицательного примера (negative).

\subsection*{Структура выборки}

Рассмотрим задачу построения эмбеддингов для предложений. Пусть задана выборка:

$$
\mathfrak{D} = \{s_i, p_i, n_i\}_{i=1}^{l},
$$

где $s_i$ — это некоторое предложение, $p_i$ — множество индексов предложений, близких по смыслу к данному, а $n_i$ — множество индексов предложений, далеких от данного.

Требуется построить отображения $f : \mathcal{S} \to \mathbb{R}^{n}$ так, чтобы выполнялись следующие условия:

$$
\sum_{i=1}^{l}\sum_{j \in p_i} \|f(s_i) - f(s_j)\| \to \min
$$

$$
\sum_{i=1}^{l}\sum_{j \in n_i} \|f(s_i) - f(s_j)\| \to \max
$$

Иногда вместо $\|\cdot\|$ используется косинусное расстояние между векторами, но это несущественно для данной задачи.

\subsection*{Функционал качества}

Самый простой функционал качества, который используется на практике, можно записать следующим образом:

$$
L(f) = \sum_{i=1}^{l}\max\left\{0, \sum_{j \in p_i}\|f(s_i) - f(s_j)\| - \sum_{j \in n_i}\|f(s_i) - f(s_j)\| + \text{margin}\right\} \to \min_{f}.
$$

Где $f$ — это некоторая параметрическая функция, и настройка параметров происходит при помощи градиентных методов. На каждом шаге генерируется батч размера $l'$, который состоит из множества троек $(s_i, \text{positive}_i, \text{negative}_i)$, где $s_i$ — одно предложение из выборки, $\text{positive}_i$ — одно предложение, похожее на $s_i$, а $\text{negative}_i$ — одно предложение, не похожее на $s_i$.

Тогда на каждом шаге решается задача оптимизации:

$$
L'(f) = \sum_{i=1}^{l}\max\left\{0, \|f(s_i) - f(\text{positive}_i)\| - \|f(s_i) - f(\text{negative}_i)\| + \text{margin}\right\} \to \min_{f}.
$$

\subsection*{Применение Triplet Loss}

Triplet Loss применяется в различных задачах, включая распознавание лиц, поиск похожих изображений и группировку текстов. Эта функция потерь позволяет моделям обучаться более эффективно, поскольку она фокусируется на относительных расстояниях между примерами, что важно для задач, связанных с представлением данных в многомерном пространстве.

\subsubsection*{Задача 1: Распознавание лиц}

\textbf{Описание:} Реализуйте модель для распознавания лиц, используя Triplet Loss. Пусть у нас есть выборка изображений лиц, состоящая из 10 лиц, каждое из которых представлено 5 изображениями (всего 50 изображений). Для каждого лица мы будем использовать следующие триплеты:

- Анкер: $a_i$ (изображение лица $i$)
- Положительное: $p_i$ (другое изображение того же лица $i$)
- Отрицательное: $n_j$ (изображение другого лица $j \neq i$)

\textbf{Решение:} Рассмотрим, что у нас есть следующие расстояния, измеренные в эмбеддинговом пространстве (предположим, что они были вычислены):

- $\|f(a_1) - f(p_1)\| = 0.2$
- $\|f(a_1) - f(n_2)\| = 0.8$

Пусть $\text{margin} = 0.5$. Тогда расчет Triplet Loss для первого триплета будет:

$$
L_1 = \max\left\{0, \|f(a_1) - f(p_1)\| - \|f(a_1) - f(n_2)\| + \text{margin}\right\} = \max\left\{0, 0.2 - 0.8 + 0.5\right\} = \max\left\{0, -0.1\right\} = 0.
$$

Таким образом, данный триплет не вносит вклад в функцию потерь, так как модель уже правильно различает положительное и отрицательное примеры.

\subsubsection*{Задача 2: Поиск похожих изображений}

\textbf{Описание:} Создайте систему, которая может находить похожие изображения в большом наборе данных. Пусть у нас есть выборка из 1000 изображений, разделенных на 100 классов (по 10 изображений в каждом классе). Для каждого класса сформируем триплеты.

\textbf{Решение:} Рассмотрим триплет для класса 1:

- Анкер: $a_1$ (изображение 1 класса)
- Положительное: $p_1$ (изображение 2 класса)
- Отрицательное: $n_2$ (изображение из класса 2)

Предположим, что у нас есть следующие расстояния:

- $\|f(a_1) - f(p_1)\| = 0.3$
- $\|f(a_1) - f(n_2)\| = 0.9$

Для этого триплета:

$$
L_2 = \max\left\{0, \|f(a_1) - f(p_1)\| - \|f(a_1) - f(n_2)\| + \text{margin}\right\} = \max\left\{0, 0.3 - 0.9 + 0.5\right\} = \max\left\{0, -0.1\right\} = 0.
$$

Таким образом, данный триплет также не вносит вклад в функцию потерь.

\subsubsection*{Задача 3: Классификация текстов с использованием Triplet Loss}

\textbf{Описание:} Примените Triplet Loss для задачи классификации текстов. Пусть у нас есть 300 текстов, разделенных на 3 класса (по 100 текстов в каждом классе). Для каждого класса сформируем триплеты.

\textbf{Решение:} Рассмотрим триплет для класса 1:

- Анкер: $a_1$ (текст 1 класса)
- Положительное: $p_1$ (текст 2 класса)
- Отрицательное: $n_2$ (текст из класса 2)

Предположим, что у нас есть следующие расстояния:

- $\|f(a_1) - f(p_1)\| = 0.4$
- $\|f(a_1) - f(n_2)\| = 1.0$

Для этого триплета:

$$
L_3 = \max\left\{0, \|f(a_1) - f(p_1)\| - \|f(a_1) - f(n_2)\| + \text{margin}\right\} = \max\left\{0, 0.4 - 1.0 + 0.5\right\} = \max\left\{0, -0.1\right\} = 0.
$$

Таким образом, данный триплет также не вносит вклад в функцию потерь.

\section*{Дивергенция Кульбака-Лейблера}

\subsection*{Введение}
Расстояние Кульбака-Лейблера (KL-дивергенция) — это мера, которая используется для оценки различий между двумя вероятностными распределениями: истинным распределением P (обычно неизвестным) и приближённым распределением Q  (например, предсказанным моделью). KL-дивергенция широко применяется в задачах оптимизации, машинного обучения и статистики, где требуется минимизировать расхождения между предсказаниями модели и реальными данными. В теории обучения KL-дивергенция часто используется в качестве функции потерь для сравнения распределений.

\subsection*{Определение KL-дивергенции}

KL-дивергенция между двумя вероятностными распределениями  P и Q, заданными на одном и том же пространстве событий $\mathcal{X}$, вычисляется по следующей формуле:
\[
D_{KL}(P | Q) = \sum_{x \in \mathcal{X}} P(x) \log \frac{P(x)}{Q(x)} \quad \text{(для дискретного случая)},
\]

или

\[
D_{KL}(P | Q) = \int_{\mathcal{X}} P(x) \log \frac{P(x)}{Q(x)} , dx \quad \text{(для непрерывного случая)}.
\]

Где:
    P(x) — истинное распределение,
    Q(x) — приближённое распределение

\subsection*{Свойства KL-дивергенции}
\begin{enumerate}
    \item Неотрицательность: $ D_{KL}(P | Q) \geq 0 $ . KL-дивергенция равна нулю только тогда, когда P(x) = Q(x)  почти всюду.
    \item Несимметричность: $ D_{KL}(P | Q) \neq D_{KL}(Q | P) $.
    \item Аддитивность для независимых распределений:
     Если $\displaystyle P_{1},P_{2}$ являются независимыми распределениями с совместным распределением $\displaystyle P(x,y)=P_{1}(x)P_{2}(y)$ и, аналогично, $\displaystyle Q(x,y)=Q_{1}(x)Q_{2}(y)$,\\ то $\displaystyle D_{\mathrm {KL} }(P\parallel Q)=D_{\mathrm {KL} }(P_{1}\parallel Q_{1})+D_{\mathrm {KL} }(P_{2}\parallel Q_{2}).$
\end{enumerate}


\subsection*{Пример KL дивергенции нормальных распределени}
Допустим что мы имеем два многомерных нормальных распределения: $f_1 \in \mathcal{N}(\mu_1, \Sigma_1), \ f_2 \in \mathcal{N}_1(\mu_2, \Sigma_2)$
\\ 
Если два распределения имеют одинаковую размерность k, то KL-дивергенция между распределениями следующее:
\[ (*)
D_{KL}(f_1, f_2) = \frac{1}{2} (tr (\Sigma_2^{-1} \Sigma_1) + (\mu_2 - \mu_1)^T\Sigma_2^{-1}(\mu_2 - \mu_1) - k + \ln\left( \frac{\det \Sigma_2}{\det \Sigma_1} \right) )
\]

\subsection*{Связь KL-дивергенции и логистической регрессии}
Расстояние Кульбака-Лейблера (KL-дивергенция) представляет собой меру различия между двумя распределениями вероятностей: истинным распределением данных $P(y|x)$ и предсказанным распределением модели  $Q(y|x, \theta)$. В контексте логистической регрессии $P(y|x)$ обычно интерпретируется как истинные метки классов (для задач классификации это "одноточечное" 
распределение, где вероятность истинного класса равна 1, а остальных — 0), а $Q(y|x, \theta)$ — это предсказанная моделью вероятность $P(y|x, \theta) = \sigma(\theta^T x)$. KL-дивергенция между этими распределениями записывается как:

\[
D_{KL}(P || Q) = \sum_{y} P(y|x) \log \frac{P(y|x)}{Q(y|x, \theta)}.
\]

Так как $P(y|x)$ является истинным распределением, в котором одна из вероятностей равна 1 (для истинного класса), а остальные равны 0, выражение упрощается до:

\[
D_{KL}(P || Q) = - \sum_{i=1}^m \left[ y^{(i)} \log Q(y=1|x^{(i)}, \theta) + (1 - y^{(i)}) \log Q(y=0|x^{(i)}, \theta) \right].
\]

Это выражение эквивалентно отрицательному логарифму правдоподобия (negative log-likelihood), которое используется как функция потерь в логистической регрессии. Таким образом, минимизация функции потерь логистической регрессии можно интерпретировать как минимизацию KL-дивергенции между истинным распределением меток и предсказанным распределением модели. Это означает, что мы стремимся сделать предсказанное распределение вероятностей как можно ближе к истинному распределению, что и является ключевой целью вероятностного подхода в машинном обучении.


\subsection*{Применение в генеративных моделях}
KL-дивергенция $D_{KL}(P | Q)$ широко применяется в задачах генеративного машинного обучения, где цель состоит в том, чтобы обучить модель $ Q $ (приблизительное распределение) генерировать данные, максимально похожие на распределение реальных данных $P$. Это расхождение используется как метрика для сравнения истинного распределения данных и распределения, сгенерированного моделью. Рассмотрим основные области её применения:

 Вариационные автокодировщики (Variational Autoencoders, VAE)

Вариационные автокодировщики — это класс генеративных моделей, которые используют KL-дивергенцию для оптимизации латентного пространства. Основная идея VAE заключается в том, чтобы аппроксимировать апостериорное распределение $ P(z|x) $ (где $ z $ — латентное представление, а $ x $ — данные) с помощью параметризованного распределения $Q(z|x)$ (обычно гауссовского).

Роль KL-дивергенции в VAE:

В процессе обучения VAE минимизирует функцию потерь, состоящую из двух частей:
\[
\mathcal{L} = \mathbb{E}{Q(z|x)}[-\log P(x|z)] + D{KL}(Q(z|x) | P(z)).
\]

    Первый член $\mathbb{E}_{Q(z|x)}[-\log P(x|z)]$ — это реконструкционная ошибка, измеряющая, насколько хорошо модель восстанавливает данные  $x$ из латентного представления $z$.
    Второй член $D_{KL}(Q(z|x) | P(z))$ регулирует, насколько распределение $Q(z|x)$ (аппроксимация апостериорного распределения) близко к заданному априорному распределению $ P(z) $ (обычно стандартному нормальному распределению).

Таким образом, KL-дивергенция помогает "регуляризовать" латентное пространство, чтобы оно следовало заданному априорному распределению. Это обеспечивает более стабильное обучение и позволяет эффективно генерировать новые данные.

\subsection*{Задачи}
\subsubsection* {Задача 1}
Предположим, что у нас есть два распределения вероятностей:

    Истинное распределение $P$, которое задаёт вероятность принадлежности объекта к классу 1:
    \[
    P(y=1) = 0.7 \quad \text{и} \quad P(y=0) = 0.3.
    \]

    Модельное распределение Q, предсказанное некоторой моделью:
    \[
    Q(y=1) = 0.6 \quad \text{и} \quad Q(y=0) = 0.4.
    \]

\textbf{Решение}:

    Формула для KL-дивергенции:
    \[
    D_{KL}(P | Q) = \sum_{y} P(y) \log \frac{P(y)}{Q(y)}.
    \]

Подставим значения для ( y=1 ) и ( y=0 ):

\[
D_{KL}(P | Q) = P(y=1) \log \frac{P(y=1)}{Q(y=1)} + P(y=0) \log \frac{P(y=0)}{Q(y=0)}.
\]

Используем  $P(y=1) = 0.7,  P(y=0) = 0.3,  Q(y=1) = 0.6 ,  Q(y=0) = 0.4$:

\[
D_{KL}(P | Q) = 0.7 \log \frac{0.7}{0.6} + 0.3 \log \frac{0.3}{0.4} = 0.0216
\]

\subsubsection*{Задача 2}
Докажите формулу (*) 

\textbf{Решение}

Доказательство формулы для KL-дивергенции между двумя многомерными нормальными распределениями $f_1 \sim \mathcal{N}(\mu_1, \Sigma_1) ) и ( f_2 \sim \mathcal{N}(\mu_2, \Sigma_2) $ начинается с общего определения KL-дивергенции:

\[
D_{KL}(f_1 | f_2) =  \math{E}_{f_1} \ln \frac{f_1(x)}{f_2(x)} dx.
\]

Подставим плотности вероятностей для $f_1(x)$ и $f_2(x)$:

\[
f_1(x) = \frac{1}{(2\pi)^{k/2} |\Sigma_1|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu_1)^T \Sigma_1^{-1} (x - \mu_1)\right),
\]

\[
f_2(x) = \frac{1}{(2\pi)^{k/2} |\Sigma_2|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu_2)^T \Sigma_2^{-1} (x - \mu_2)\right).
\]



Поделим одно распределение на другое:

\[
\frac{f_1(x)}{f_2(x)} = \frac{\frac{1}{(2\pi)^{k/2} |\Sigma_1|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu_1)^T \Sigma_1^{-1} (x - \mu_1)\right)}{\frac{1}{(2\pi)^{k/2} |\Sigma_2|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu_2)^T \Sigma_2^{-1} (x - \mu_2)\right)}.
\]

Упростим это выражение:

\[
\ln \frac{f_1(x)}{f_2(x)} = \ln \frac{|\Sigma_2|^{1/2}}{|\Sigma_1|^{1/2}} + \frac{1}{2}(x - \mu_2)^T \Sigma_2^{-1} (x - \mu_2) - \frac{1}{2}(x - \mu_1)^T \Sigma_1^{-1} (x - \mu_1) =  \]
\[
= \ln \frac{|\Sigma_2|^{1/2}}{|\Sigma_1|^{1/2}} + \frac{1}{2}(tr(\Sigma_2^{-1}(x - \mu_2)(x - \mu_2)^T) - tr(\Sigma_1^{-1}(x - \mu_1)(x - \mu_1)^T)) = 
\]
\[
= \ln \frac{|\Sigma_2|^{1/2}}{|\Sigma_1|^{1/2}} -\frac{1}{2}(tr(\Sigma_1^{-1}\Sigma_1) + tr(\Sigma_2^{-1}(xx^T - 2x \mu_2^T + \mu_2 \mu_2^T))) =
\]
\[
= \ln \frac{|\Sigma_2|^{1/2}}{|\Sigma_1|^{1/2}} -\frac{1}{2}n + \frac{1}{2} tr(\Sigma_2^{-1} (\Sigma_1 + \mu_1 \mu_1^T - 2\mu_2 \mu_1^T + \mu_2 \mu_2 ^T)) = 
\]
\[
 = \ln \frac{|\Sigma_2|^{1/2}}{|\Sigma_1|^{1/2}}  - \frac{1}{2}n  + \frac{1}{2}tr(\Sigma_2^{-1}\Sigma_1) + \frac{1}{2}tr(\mu_1^T \Sigma_2^{-1} \mu_1 - 2\mu_1^T \Sigma_2^{-1} \mu_2 + \mu_2^T\Sigma_2^{-1}\mu_2) 
\]

Теперь KL-дивергенция принимает вид:

\[
D_{KL}(f_1 | f_2) = \frac{1}{2} (ln\frac{|\Sigma_2|}{|\Sigma_1|} - n + tr(\Sigma_2^{-1}\Sigma_1) + (\mu_2 - \mu_1)^T\Sigma_2^{-1}(\mu_2 - \mu_1))
\]

\subsubsection*{Задача 3 минимизация KL дивергенции}
Пусть имеется некоторое вероятностное распределение, известное с точностью до нормировачной константы 
\[p(x) = \frac{1}{Z_p}\Tilde{p}(x) \]
Заметим, что из-за неизвестности $Zp$ значение оптимизируемого
функционала $KL(q||p)$ не может быть вычислено напрямую.
Докажите что: 
\[KL(q||p) \rightarrow \underset{q}{\min} \Leftrightarrow \mathcal{L}(q) \rightarrow \underset{q}{\min}\] 
где: $\mathcal{L}(q) = \int q(x) \log \frac{\Tilde{p}(x)}{p(x)}dx$.


\textbf{Решение}
\[
\log Z_p = \int q(x) \log Z_pdx  = \int q(x) \log \frac{\Tilde{p}(x)}{p(x)}dx = \int q(x) \log \frac{\Tilde{p}(x)}{q(x)}\frac{q(x)}{p(x)}dx
= \]
\[
= \int q(x) \log \frac{\Tilde{p}(x)}{q(x)}dx - \int q(x) \log \frac{p(x)}{q(x)}dx = \mathcal{L}(q) + KL(p||q)
\]

В силу неотрицательности KL-дивергенции отсюда следует,\\ что $\log Z_p \geq \mathcal{L}(q)$, причем равенство
достигается тогда и только тогда, когда $q(x) \equiv p(x)$. \\ 
Таким образом, задача минимизации прямой KL-дивергенции сведена с эквивалентной задаче максимизации функционала $L(q)$


\section*{Функция потерь для задачи поиска близких предложений.}

\subsubsection*{Интуиция:}
мы хотим категоризировать объекты, то есть ввести расстояние так, чтобы объекты из одного класса были близки, а из разных - далеки.

\subsubsection*{Функция потерь Triplet loss:}
в качестве решения вводится триплетная функция потерь (triplet loss).
Для обучения триплетного лосса рассматриваются тройки объектов:

\begin{itemize}
    \item якорный объект - произвольный обьект какого-то класса 
    \item позитивный к якорному объект - обьект, похожий на якорный
    \item негативный к якорному объект - обьект, далекий от якорного
\end{itemize}

\subsubsection*{Как устроено обучение:}

Пусть $i$-й вход имеет вид $(x_i^a, x_i^p, x_i^n)$ - якорный, позитивный и негативный объекты соответственно. Тогда будем минимизировать:

$$L = \sum\limits_i \left[\left \lVert f(x_i^a) - f(x_i^p) \right \rVert_2^2 - \left \lVert f(x_i^a) - f(x_i^n) \right \rVert_2^2 + \alpha \right]_+$$

Где $\alpha$ - фиксированный гиперпараметр - смещение(margin), то есть при $\alpha = 0$, нам достаточно что негативный объект дальше, чем позитивный, 
а при положительном $\alpha$ - что негативный объект дальше как минимум на $\alpha$.

Таким образом, мы минимизируем расстояния между якорными и позитивными объектами,
и максимизируем расстояния между якорными и негативными объектами за счет знака минус в формуле.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{chapters/nonstandart_error/images/triplet_loss_learning.png}
\end{figure}

\subsubsection*{Алгоритм формирования троек:}

обучение с триплет лоссом сильно зависит от алгоритма формирования троек.
Если формировать тройки случайно, то большинство троек будут слишком легкими, не информативными.
Негативные объекты будет слишком легко отличить от позитивных, поэтому обучающего сигнала от таких троек будет мало.
Поэтому хочется собрать наиболее сложные тройки из всех объектов в датасете или минибатче.
Такой процесс называется hard negative/positive mining и часто используется для обучения с триплетной функцией потерь.

\subsubsection*{Пример - задача поиска близких предложений}
\begin{itemize}
    \item в качестве позитивных объектов берем якорное предложение с опечатками, якорное предложение на других языках, с uppercase, lowercase буквами вперемешку
    \item в качестве негативных - другие предложения
\end{itemize}

\subsubsection*{Задача 1.}

Придумайте алгоритм подбора троек для задачи распознавания лиц

Ответ: 
\begin{itemize}
    \item в качестве позитивных объектов берем якорое лицо в другом освещение, с разных сторон
    \item в качестве негативных - лица другиз людей
\end{itemize}

\subsubsection*{Задача 2.}

Подумайте на что влияет $\alpha$, как будут меняться поведение модели при большом/нулевом $\alpha$

Ответ: роль $\alpha$ - значимое разделение классов, 
при нуле - не будет заметна разница между похожими и отличающимися объектами,
при слишком большом $\alpha$ - будем часто считать объекты одного класса различными

\subsubsection*{Задача 3.}

Реализуйте triplet loss с помощью PyTorch, для разных норм, сравните, есть ли отличия.


\newpage

\section*{Нестандартные функции потерь. Функции потерь для обработки временных рядов.}

Временные ряды часто имеют уникальные особенности, такие как сезонность, тренд и автокорреляция, которые необходимо учитывать при построение модели. Поэтому использование стандартных функций потерь не всегда рационально.

Сезонность относится к периодическим колебаниям в данных временных рядов, которые происходят в определенные временные интервалы. Это могут быть ежедневные, еженедельные, ежемесячные или годовые паттерны. Например, увеличение продаж в праздничные периоды.
Поэтому требуются функции потерь, способные учитывать периодические паттерны, важно правильно взвешивать ошибки для разных периодов. Стандартные функции потерь не справляются с этой задачей.

Тренд - долгосрочные изменения в данных, которые могут быть линейными или нелинейными,
Например, рост цен на жилье связанное с ростом населения города или экономики в целом.
Функции потерь должны адекватно работать при наличии долгосрочного тренда, важно учитывать разные масштабы значений в начале и конце прогнозируемого ряда.

Автокорреляция это мера корреляции между значениями ряда с разницей во времени, зависимость текущих значений от предыдущих, что требует учета временной структуры данных.
Стандартные функции потерь не учитывают эту зависимость, тогда как, специальные функции могут учитывать структуру автокорреляции.

Далее используются следующие обозначения:\\
$y_{i,t}$ - значение $i$ временного ряда в момент времени $t$.\\
$f_{i,t}$ - предсказанное значение $i$ временного ряда в момент времени $t$.\\
$f_{i,t}^q$ - предсказанный квантиль $q$ временного ряда $i$ в момент времени $t$\\
$N$ - количество временных рядов.\\
$T$ -  длина временного ряда.\\
$H$ - длина предсказанных значений.\\

Нестандартные функции потерь используемые для работы с временными рядами:\\
\\
$MASE$ (Mean Absolute Scaled Error)

$$MASE = \frac{1}{N} \frac{1}{H} \sum_{i = 1}^{N} \frac{1}{a_{i}}\sum_{t = T + 1} ^ {T + H}|y_{i,t} - f_{i,t}| $$ , где $a_{i}$
$$a_{i} = \frac{1}{T - m} \sum_{t = m + 1}^{T}|y_{i,t} - y_{i, t - m}|$$, для $m$ - период сезонности.

Фактически $MASE$ это $MAE$ предсказанных значений разделенное на $MAE$ наивного прогноза(предсказанное значение это последнее наблюдаемое значение), с поправкой на сезонность ряда.
$MASE$ устойчивее к выбросам, учитывает сезонность поэтому подходят для оценки временных рядов.\\
\\
$SQL$ Scaled quantile loss.
$$SQL = \frac{1}{N} \frac{1}{H} \sum_{i = 1}^{N} \frac{1}{a_{i}}\sum_{t = T + 1} ^ {T + H}p_{q}(y_{i,t}, f_{i,t}^{q})$$, где
$$p_{q}(y_{i,t}, f_{i,t}^{q}) = \begin{cases}
    2(1 - q)(f_{i,t}^q - y_{i,t}), если y_{i,t} < f_{i,t}^q\\
    2q(y_{i,t} - f_{i,t}^q ),если y_{i,t} \geq f_{i,t}^q
\end{cases}$$
\\
Совпадает с $MASE$, если $q = \frac{1}{2}$, устойчивость к выбросам, позволяет оценивать качество предсказаний на разных уровнях (квинтилях), что дает более полное представление о распределении ошибок. Это полезно, когда важно учитывать как нижние, так и верхние границы предсказаний. Это делает $SQL$ полезным инструментом для анализа и предсказания временных рядов, особенно в ситуациях, когда важно учитывать распределение ошибок и устойчивость к выбросам.\\
\\
Функция потерь учитывающая автокорреляцию. ACF loss может быть определена как разница между автокорреляцией фактических значений и автокорреляцией предсказанных значений

Для временного ряда - автокорреляция на лаге k определяется как:
    \[
   \text{ACF}(k) = \frac{\sum_{t=k+1}^{N} (y_t - \bar{y})(y_{t-k} - \bar{y})}{\sum_{t=1}^{N} (y_t - \bar{y})^2}
   \]

   где \( \bar{y} \) — среднее значение временного ряда\\

$ACF_$ loss
   \[
   \text{ACF loss} = \sum_{k=1}^{K} \left( \text{ACF}_{\text{actual}}(k) - \text{ACF}_{\text{predicted}}(k) \right)^2
   \]
   где \( K \) — максимальный лаг, на котором вы хотите оценивать автокорреляцию.
Таким образом, ACF loss измеряет, насколько хорошо предсказанные значения сохраняют автокорреляционные свойства временного ряда. \\
\\
Задача 1\\
\\
При прогнозировании спроса на товар важнее не допустить нехватки товара, чем его переизбытка. Какую функцию потерь следует использовать?
\\
\\
Ответ:
Scaled Quantile Loss с $q$ > 0.5 (например, 0.8), что приведет к более "осторожным" прогнозам с меньшей вероятностью недооценки, также такая фукнция потерь учитывает сезонность, в отличие от стандартных функций потерь.
\\
\\
Задача 2\\
\\
Для задачи финансового анализа необходимо предсказывать цены акций, которые имеют сильные временные зависимости. Какую функцию потерь следует использовать для этой задачи и почему?
\\
\\
Ответ: Используйте ACF loss, чтобы сохранить автокорреляционные свойства временного ряда. Это важно, так как финансовые данные часто имеют зависимость от времени, и использование ACF loss поможет избежать искажений в оценках риска и улучшить качество предсказаний, сохраняя структуру временных зависимостей.
\\
\\
Задача 3\\
Компании необходимо прогнозировать ежемесячные продажи. Товар компании имеет явную сезонность, какую фукнцию потерь лучше использовать и почему?
\\
Ответ: MASE будет оптимальным выбором, так как она, в отличие от стандартных функций, учитывает сезонность и менее чувствительна к выбросам.
