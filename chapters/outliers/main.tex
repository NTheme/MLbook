
\section{Задачи выявления аномалий}

Аномалии (anomalies) --- данные, существенно отклоняющиеся от общего распределения в выборке. В контексте машинного обучения аномалии могут быть двух типов: выбросы (outliers) и новизна (novelty).

\textbf{Выявление выбросов (Outlier detection)}

\begin{itemize}
\item Ошибки в данных обучающего или тестового объекта
\item Неадекватность модели на некоторых объектах
\end{itemize}

\textbf{Выявление «новизны» (Novelty detection)}

\begin{itemize}
\item Ничего подобного не было в обучающей выборке
\end{itemize}


\subsection{Приложения}

Обнаружение аномалий имеет широкое применение в различных областях:

\begin{itemize}
    \item Обнаружение мошенничества
    \item Обнаружение инсайдерской торговли на бирже
    \item Обнаружение неполадок по показаниям датчиков
    \item Медицинская диагностика
\end{itemize}

\section{Эвристики для оценивания аномальности объектов}

\subsection{Метрические методы}

\textit{Аномальность} объекта определяется как расстояние до его $k$-го ближайшего соседа. Чем больше это расстояние, тем менее плотной является область, в которой находится данный объект. Объекты, находящиеся в разреженных областях пространства признаков, скорее всего, являются аномальными.


\subsection{Isolation Forest}

Метод $\textit{Isolation Forest}$ использует случайные деревья для поиска аномальных объектов.

В одном случайном дереве ветвления разбивают данные по случайному признаку и порогу. В итоге в листьях окажется по одному объекту.

\textit{Аномальность} объекта определяется как средняя глубина листьев построенных деревьев, в которые он попадает.

Если объект оказывается в неглубоких листьях дерева (то есть его можно изолировать с помощью небольшого количества случайных разделений), это указывает на его аномальность.

\subsection{Robust PCA}

В традиционном методе главных компонент (PCA) мы пытаемся найти новое представление данных с меньшим количеством признаков. Однако этот метод чувствителен к выбросам. Робастный метод главных компонент \textit{Robust PCA} позволяет учитывать возможные выбросы и разреженные шумы.

Напомним, что в методе главных компонент мы для объектов с исходными числовыми признаками $f_1(x) ... f_n(x)$ строили новые числовые признаки $g_1(x) ... g_m(x)$ так, чтобы при этом по новым можно было реконструировать старые: $\hat{f}_j(x) = \sum_{s=1}^m g_s(x)u_{js}$. Для этого решалась оптимизационная задача поиска оптимальных новых признаков $G$ и преобразования $U$:

$$\sum_{i=1}^l\sum_{j=1}^n (\hat{f}_j(x_i) - f_j(x_i))^2 = \|GU^T - F\|^2 \to \min\limits_{G,U}$$

Теперь \textit{аномальностью} объекта назовем неизвестный разреженный шум $(\varepsilon_i)_{i=1}^l$.

Новая оптимизационная задача будет находить еще и разреженную матрицу шума $E$, таким образом определяя аномальности объектов.

$$\|GU^T - F - E\|^2 + \lambda \|E\|_1 \to \min\limits_{G, U, E}$$

\section{Алгоритм LOWESS}

Алгоритм LOWESS (LOcally WEighted Scatter plot Smoothing) — это метод локального сглаживания данных, который используется для построения гладкой кривой через набор данных, с сохранением локальных структур в данных. LOWESS применяет взвешенное линейное регрессионное сглаживание, где для каждого точки данных используется локальная линейная регрессия на основе её соседей.

Идея состоит в том, чтобы сглаживать график по формуле ядерного сглаживания Надарая-Ватсона. Для этого будет использоваться невозрастающее ограниченное гладкое ядро $K(r)$ с шириной окна $h$. Напомним формулу:

$$a_h(x; X^l) = \frac{\sum_{i=1}^ly_i K(\frac{\rho(x, x_i)}{h})}{\sum_{i=1}^l K(\frac{\rho(x, x_i)}{h})}$$

Для точек из обучающей выборки $X^l$ будут установлены веса $\gamma_i$, изначально инициализированные $\gamma_i=1$. Алгоритм будет пересчитывать эти веса до их стабилизации.

На каждой итерации вычисляются оценки скользящего контроля в каждом объекте:

$$a_i := a_h(x_i;X^l\setminus \{x_i\}) = \frac{\sum_{j=1, j \neq i}^l y_j \gamma_j K(\frac{\rho(x_i, x_j)}{h})}{\sum_{j=1, j \neq i}^l \gamma_j K(\frac{\rho(x, x_j)}{h})}$$

После этого обновляются веса

$$\gamma_i := \tilde{K}(|a_i - y_i|)$$

\section{Систематизация подходов}

Во всех описанных подходах \textit{аномальность} объекта (anomaly/novelty/surprise score) --- это значение функции потерь $\mathcal{L}(a(x_i), y_i)$ на данном объекте. Эта функция же может определяться по-разному:

\begin{itemize}
    \item Аномальность оценивается для объекта обучающей выборки или для нового объекта.
    \item Потеря зависит от $y_i$ (supervised) или не зависит (unsupervised).
    \item При оценивании аномальности обучающего объекта он исключается из выборки ($a(x_i;X^l \setminus \{x_i\}$)  или нет ($a(x_i;X^l)$).
    \item Функция потерь та же, что в критерии обучения или нет.
\end{itemize}

Затем оценки аномальности могут использоваться либо для жёсткого удаления аномальных объектов из выборки, либо для мягкого перевзвешивания весов объектов.

\section{Задачи}
\begin{enumerate}
    \item Для предложенных приложений формальнее сформулируйте постановку задачи, и объясните, почему возникают аномалии в данных, в чем они выражаются.
    \item Используя набор данных с известными аномалиями (например, набор данных о мошенничестве с кредитными картами), реализуйте два метода для обнаружения аномалий: $k$-ближайших соседей и Isolation Forest. Оцените их точность и вычислительные затраты.
    \item Реализуйте алгоритм LOWESS, экспериментально проверьте, как быстро он сходится на синтетических данных. 
\end{enumerate}
