
\section{Задачи выявления аномалий}

Аномалии (anomalies) --- данные, существенно отклоняющиеся от общего распределения в выборке. В контексте машинного обучения аномалии могут быть двух типов: выбросы (outliers) и новизна (novelty).

\textbf{Выявление выбросов (Outlier detection)}

\begin{itemize}
\item Ошибки в данных обучающего или тестового объекта
\item Неадекватность модели на некоторых объектах
\end{itemize}

\textbf{Выявление «новизны» (Novelty detection)}

\begin{itemize}
\item Ничего подобного не было в обучающей выборке
\end{itemize}


\subsection{Приложения}

Обнаружение аномалий имеет широкое применение в различных областях:

\begin{itemize}
    \item Обнаружение мошенничества
    \item Обнаружение инсайдерской торговли на бирже
    \item Обнаружение неполадок по показаниям датчиков
    \item Медицинская диагностика
\end{itemize}

\section{Эвристики для оценивания аномальности объектов}

\subsection{Метрические методы}

\textit{Аномальность} объекта определяется как расстояние до его $k$-го ближайшего соседа. Чем больше это расстояние, тем менее плотной является область, в которой находится данный объект. Объекты, находящиеся в разреженных областях пространства признаков, скорее всего, являются аномальными.


\subsection{Isolation Forest}

Метод $\textit{Isolation Forest}$ использует случайные деревья для поиска аномальных объектов.

В одном случайном дереве ветвления разбивают данные по случайному признаку и порогу. В итоге в листьях окажется по одному объекту.

\textit{Аномальность} объекта определяется как средняя глубина листьев построенных деревьев, в которые он попадает.

Если объект оказывается в неглубоких листьях дерева (то есть его можно изолировать с помощью небольшого количества случайных разделений), это указывает на его аномальность.

\subsection{Robust PCA}

В традиционном методе главных компонент (PCA) мы пытаемся найти новое представление данных с меньшим количеством признаков. Однако этот метод чувствителен к выбросам. Робастный метод главных компонент \textit{Robust PCA} позволяет учитывать возможные выбросы и разреженные шумы.

Напомним, что в методе главных компонент мы для объектов с исходными числовыми признаками $f_1(x) ... f_n(x)$ строили новые числовые признаки $g_1(x) ... g_m(x)$ так, чтобы при этом по новым можно было реконструировать старые: $\hat{f}_j(x) = \sum_{s=1}^m g_s(x)u_{js}$. Для этого решалась оптимизационная задача поиска оптимальных новых признаков $G$ и преобразования $U$:

$$\sum_{i=1}^l\sum_{j=1}^n (\hat{f}_j(x_i) - f_j(x_i))^2 = \|GU^T - F\|^2 \to \min\limits_{G,U}$$

Теперь \textit{аномальностью} объекта назовем неизвестный разреженный шум $(\varepsilon_i)_{i=1}^l$.

Новая оптимизационная задача будет находить еще и разреженную матрицу шума $E$, таким образом определяя аномальности объектов.

$$\|GU^T - F - E\|^2 + \lambda \|E\|_1 \to \min\limits_{G, U, E}$$

\section{Алгоритм LOWESS}

Алгоритм LOWESS (LOcally WEighted Scatter plot Smoothing) — это метод локального сглаживания данных, который используется для построения гладкой кривой через набор данных, с сохранением локальных структур в данных. LOWESS применяет взвешенное линейное регрессионное сглаживание, где для каждого точки данных используется локальная линейная регрессия на основе её соседей.

Идея состоит в том, чтобы сглаживать график по формуле ядерного сглаживания Надарая-Ватсона. Для этого будет использоваться невозрастающее ограниченное гладкое ядро $K(r)$ с шириной окна $h$. Напомним формулу:

$$a_h(x; X^l) = \frac{\sum_{i=1}^ly_i K(\frac{\rho(x, x_i)}{h})}{\sum_{i=1}^l K(\frac{\rho(x, x_i)}{h})}$$

Для точек из обучающей выборки $X^l$ будут установлены веса $\gamma_i$, изначально инициализированные $\gamma_i=1$. Алгоритм будет пересчитывать эти веса до их стабилизации.

На каждой итерации вычисляются оценки скользящего контроля в каждом объекте:

$$a_i := a_h(x_i;X^l\setminus \{x_i\}) = \frac{\sum_{j=1, j \neq i}^l y_j \gamma_j K(\frac{\rho(x_i, x_j)}{h})}{\sum_{j=1, j \neq i}^l \gamma_j K(\frac{\rho(x, x_j)}{h})}$$

После этого обновляются веса

$$\gamma_i := \tilde{K}(|a_i - y_i|)$$

\section{Систематизация подходов}

Во всех описанных подходах \textit{аномальность} объекта (anomaly/novelty/surprise score) --- это значение функции потерь $\mathcal{L}(a(x_i), y_i)$ на данном объекте. Эта функция же может определяться по-разному:

\begin{itemize}
    \item Аномальность оценивается для объекта обучающей выборки или для нового объекта.
    \item Потеря зависит от $y_i$ (supervised) или не зависит (unsupervised).
    \item При оценивании аномальности обучающего объекта он исключается из выборки ($a(x_i;X^l \setminus \{x_i\}$)  или нет ($a(x_i;X^l)$).
    \item Функция потерь та же, что в критерии обучения или нет.
\end{itemize}

Затем оценки аномальности могут использоваться либо для жёсткого удаления аномальных объектов из выборки, либо для мягкого перевзвешивания весов объектов.

\section{Задачи}
\begin{enumerate}
    \item Для предложенных приложений формальнее сформулируйте постановку задачи, и объясните, почему возникают аномалии в данных, в чем они выражаются.
    \item Используя набор данных с известными аномалиями (например, набор данных о мошенничестве с кредитными картами), реализуйте два метода для обнаружения аномалий: $k$-ближайших соседей и Isolation Forest. Оцените их точность и вычислительные затраты.
    \item Реализуйте алгоритм LOWESS, экспериментально проверьте, как быстро он сходится на синтетических данных. 
\end{enumerate}

\section*{Выявление аномалий с помощью одноклассового SVM}

Одноклассовые SVM представляют собой вариант традиционного алгоритма SVM, используемого в основном для задач выявления аномалий и novelty detection. В отличие от традиционных SVM, которые решают задачи бинарной классификации, одноклассовые SVM обучаются исключительно на точках данных одного (целевого) класса. Одноклассовая SVM нацелена на обучение граничной функции или функции принятия решений, которая охватывает целевой класс в пространстве признаков, эффективно моделируя нормальное поведение данных.
\\

\subsection{Постановка задачи}

\textbf{Дано} \\

Выборка точек $\{x_i, \in \mathbb{R}^n : i = 1, ... l\}$\\

\textbf{Задача} \\

Охватить одной окружностью всю выборку. Все точки, которые не попали внутрь окружности, принимаем за аномальные.\\

\textbf{Искомые параметры} \\

Центр $c$ и радиус $r$ окружности. \\

\textbf{Критерий} \\

Минимизируем радиус шара и сумму штрафов за выход из шара:\\

\[ \nu r^2 + \sum_{i = 1}^l \mathcal{L} (r^2 - ||x_i - c||^2) \longrightarrow \min_{c, r}\] \\

Выражение в скобках есть расстояние от точки до разделяющей окружности ($\zeta_i = margin(c,r)$). Мы можем варьировать параметр $\nu$ и функцию потерь $\mathcal{L}$. \\

\textbf{Фунция потерь} \\

Естественное ограничение на функцию потерь = $\mathcal{L} (r^2 - ||x_i - c||^2) = 0$ при $||x_i - c||^2 < r^2$. В этом случае всё сводится к двойственной выпуклой задачe.

\subsection{Выявление аномальных точек} 

Опорными векторами в этой задаче, таким образом, будут точки с ненулевым значением $\mathcal{L} (r^2 - ||x_i - c||^2)$. Эти точки мы принимаем за аномальные. \\

\subsection{Применение Kernel Trick} 

Как и в стандартной задаче SVM, мы можем применять в задаче одноклассового SVM метод Kernel Trick. Для этого мы вводим для выборки новые признаки и переходим в пространство признаков большей размерности, в котором определяем окружность, то есть гиперплоскость в этом пространстве. Таким образом в исходном признаковом пространстве мы можем определять границу класса более сложной поверхностью.

\subsection{Достоинства и недостатки метода} 

\textbf{Достоинства} \\

- Метод эффективен при работе с несбалансированными данными: Хорошо работает, когда доступны данные только нормального класса.

- Использование Kernel Trick позволяет моделировать нелинейные отношения.

- Устойчивость: Эффективно справляется с шумом в данных.\\

\textbf{Недостатки} \\

- Чувствительность к параметрам: Производительность может быть чувствительна к выбору таких параметров, как функция потерь и $\nu$.

- Интенсивность вычислений: может быть вычислительно затратным для больших наборов данных из-за квадратичной задачи оптимизации.

- Требуется масштабирование признаков: Для эффективной работы необходимо правильно масштабировать входные признаки.

\subsection{Задачи} 

\textbf{Задача 1.} \\

Дана выборка на плоскости: $(1, 1), (1, 2), (2, 1), (2, 2), (1.5, 1.5)$
В качества скалярного произведения выберем стандартное. Функция потерь - линейное отклонение: $\mathcal{L} (r^2 - ||x_i - c||^2) = |r^2 - ||x_i - c||^2|$. Для параметра $\nu$ рассмотрим три значения: 0.1, 0.5, 1.5. Для этих случаев вычислите, какие точки будут определены как выбросы. Постройте точки и полученные окружности. \\

\textbf{Решение} \\

Из построения видно, что центр искомой окружности будет находиться в точке $(1.5, 1.5)$. Следовательно, есть только два варианта: либо все пять точек входят в окружность, либо в окружность входит только центральная точка.

При $\nu = 0.1$ минимум достигается при радиусе $\sqrt{2} / 2$. Следовательно, в этом случае все точки будут лежать внутри окружности и ее радиус будет равен $\sqrt{2} / 2$.

При $\nu = 0.5$ ситуация аналогична предыдущей.

При $\nu = 1.5$ минимум достигается при $r = 0$ и в окружность входит только ее центр. \\

\textbf{Задача 2.} \\

Составьте выборку из 4 точек на плоскости. Выберите еще 2 точки, которые будут считаться выбросами. Ответьте на вопросы: \\

1) Возможно ли разделить выбранные вами точки на плоскости одной окружностью, не используя пространство большей размерности?

2) Определите центр и радиус разделяющей окружности. \\

3) При каком $\nu$ можно получить эту окружность в результате работы алгоритма одноклассового SVM?\\

\textbf{Решение} \\

В качестве правильных точек выберем $\{(2,0), (1,2), (-1,1), (-1,-1)\}$, в качестве аномальных - точки $\{(-2,4), (-1,-4)\}$.

1) Эти точки можно разделить окружностью на плоскости.

2) Возможные параметры окружности : $r = 3, c = (0,0)$. Мы используем стандартное скалярное произведение. Можно также посчитать $\nu$, при котором в результате работы алгоритма получится такая окружность. \\ 

\textbf{Задача 3.} \\

Даны точки на прямой: $\{-15, -1, 1, 2, 3, 10, 38, 65, 68, 70, 100, 140\}$. Предложите функцию $f: \mathbb{R} \rightarrow \mathbb{R}^2$, которая перевела бы точки на плоскость так, чтобы их можно было разделить окружностью. Вычислите центр и радиус этой окружности.\\

\textbf{Решение} \\

В этой задаче выбросы характеризуются большим расстоянием до соседних точек. Поэтому можно попробовать ввести признак $y = $ "сумма модулей расстояний до двух соседних с ней точек". Тогда аномальные точки будут находиться на графике выше правильных. В качестве разделяющей окружности подходит, например, окружность с центром (36, 0) радиусом $r = 40$.

